{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SuperComputing Luxembourg and EuroCC (EuroHPC Joint Undertaking) \u00b6 We have categorized our training events into three: Courses: \u00b6 Courses are primarily designed for just one day. It is mainly considered an effective way of learning rather than focusing on more than one training day. Courses are mainly focused on parallel programming on CPUs and GPUs and acceleration of your scientific software on supercomputers. For example, OpenMP, MPI, OpenACC, CUDA, and OpenMP Offloading using programming languages such as C/C++ and FORTRAN. Plus, also we focus on Python, R, Matlab and Julia to parallelize your application. Regarding the software acceleration, GROMACS, CFD and FEM software, material science and bioinformatic, etc., will be offered. The one-day training consists of three stages: preparation (pre-preparation to follow up on the lecture and practicals) lecture (introduction and focusing on the course topic) practicals (hands-on session) Bootcamps: \u00b6 Bootcamps are usually one- or two-day events designed to teach scientists and researchers how to start quickly accelerating codes on modern processors (for example, GPUs). Participants will be introduced to available libraries, programming models, and platforms. They will learn the basics of parallel (CPU, GPU and hybrid) programming through extensive hands-on collaboration based on real-life codes using the parallel programming model. Hackathons: \u00b6 Hackathons are dedicated for a longer duration; it could be up to one or two months. During the hackathons, participants mainly focus on the HPC problems (HPC, AI and HPDA) that come from industry, particularly local Luxembourg industries. Participants will be grouped into many and each group member will get mentors from organizers (for example, Nvidia) and also from experts from supercomputing NCC Luxembourg. At the end of the event, there will also be a winner and prize for outstanding contribution.","title":"Trainings organization"},{"location":"#supercomputing-luxembourg-and-eurocc-eurohpc-joint-undertaking","text":"We have categorized our training events into three:","title":"SuperComputing Luxembourg and EuroCC (EuroHPC Joint Undertaking)"},{"location":"#courses","text":"Courses are primarily designed for just one day. It is mainly considered an effective way of learning rather than focusing on more than one training day. Courses are mainly focused on parallel programming on CPUs and GPUs and acceleration of your scientific software on supercomputers. For example, OpenMP, MPI, OpenACC, CUDA, and OpenMP Offloading using programming languages such as C/C++ and FORTRAN. Plus, also we focus on Python, R, Matlab and Julia to parallelize your application. Regarding the software acceleration, GROMACS, CFD and FEM software, material science and bioinformatic, etc., will be offered. The one-day training consists of three stages: preparation (pre-preparation to follow up on the lecture and practicals) lecture (introduction and focusing on the course topic) practicals (hands-on session)","title":"Courses:"},{"location":"#bootcamps","text":"Bootcamps are usually one- or two-day events designed to teach scientists and researchers how to start quickly accelerating codes on modern processors (for example, GPUs). Participants will be introduced to available libraries, programming models, and platforms. They will learn the basics of parallel (CPU, GPU and hybrid) programming through extensive hands-on collaboration based on real-life codes using the parallel programming model.","title":"Bootcamps:"},{"location":"#hackathons","text":"Hackathons are dedicated for a longer duration; it could be up to one or two months. During the hackathons, participants mainly focus on the HPC problems (HPC, AI and HPDA) that come from industry, particularly local Luxembourg industries. Participants will be grouped into many and each group member will get mentors from organizers (for example, Nvidia) and also from experts from supercomputing NCC Luxembourg. At the end of the event, there will also be a winner and prize for outstanding contribution.","title":"Hackathons:"},{"location":"Bootcamps/ai/introduction/","text":"The Luxembourg Competence Centre in High-Performance Computing (HPC), in collaboration with NVIDIA and OpenACC.org, is hosting online the AI for Science and Engineering Bootcamp during 2 half-days. The first part will be dedicated to theory, and the second part will focus on hands-on challenges on GPU accelerators of the MeluXina supercomputer. For whom? Both current or prospective users of large hybrid CPU/GPU clusters, which develop HPC and AI applications and could benefit from GPU acceleration, are encouraged to participate! What will you learn and how? During this online Bootcamp, participants will learn how to apply AI tools, techniques, and algorithms to real-life problems. Participants will be introduced to the critical concepts of Deep Neural Networks, how to build Deep Learning models, and how to measure and improve the accuracy of their models. Participants will also learn essential data pre-processing techniques to ensure a robust machine-learning pipeline. The Bootcamp is a hands-on learning experience where mentors guide participants. Learning outcomes After this course, participants will be able to: \u00b6 Apply Deep Convolutional Neural Networks for science and engineering applications Understand the Classification (multi-class classification) methodology in AI Implement AI algorithms using Keras (e.g. TensorFlow) Use an efficient usage of the GPU for AI algorithms (e.g. CNN) with handling large data set Run AI applications in the Jupyter notebook environment (and understand singularity containers) Prerequisites \u00b6 Priority will be given to users with basic experience with Python. No GPU programming knowledge is required. GPU Compute Resource Participants attending the event will be given access to\u202fthe MeluXina supercomputer\u202fduring the hackathon. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide. Agenda This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email. Day 1 \u2013 Thursday, February 9 th 2023: 01:30 PM \u2013 05:00 PM 01:30 PM \u2013 01:45 PM: Welcome (Moderator) 01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture) 02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture) 03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab) Day 2 \u2013 Friday, February 10 th 2023: 01:30 PM \u2013 05:00 PM 01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge) 04:45 PM \u2013 05:00 PM: Wrap up and QA","title":"Introduction"},{"location":"Bootcamps/ai/introduction/#after-this-course-participants-will-be-able-to","text":"Apply Deep Convolutional Neural Networks for science and engineering applications Understand the Classification (multi-class classification) methodology in AI Implement AI algorithms using Keras (e.g. TensorFlow) Use an efficient usage of the GPU for AI algorithms (e.g. CNN) with handling large data set Run AI applications in the Jupyter notebook environment (and understand singularity containers)","title":"After this course, participants will be able to:"},{"location":"Bootcamps/ai/introduction/#prerequisites","text":"Priority will be given to users with basic experience with Python. No GPU programming knowledge is required. GPU Compute Resource Participants attending the event will be given access to\u202fthe MeluXina supercomputer\u202fduring the hackathon. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide. Agenda This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email. Day 1 \u2013 Thursday, February 9 th 2023: 01:30 PM \u2013 05:00 PM 01:30 PM \u2013 01:45 PM: Welcome (Moderator) 01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture) 02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture) 03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab) Day 2 \u2013 Friday, February 10 th 2023: 01:30 PM \u2013 05:00 PM 01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge) 04:45 PM \u2013 05:00 PM: Wrap up and QA","title":"Prerequisites"},{"location":"Hackathons/hpda/introduction/","text":"HPDA with (Nvidia and CERATIZIT - 2024) \u00b6","title":"Introduction"},{"location":"Hackathons/hpda/introduction/#hpda-with-nvidia-and-ceratizit-2024","text":"","title":"HPDA with (Nvidia and CERATIZIT - 2024)"},{"location":"cuda/","text":"Introduction to GPU programming using CUDA \u00b6 Participants from this course will learn GPU programming using the CUDA programming model, such as synchronisation, memory allocation and device and host calls. Furthermore, understanding the GPU architecture and how parallel threads blocks are used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the CUDA programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the CUDA programming model with mentors\u2019 guidance later in the hands-on tutorial part. Learning outcomes \u00b6 After this course, participants will be able to: \u00b6 Understanding the GPU architecture (and also the difference between GPU and CPU) Streaming architecture Threads blocks Implement CUDA programming model Programming structure Device calls (threads block organisation) Host calls Efficient handling of memory management Host to Device Unified memory Apply the CUDA programming knowledge to accelerate examples from science and engineering Iterative solvers from science and engineering Matrix multiplication, vector addition, etc Prerequisites \u00b6 Priority will be given to users with good experience with C/C++ . No GPU programming knowledge is required. However, knowing some basic parallel programming concepts are advantage but not necessary. GPU Compute Resource \u00b6 Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide .","title":"Introduction"},{"location":"cuda/#introduction-to-gpu-programming-using-cuda","text":"Participants from this course will learn GPU programming using the CUDA programming model, such as synchronisation, memory allocation and device and host calls. Furthermore, understanding the GPU architecture and how parallel threads blocks are used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the CUDA programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the CUDA programming model with mentors\u2019 guidance later in the hands-on tutorial part.","title":"Introduction to GPU programming using CUDA"},{"location":"cuda/#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"cuda/#after-this-course-participants-will-be-able-to","text":"Understanding the GPU architecture (and also the difference between GPU and CPU) Streaming architecture Threads blocks Implement CUDA programming model Programming structure Device calls (threads block organisation) Host calls Efficient handling of memory management Host to Device Unified memory Apply the CUDA programming knowledge to accelerate examples from science and engineering Iterative solvers from science and engineering Matrix multiplication, vector addition, etc","title":"After this course, participants will be able to:"},{"location":"cuda/#prerequisites","text":"Priority will be given to users with good experience with C/C++ . No GPU programming knowledge is required. However, knowing some basic parallel programming concepts are advantage but not necessary.","title":"Prerequisites"},{"location":"cuda/#gpu-compute-resource","text":"Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide .","title":"GPU Compute Resource"},{"location":"cuda/exercise-1/","text":"Now our first exercise would be to print out the hello world from GPU. To do that, we need to do the following things: Run a part or entire application on the GPU Call cuda function on device It should be called using function qualifier __global__ Calling the device function on the main program: C/C++ example, c_function() CUDA example, cuda_function<<<1,1>>>() (just using 1 thread) <<< >>> , specify the threads blocks within the bracket Make sure to synchronize the threads __syncthreads() synchronizes all the threads within a thread block CudaDeviceSynchronize() synchronizes a kernel call in host Most of the CUDA APIs are synchronized calls by default (but sometimes it is good to call explicit synchronized calls to avoid errors in the computation) Questions and Solutions \u00b6 Examples: Hello World Serial-version CUDA-version //-*-C++-*- // Hello-world.c #include <stdio.h> #include <cuda.h> void c_function () { printf ( \"Hello World! \\n \" ); } int main () { c_function (); return 0 ; } //-*-C++-*- // Hello-world.cu #include <stdio.h> #include <cuda.h> // device function will be executed on device (GPU) __global__ void cuda_function () { printf ( \"Hello World from GPU! \\n \" ); // synchronize all the threads __syncthreads (); } int main () { // call the kernel function cuda_function <<< 1 , 1 >>> (); // synchronize the device kernel call cudaDeviceSynchronize (); return 0 ; } Compilation and Output Serial-version CUDA-version // compilation $ gcc Hello - world . c - o Hello - World - CPU // execution $ . / Hello - World - CPU // output $ Hello World from CPU ! // compilation $ nvcc - arch = compute_70 Hello - world . cu - o Hello - World - GPU // execution $ . / Hello - World - GPU // output $ Hello World from GPU ! Question Right now, you are printing just one Hello World from GPU , but what if you would like to print more Hello World from GPU ? How can you do that? Question Answer Solution Output //-*-C++-*- #include <stdio.h> #include <cuda.h> __global__ void cuda_function () { printf ( \"Hello World from GPU! \\n \" ); __syncthreads (); } int main () { // define your thread block here cuda_function <<<>>> (); cudaDeviceSynchronize (); return 0 ; } //-*-C++-*- #include <stdio.h> #include <cuda.h> __global__ void cuda_function () { printf ( \"Hello World from GPU! \\n \" ); __syncthreads (); } int main () { // define your thread block here cuda_function <<< 10 , 1 >>> (); cudaDeviceSynchronize (); return 0 ; } Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU !","title":"Hello World"},{"location":"cuda/exercise-1/#questions-and-solutions","text":"Examples: Hello World Serial-version CUDA-version //-*-C++-*- // Hello-world.c #include <stdio.h> #include <cuda.h> void c_function () { printf ( \"Hello World! \\n \" ); } int main () { c_function (); return 0 ; } //-*-C++-*- // Hello-world.cu #include <stdio.h> #include <cuda.h> // device function will be executed on device (GPU) __global__ void cuda_function () { printf ( \"Hello World from GPU! \\n \" ); // synchronize all the threads __syncthreads (); } int main () { // call the kernel function cuda_function <<< 1 , 1 >>> (); // synchronize the device kernel call cudaDeviceSynchronize (); return 0 ; } Compilation and Output Serial-version CUDA-version // compilation $ gcc Hello - world . c - o Hello - World - CPU // execution $ . / Hello - World - CPU // output $ Hello World from CPU ! // compilation $ nvcc - arch = compute_70 Hello - world . cu - o Hello - World - GPU // execution $ . / Hello - World - GPU // output $ Hello World from GPU ! Question Right now, you are printing just one Hello World from GPU , but what if you would like to print more Hello World from GPU ? How can you do that? Question Answer Solution Output //-*-C++-*- #include <stdio.h> #include <cuda.h> __global__ void cuda_function () { printf ( \"Hello World from GPU! \\n \" ); __syncthreads (); } int main () { // define your thread block here cuda_function <<<>>> (); cudaDeviceSynchronize (); return 0 ; } //-*-C++-*- #include <stdio.h> #include <cuda.h> __global__ void cuda_function () { printf ( \"Hello World from GPU! \\n \" ); __syncthreads (); } int main () { // define your thread block here cuda_function <<< 10 , 1 >>> (); cudaDeviceSynchronize (); return 0 ; } Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU ! Hello World from GPU !","title":"Questions and Solutions"},{"location":"cuda/exercise-2/","text":"In this example, we will continue with vector addition in GPU using the CUDA programming model. This is an excellent example to begin with because we usually need to do some arithmetic operations using matrices or vectors. For that, we need to know how to access the indexes of the matrix or vector to do the computation efficiently. In this example, we will practice SIMT computation by adding two vectors. Memory allocation on both CPU and GPU. Because as discussed before, GPU is an accelerator and can not act as a host machine .So therefore, the computation has to be initiated via CPU. That means, we need to first initialise the data on the host, that is CPU. At the same time, we also need to initialise the memory allocation on the GPU. Because, we need to transfer the data from a CPU to GPU. Allocating the CPU memory for a, b, and out vector // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); out = ( float * ) malloc ( sizeof ( float ) * N ); Allocating the GPU memory for d_a, d_b, and d_out matrix // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_out , sizeof ( float ) * N ); Now we need to fill the values for the arrays a and b. // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } Transfer initialized value from CPU to GPU // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * N , cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * N , cudaMemcpyHostToDevice ); Creating a 2D thread block // Thread organization dim3 dimGrid ( 1 , 1 , 1 ); dim3 dimBlock ( 16 , 16 , 1 ); Conversion of thread blocks //1D grid of 1D blocks __device__ int getGlobalIdx_1D_1D () { return blockIdx . x * blockDim . x + threadIdx . x ; } //1D grid of 2D blocks __device__ int getGlobalIdx_1D_2D () { return blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; } //1D grid of 3D blocks __device__ int getGlobalIdx_1D_3D () { return blockIdx . x * blockDim . x * blockDim . y * blockDim . z + threadIdx . z * blockDim . y * blockDim . x + threadIdx . y * blockDim . x + threadIdx . x ; } //2D grid of 1D blocks __device__ int getGlobalIdx_2D_1D () { int blockId = blockIdx . y * gridDim . x + blockIdx . x ; int threadId = blockId * blockDim . x + threadIdx . x ; return threadId ; } //2D grid of 2D blocks __device__ int getGlobalIdx_2D_2D () { int blockId = blockIdx . x + blockIdx . y * gridDim . x ; int threadId = blockId * ( blockDim . x * blockDim . y ) + ( threadIdx . y * blockDim . x ) + threadIdx . x ; return threadId ; } //2D grid of 3D blocks __device__ int getGlobalIdx_2D_3D () { int blockId = blockIdx . x + blockIdx . y * gridDim . x ; int threadId = blockId * ( blockDim . x * blockDim . y * blockDim . z ) + ( threadIdx . z * ( blockDim . x * blockDim . y )) + ( threadIdx . y * blockDim . x ) + threadIdx . x ; return threadId ; } //3D grid of 1D blocks __device__ int getGlobalIdx_3D_1D () { int blockId = blockIdx . x + blockIdx . y * gridDim . x + gridDim . x * gridDim . y * blockIdx . z ; int threadId = blockId * blockDim . x + threadIdx . x ; return threadId ; } //3D grid of 2D blocks __device__ int getGlobalIdx_3D_2D () { int blockId = blockIdx . x + blockIdx . y * gridDim . x + gridDim . x * gridDim . y * blockIdx . z ; int threadId = blockId * ( blockDim . x * blockDim . y ) + ( threadIdx . y * blockDim . x ) + threadIdx . x ; return threadId ; } //3D grid of 3D blocks __device__ int getGlobalIdx_3D_3D () { int blockId = blockIdx . x + blockIdx . y * gridDim . x + gridDim . x * gridDim . y * blockIdx . z ; int threadId = blockId * ( blockDim . x * blockDim . y * blockDim . z ) + ( threadIdx . z * ( blockDim . x * blockDim . y )) + ( threadIdx . y * blockDim . x ) + threadIdx . x ; return threadId ; Calling the kernel function // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); Vector addition kernel function call definition vector addition function call Serial-version CUDA-version // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * out , int n ) { for ( int i = 0 ; i < n ; i ++ ) { out [ i ] = a [ i ] + b [ i ]; } return out ; } // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronize all the threads __syncthreads (); } Copy back computed value from GPU to CPU // Transfer data back to host memory cudaMemcpy ( out , d_out , sizeof ( float ) * N , cudaMemcpyDeviceToHost ); Deallocate the host and device memory // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); // Deallocate host memory free ( a ); free ( b ); free ( out ); Questions and Solutions \u00b6 Examples: Vector Addition Serial-version CUDA-template CUDA-version //-*-C++-*- // Vector-addition.c #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * out , int n ) { for ( int i = 0 ; i < n ; i ++ ) { out [ i ] = a [ i ] + b [ i ]; } return out ; } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); out = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // Executing CPU function Vector_Add ( a , b , out , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate host memory free ( a ); free ( b ); free ( out ); return 0 ; } //-*-C++-*- // Vector-addition-template.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #include <cuda.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { // allign your thread id indexes int i = ........ // Allow the threads only within the size of N if ------ { out [ i ] = a [ i ] + b [ i ]; } // Synchronize all the threads } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * )...... // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a ,...... // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = .... b [ i ] = .... } // Transfer data from host to device memory cudaMemcpy ..... // Thread organization dim3 dimGrid .... dim3 dimBlock .... // execute the CUDA kernel function vector_add <<< >>> .... // Transfer data back to host memory cudaMemcpy .... // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ... // Deallocate host memory free .. return 0 ; } //-*-C++-*- // Vector-addition.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #include <cuda.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); out = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_out , sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * N , cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * N , cudaMemcpyHostToDevice ); // Thread organization dim3 dimGrid ( ceil ( N / 32 ), ceil ( N / 32 ), 1 ); dim3 dimBlock ( 32 , 32 , 1 ); // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); // Transfer data back to host memory cudaMemcpy ( out , d_out , sizeof ( float ) * N , cudaMemcpyDeviceToHost ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); // Deallocate host memory free ( a ); free ( b ); free ( out ); return 0 ; } Compilation and Output Serial-version CUDA-version // compilation $ gcc Vector - addition . c - o Vector - Addition - CPU // execution $ . / Vector - Addition - CPU // output $ . / Vector - addition - CPU out [ 0 ] = 3.000000 PASSED // compilation $ nvcc - arch = compute_70 Vector - addition . cu - o Vector - Addition - GPU // execution $ . / Vector - Addition - GPU // output $ . / Vector - addition - GPU out [ 0 ] = 3.000000 PASSED Questions What happens if you remove the __syncthreads(); from the __global__ void vector_add(float *a, float *b, float *out, int n) function. Can you remove the if condition if(i < n) from the __global__ void vector_add(float *a, float *b, float *out, int n) function. If so how can you do that? Here we do not use the cudaDeviceSynchronize() in the main application, can you figure out why we do not need to use it. Can you create a different kinds of threads block for larger number of array?","title":"Vector Addition"},{"location":"cuda/exercise-2/#questions-and-solutions","text":"Examples: Vector Addition Serial-version CUDA-template CUDA-version //-*-C++-*- // Vector-addition.c #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * out , int n ) { for ( int i = 0 ; i < n ; i ++ ) { out [ i ] = a [ i ] + b [ i ]; } return out ; } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); out = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // Executing CPU function Vector_Add ( a , b , out , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate host memory free ( a ); free ( b ); free ( out ); return 0 ; } //-*-C++-*- // Vector-addition-template.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #include <cuda.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { // allign your thread id indexes int i = ........ // Allow the threads only within the size of N if ------ { out [ i ] = a [ i ] + b [ i ]; } // Synchronize all the threads } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * )...... // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a ,...... // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = .... b [ i ] = .... } // Transfer data from host to device memory cudaMemcpy ..... // Thread organization dim3 dimGrid .... dim3 dimBlock .... // execute the CUDA kernel function vector_add <<< >>> .... // Transfer data back to host memory cudaMemcpy .... // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ... // Deallocate host memory free .. return 0 ; } //-*-C++-*- // Vector-addition.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #include <cuda.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); out = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_out , sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * N , cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * N , cudaMemcpyHostToDevice ); // Thread organization dim3 dimGrid ( ceil ( N / 32 ), ceil ( N / 32 ), 1 ); dim3 dimBlock ( 32 , 32 , 1 ); // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); // Transfer data back to host memory cudaMemcpy ( out , d_out , sizeof ( float ) * N , cudaMemcpyDeviceToHost ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); // Deallocate host memory free ( a ); free ( b ); free ( out ); return 0 ; } Compilation and Output Serial-version CUDA-version // compilation $ gcc Vector - addition . c - o Vector - Addition - CPU // execution $ . / Vector - Addition - CPU // output $ . / Vector - addition - CPU out [ 0 ] = 3.000000 PASSED // compilation $ nvcc - arch = compute_70 Vector - addition . cu - o Vector - Addition - GPU // execution $ . / Vector - Addition - GPU // output $ . / Vector - addition - GPU out [ 0 ] = 3.000000 PASSED Questions What happens if you remove the __syncthreads(); from the __global__ void vector_add(float *a, float *b, float *out, int n) function. Can you remove the if condition if(i < n) from the __global__ void vector_add(float *a, float *b, float *out, int n) function. If so how can you do that? Here we do not use the cudaDeviceSynchronize() in the main application, can you figure out why we do not need to use it. Can you create a different kinds of threads block for larger number of array?","title":"Questions and Solutions"},{"location":"cuda/exercise-3/","text":"We will now look into the basic matrix multiplication. In this example, we will perform the matrix multiplication. Matrix multiplication involves a nested loop. Again, most of the time, we might end up doing computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future. b Allocating the CPU memory for A, B, and C matrix. Here we notice that the matrix is stored in a 1D array because we want to consider the same function concept for CPU and GPU. // Initialize the memory on the host float * a , * b , * c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); Allocating the GPU memory for A, B, and C matrix // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); Now we need to fill the values for the matrix A and B. // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } Transfer initialized A and B matrix from CPU to GPU cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); 2D thread block for indexing x and y // Thread organization int blockSize = 32 ; dim3 dimBlock ( blockSize , blockSize , 1 ); dim3 dimGrid ( ceil ( N / float ( blockSize )), ceil ( N / float ( blockSize )), 1 ); Calling the kernel function // Device function call matrix_mul <<< dimGrid , dimBlock >>> ( d_a , d_b , d_c , N ); ??? \"matrix multiplication function call\" === \"serial\" ```c float * matrix_mul(float *h_a, float *h_b, float *h_c, int width) { for(int row = 0; row < width ; ++row) { for(int col = 0; col < width ; ++col) { float temp = 0; for(int i = 0; i < width ; ++i) { temp += h_a[row*width+i] * h_b[i*width+col]; } h_c[row*width+col] = temp; } } return h_c; } ``` === \"cuda\" ```c __global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width) { int row = blockIdx.x * blockDim.x + threadIdx.x; int col = blockIdx.y * blockDim.y + threadIdx.y; if ((row < width) && (col < width)) { float temp = 0; // each thread computes one // element of the block sub-matrix for (int i = 0; i < width; ++i) { temp += d_a[row*width+i]*d_b[i*width+col]; } d_c[row*width+col] = temp; } } ``` Copy back computed value from GPU to CPU; transfer the data back to GPU (from device to host). Here is the C matrix that contains the product of the two matrices. // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); Deallocate the host and device memory // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); Questions and Solutions \u00b6 Examples: Matrix Multiplication Serial-version CUDA-template CUDA-version //-*-C++-*- // Matrix-multiplication.c #include <iostream> #include <cuda.h> using namespace std ; float * matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float temp = 0 ; for ( int i = 0 ; i < width ; ++ i ) { temp += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = temp ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix (square matrix )size is N*N \" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Device function call matrix_mul ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << c [ j ] << \" \" ; } cout << \" \" << endl ; } // Deallocate host memory free ( a ); free ( b ); free ( c ); return 0 ; } //-*-C++-*- // Matrix-multiplication-template.cu #include <iostream> #include <cuda.h> using namespace std ; __global__ void matrix_mul ( float * d_a , float * d_b , float * d_c , int width ) { // create a 2d threads block int row = .................. int col = .................... // only allow the threads that are needed for the computation if (................................) { float temp = 0 ; // each thread computes one // element of the block sub-matrix for ( int i = 0 ; i < width ; ++ i ) { temp += d_a [ row * width + i ] * d_b [ i * width + col ]; } d_c [ row * width + col ] = temp ; } } // Host call (matrix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float single_entry = 0 ; for ( int i = 0 ; i < width ; ++ i ) { single_entry += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = single_entry ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix (square matrix) size is N*N \" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); ... ... // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); ... ... // Transfer data from host to device memory cudaMemcpy (.........................); cudaMemcpy (.........................); // Thread organization int blockSize = ..............; dim3 dimBlock (......................); dim3 dimGrid (.......................); // Device function call matrix_mul <<< dimGrid , dimBlock >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // CPU computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"Two matrices are not equal\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ... // Deallocate host memory free ... return 0 ; } //-*-C++-*- // Matrix-multiplication.cu #include <iostream> #include <cuda.h> using namespace std ; __global__ void matrix_mul ( float * d_a , float * d_b , float * d_c , int width ) { int row = blockIdx . x * blockDim . x + threadIdx . x ; int col = blockIdx . y * blockDim . y + threadIdx . y ; if (( row < width ) && ( col < width )) { float temp = 0 ; // each thread computes one // element of the block sub-matrix for ( int i = 0 ; i < width ; ++ i ) { temp += d_a [ row * width + i ] * d_b [ i * width + col ]; } d_c [ row * width + col ] = temp ; } } // Host call (matrix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float single_entry = 0 ; for ( int i = 0 ; i < width ; ++ i ) { single_entry += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = single_entry ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix (square matrix) size is N*N \" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); host_check = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); // Thread organization int blockSize = 32 ; dim3 dimBlock ( blockSize , blockSize , 1 ); dim3 dimGrid ( ceil ( N / float ( blockSize )), ceil ( N / float ( blockSize )), 1 ); // Device function call matrix_mul <<< dimGrid , dimBlock >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // cpu computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"Two matrices are not equal\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); free ( host_check ); return 0 ; } Compilation and Output Serial-version CUDA-version // compilation $ gcc Matrix - multiplication . c - o Matrix - Multiplication - CPU // execution $ . / Matrix - Multiplication - CPU // output $ g ++ Matrix - multiplication . cc - o Matrix - multiplication $ . / Matrix - multiplication Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 // compilation $ nvcc - arch = compute_70 Matrix - multiplication . cu - o Matrix - Multiplication - GPU // execution $ . / Matrix - Multiplication - GPU Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number $ 256 // output $ Two matrices are equal Questions Right now, we are using the 1D array to represent the matrix. However, you can also do it with the 2D matrix. Can you try with 2D array matrix multiplication with 2D thread block? Can you get the correct soltion if you remove the if ((row < width) && (col < width)) condition from the __global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width) function? Please try with different thread blocks and different matrix sizes. // Thread organization int blockSize = 32; dim3 dimBlock(blockSize,blockSize,1); dim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);","title":"Matrix Multiplication"},{"location":"cuda/exercise-3/#questions-and-solutions","text":"Examples: Matrix Multiplication Serial-version CUDA-template CUDA-version //-*-C++-*- // Matrix-multiplication.c #include <iostream> #include <cuda.h> using namespace std ; float * matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float temp = 0 ; for ( int i = 0 ; i < width ; ++ i ) { temp += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = temp ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix (square matrix )size is N*N \" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Device function call matrix_mul ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << c [ j ] << \" \" ; } cout << \" \" << endl ; } // Deallocate host memory free ( a ); free ( b ); free ( c ); return 0 ; } //-*-C++-*- // Matrix-multiplication-template.cu #include <iostream> #include <cuda.h> using namespace std ; __global__ void matrix_mul ( float * d_a , float * d_b , float * d_c , int width ) { // create a 2d threads block int row = .................. int col = .................... // only allow the threads that are needed for the computation if (................................) { float temp = 0 ; // each thread computes one // element of the block sub-matrix for ( int i = 0 ; i < width ; ++ i ) { temp += d_a [ row * width + i ] * d_b [ i * width + col ]; } d_c [ row * width + col ] = temp ; } } // Host call (matrix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float single_entry = 0 ; for ( int i = 0 ; i < width ; ++ i ) { single_entry += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = single_entry ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix (square matrix) size is N*N \" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); ... ... // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); ... ... // Transfer data from host to device memory cudaMemcpy (.........................); cudaMemcpy (.........................); // Thread organization int blockSize = ..............; dim3 dimBlock (......................); dim3 dimGrid (.......................); // Device function call matrix_mul <<< dimGrid , dimBlock >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // CPU computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"Two matrices are not equal\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ... // Deallocate host memory free ... return 0 ; } //-*-C++-*- // Matrix-multiplication.cu #include <iostream> #include <cuda.h> using namespace std ; __global__ void matrix_mul ( float * d_a , float * d_b , float * d_c , int width ) { int row = blockIdx . x * blockDim . x + threadIdx . x ; int col = blockIdx . y * blockDim . y + threadIdx . y ; if (( row < width ) && ( col < width )) { float temp = 0 ; // each thread computes one // element of the block sub-matrix for ( int i = 0 ; i < width ; ++ i ) { temp += d_a [ row * width + i ] * d_b [ i * width + col ]; } d_c [ row * width + col ] = temp ; } } // Host call (matrix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float single_entry = 0 ; for ( int i = 0 ; i < width ; ++ i ) { single_entry += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = single_entry ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix (square matrix) size is N*N \" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); host_check = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host matrix for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); // Thread organization int blockSize = 32 ; dim3 dimBlock ( blockSize , blockSize , 1 ); dim3 dimGrid ( ceil ( N / float ( blockSize )), ceil ( N / float ( blockSize )), 1 ); // Device function call matrix_mul <<< dimGrid , dimBlock >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // cpu computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"Two matrices are not equal\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); free ( host_check ); return 0 ; } Compilation and Output Serial-version CUDA-version // compilation $ gcc Matrix - multiplication . c - o Matrix - Multiplication - CPU // execution $ . / Matrix - Multiplication - CPU // output $ g ++ Matrix - multiplication . cc - o Matrix - multiplication $ . / Matrix - multiplication Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 // compilation $ nvcc - arch = compute_70 Matrix - multiplication . cu - o Matrix - Multiplication - GPU // execution $ . / Matrix - Multiplication - GPU Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number $ 256 // output $ Two matrices are equal Questions Right now, we are using the 1D array to represent the matrix. However, you can also do it with the 2D matrix. Can you try with 2D array matrix multiplication with 2D thread block? Can you get the correct soltion if you remove the if ((row < width) && (col < width)) condition from the __global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width) function? Please try with different thread blocks and different matrix sizes. // Thread organization int blockSize = 32; dim3 dimBlock(blockSize,blockSize,1); dim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);","title":"Questions and Solutions"},{"location":"cuda/exercise-4/","text":"In this example, we try shared memory matrix multiplication. This is achieved by blocking the global matrix into a small block matrix (tiled matrix) that can fit into the shared memory of the Nvidia GPU. Shared memory from the GPUs, which has a good bandwidth within the GPUs compared to access to the global memory. This is very similar to the previous example; however, we just need to allocate the small block matrix into shared memory. The below example shows the blocking size for a and b matrix respectively for gobal A and B matrix. // Shared memory allocation for the block matrix __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE]; __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE]; Then we need to iterate elements within the block size and, finally with the global index. These can be achieved with CUDA threads. You can also increase the shared memory or L1 cache size by using cudaFuncSetCacheConfig . For more information about CUDA API, please refer to cudaFuncSetCacheConfig . Tips cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1); //cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared); cudaFuncCachePreferNone: no preference for shared memory or L1 (default) cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory // simple example usage increasing more shared memory #include<stdio.h> int main() { // example of increasing the shared memory cudaDeviceSetCacheConfig(My_Kernel, cudaFuncCachePreferShared); My_Kernel<<<>>>(); cudaDeviceSynchronize(); return 0; } Different Nvidia GPUs provides different configuration, for example, Ampere GA102 GPU Architecture, will support the following configuration: 128 KB L1 + 0 KB Shared Memory 120 KB L1 + 8 KB Shared Memory 112 KB L1 + 16 KB Shared Memory 96 KB L1 + 32 KB Shared Memory 64 KB L1 + 64 KB Shared Memory 28 KB L1 + 100 KB Shared Memory Questions and Solutions \u00b6 Example: Shared Memory - Matrix Multiplication Matrix-multiplication-shared-template Matrix-multiplication-shared.cu // Matrix-multiplication-shared-template.cu //-*-C++-*- #include <iostream> #include <cuda.h> // block size for the matrix #define BLOCK_SIZE 16 using namespace std ; // Device call (matrix multiplication) __global__ void matrix_mul ( const float * d_a , const float * d_b , float * d_c , int width ) { // Shared memory allocation for the block matrix __shared__ int a_block [ BLOCK_SIZE ][ BLOCK_SIZE ]; ... // Indexing for the block matrix int tx = threadIdx . x ; ... // Indexing global matrix to block matrix int row = threadIdx . x + blockDim . x * blockIdx . x ; ... // Allow threads only for size of rows and columns (we assume square matrix) if (( row < width ) && ( col < width )) { // Save temporary value for the particular index float temp = 0 ; for ( int i = 0 ; i < width / BLOCK_SIZE ; ++ i ) { // Allign the global matrix to block matrix a_block [ ty ][ tx ] = d_a [ row * width + ( i * BLOCK_SIZE + tx )]; b_block [ ty ][ tx ] = d_b [( i * BLOCK_SIZE + ty ) * width + col ]; // Make sure all the threads are synchronized .... // Multiply the block matrix for ( int j = 0 ; j < BLOCK_SIZE ; ++ j ) { temp += a_block [ ty ][ j ] * b_block [ j ][ tx ]; } // Make sure all the threads are synchronized ... } // Save block matrix entry to global matrix ... } } // Host call (matrix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float temp = 0 ; for ( int i = 0 ; i < width ; ++ i ) { temp += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = temp ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix size is N*N \" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); host_check = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_c , c , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); // Thread organization dim3 Block_dim ( BLOCK_SIZE , BLOCK_SIZE , 1 ); ... // Device function call matrix_mul <<< Grid_dim , Block_dim >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // Cpu computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"But,two matrices are not equal\" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); free ( host_check ); return 0 ; } // Matrix-multiplication-shared.cu //-*-C++-*- #include <iostream> #include <cuda.h> // block size for the matrix #define BLOCK_SIZE 16 using namespace std ; // Device call (matrix multiplication) __global__ void matrix_mul ( const float * d_a , const float * d_b , float * d_c , int width ) { // Shared memory allocation for the block matrix __shared__ int a_block [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ int b_block [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Indexing for the block matrix int tx = threadIdx . x ; int ty = threadIdx . y ; // Indexing global matrix to block matrix int row = threadIdx . x + blockDim . x * blockIdx . x ; int col = threadIdx . y + blockDim . y * blockIdx . y ; // Allow threads only for size of rows and columns (we assume square matrix) if (( row < width ) && ( col < width )) { // Save temporary value for the particular index float temp = 0 ; for ( int i = 0 ; i < width / BLOCK_SIZE ; ++ i ) { // Allign the global matrix to block matrix a_block [ ty ][ tx ] = d_a [ row * width + ( i * BLOCK_SIZE + tx )]; b_block [ ty ][ tx ] = d_b [( i * BLOCK_SIZE + ty ) * width + col ]; // Make sure all the threads are synchronized __syncthreads (); // Multiply the block matrix for ( int j = 0 ; j < BLOCK_SIZE ; ++ j ) { temp += a_block [ ty ][ j ] * b_block [ j ][ tx ]; } __syncthreads (); } // Save block matrix entry to global matrix d_c [ row * width + col ] = temp ; } } // Host call (matix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float single_entry = 0 ; for ( int i = 0 ; i < width ; ++ i ) { single_entry += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = single_entry ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix size is N*N \" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); host_check = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_c , c , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); // Thread organization dim3 Block_dim ( BLOCK_SIZE , BLOCK_SIZE , 1 ); dim3 Grid_dim ( ceil ( N / BLOCK_SIZE ), ceil ( N / BLOCK_SIZE ), 1 ); // Device function call matrix_mul <<< Grid_dim , Block_dim >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // cpu computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"But,two matrices are not equal\" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); free ( host_check ); return 0 ; } Compilation and Output CUDA-version // compilation $ nvcc - arch = sm_70 Matrix - multiplication - shared . cu - o Matrix - multiplication - shared // execution $ . / Matrix - multiplication - shared Programme assumes that matrix size is N * N Matrix dimensions are assumed to be multiples of BLOCK_SIZE = 16 Please enter the N size number $ 256 // output $ Two matrices are equal Questions Could you resize the BLOCK_SIZE number and check the solution's correctness? Can you also create a different kind of thread block and matrix size and check the solution's correctness? Please try with cudaFuncSetCacheConfig and check if you can successfully execute the application.","title":"Shared Memory"},{"location":"cuda/exercise-4/#questions-and-solutions","text":"Example: Shared Memory - Matrix Multiplication Matrix-multiplication-shared-template Matrix-multiplication-shared.cu // Matrix-multiplication-shared-template.cu //-*-C++-*- #include <iostream> #include <cuda.h> // block size for the matrix #define BLOCK_SIZE 16 using namespace std ; // Device call (matrix multiplication) __global__ void matrix_mul ( const float * d_a , const float * d_b , float * d_c , int width ) { // Shared memory allocation for the block matrix __shared__ int a_block [ BLOCK_SIZE ][ BLOCK_SIZE ]; ... // Indexing for the block matrix int tx = threadIdx . x ; ... // Indexing global matrix to block matrix int row = threadIdx . x + blockDim . x * blockIdx . x ; ... // Allow threads only for size of rows and columns (we assume square matrix) if (( row < width ) && ( col < width )) { // Save temporary value for the particular index float temp = 0 ; for ( int i = 0 ; i < width / BLOCK_SIZE ; ++ i ) { // Allign the global matrix to block matrix a_block [ ty ][ tx ] = d_a [ row * width + ( i * BLOCK_SIZE + tx )]; b_block [ ty ][ tx ] = d_b [( i * BLOCK_SIZE + ty ) * width + col ]; // Make sure all the threads are synchronized .... // Multiply the block matrix for ( int j = 0 ; j < BLOCK_SIZE ; ++ j ) { temp += a_block [ ty ][ j ] * b_block [ j ][ tx ]; } // Make sure all the threads are synchronized ... } // Save block matrix entry to global matrix ... } } // Host call (matrix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float temp = 0 ; for ( int i = 0 ; i < width ; ++ i ) { temp += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = temp ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix size is N*N \" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); host_check = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_c , c , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); // Thread organization dim3 Block_dim ( BLOCK_SIZE , BLOCK_SIZE , 1 ); ... // Device function call matrix_mul <<< Grid_dim , Block_dim >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // Cpu computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"But,two matrices are not equal\" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); free ( host_check ); return 0 ; } // Matrix-multiplication-shared.cu //-*-C++-*- #include <iostream> #include <cuda.h> // block size for the matrix #define BLOCK_SIZE 16 using namespace std ; // Device call (matrix multiplication) __global__ void matrix_mul ( const float * d_a , const float * d_b , float * d_c , int width ) { // Shared memory allocation for the block matrix __shared__ int a_block [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ int b_block [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Indexing for the block matrix int tx = threadIdx . x ; int ty = threadIdx . y ; // Indexing global matrix to block matrix int row = threadIdx . x + blockDim . x * blockIdx . x ; int col = threadIdx . y + blockDim . y * blockIdx . y ; // Allow threads only for size of rows and columns (we assume square matrix) if (( row < width ) && ( col < width )) { // Save temporary value for the particular index float temp = 0 ; for ( int i = 0 ; i < width / BLOCK_SIZE ; ++ i ) { // Allign the global matrix to block matrix a_block [ ty ][ tx ] = d_a [ row * width + ( i * BLOCK_SIZE + tx )]; b_block [ ty ][ tx ] = d_b [( i * BLOCK_SIZE + ty ) * width + col ]; // Make sure all the threads are synchronized __syncthreads (); // Multiply the block matrix for ( int j = 0 ; j < BLOCK_SIZE ; ++ j ) { temp += a_block [ ty ][ j ] * b_block [ j ][ tx ]; } __syncthreads (); } // Save block matrix entry to global matrix d_c [ row * width + col ] = temp ; } } // Host call (matix multiplication) float * cpu_matrix_mul ( float * h_a , float * h_b , float * h_c , int width ) { for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { float single_entry = 0 ; for ( int i = 0 ; i < width ; ++ i ) { single_entry += h_a [ row * width + i ] * h_b [ i * width + col ]; } h_c [ row * width + col ] = single_entry ; } } return h_c ; } int main () { cout << \"Programme assumes that matrix size is N*N \" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; cout << \"Please enter the N size number \" << endl ; int N = 0 ; cin >> N ; // Initialize the memory on the host float * a , * b , * c , * host_check ; // Initialize the memory on the device float * d_a , * d_b , * d_c ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); host_check = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize host arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 2.0f ; b [ i ] = 2.0f ; } // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * ( N * N )); cudaMalloc (( void ** ) & d_c , sizeof ( float ) * ( N * N )); // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); cudaMemcpy ( d_c , c , sizeof ( float ) * ( N * N ), cudaMemcpyHostToDevice ); // Thread organization dim3 Block_dim ( BLOCK_SIZE , BLOCK_SIZE , 1 ); dim3 Grid_dim ( ceil ( N / BLOCK_SIZE ), ceil ( N / BLOCK_SIZE ), 1 ); // Device function call matrix_mul <<< Grid_dim , Block_dim >>> ( d_a , d_b , d_c , N ); // Transfer data back to host memory cudaMemcpy ( c , d_c , sizeof ( float ) * ( N * N ), cudaMemcpyDeviceToHost ); // cpu computation for verification cpu_matrix_mul ( a , b , host_check , N ); // Verification bool flag = 1 ; for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { if ( c [ j * N + i ] != host_check [ j * N + i ]) { flag = 0 ; break ; } } } if ( flag == 0 ) { cout << \"But,two matrices are not equal\" << endl ; cout << \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" << endl ; } else cout << \"Two matrices are equal\" << endl ; // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_c ); // Deallocate host memory free ( a ); free ( b ); free ( c ); free ( host_check ); return 0 ; } Compilation and Output CUDA-version // compilation $ nvcc - arch = sm_70 Matrix - multiplication - shared . cu - o Matrix - multiplication - shared // execution $ . / Matrix - multiplication - shared Programme assumes that matrix size is N * N Matrix dimensions are assumed to be multiples of BLOCK_SIZE = 16 Please enter the N size number $ 256 // output $ Two matrices are equal Questions Could you resize the BLOCK_SIZE number and check the solution's correctness? Can you also create a different kind of thread block and matrix size and check the solution's correctness? Please try with cudaFuncSetCacheConfig and check if you can successfully execute the application.","title":"Questions and Solutions"},{"location":"cuda/exercise-5/","text":"Unified memory simplifies the explicit data movement from host to device by programmers. CUDA API will manage the data transfer between CPU and GPU. In this example, we will look into vector addition in GPU using the unified memory concept. Just one memory allocation is enough cudaMallocManaged() . The blow table summerise the required steps needed for the unified memory concept. Without unified memory With unified memory Allocate the host memory Allocate the host memory Allocate the device memory Allocate the device memory Initialize the host value Initialize the host value Transfer the host value to the device memory location Transfer the host value to the device memory location Do the computation using the CUDA kernel Do the computation using the CUDA kernel Transfer the data from the device to host Transfer the data from the device to host Free device memory Free device memory Free host memory Free host memory Questions and Solutions \u00b6 Examples: Unified Memory - Vector Addition Without Unified Memory With Unified Memory - template With Unified Memory-version //-*-C++-*- // Without-unified-memory.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_out , sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * N , cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * N , cudaMemcpyHostToDevice ); // Thread organization dim3 dimGrid ( ceil ( N / 32 ), ceil ( N / 32 ), 1 ); dim3 dimBlock ( 32 , 32 , 1 ); // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); // Transfer data back to host memory cudaMemcpy ( out , d_out , sizeof ( float ) * N , cudaMemcpyDeviceToHost ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); // Deallocate host memory free ( a ); free ( b ); free ( out ); return 0 ; } //-*-C++-*- #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { /* // Initialize the memory on the host float *a, *b, *out; // Allocate host memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); c = (float*)malloc(sizeof(float) * N); */ // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device(unified) memory cudaMallocManaged ...... // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { d_a [ i ] = ... d_b [ i ] = ... } /* // Transfer data from host to device memory cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); */ // Thread organization dim3 dimGrid ... dim3 dimBlock ... // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); // synchronize if needed ...... /* // Transfer data back to host memory cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); */ // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( d_out [ i ] - d_a [ i ] - d_b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , d_out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device(unified) memory cudaFree ... /* // Deallocate host memory free(a); free(b); free(out); */ return 0 ; } //-*-C++-*- // With-unified-memory.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { /* // Initialize the memory on the host float *a, *b, *out; // Allocate host memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); c = (float*)malloc(sizeof(float) * N); */ // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMallocManaged ( & d_a , sizeof ( float ) * N ); cudaMallocManaged ( & d_b , sizeof ( float ) * N ); cudaMallocManaged ( & d_out , sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { d_a [ i ] = 1.0f ; d_b [ i ] = 2.0f ; } /* // Transfer data from host to device memory cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); */ // Thread organization dim3 dimGrid ( ceil ( N / 32 ), ceil ( N / 32 ), 1 ); dim3 dimBlock ( 32 , 32 , 1 ); // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); cudaDeviceSynchronize (); /* // Transfer data back to host memory cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); */ // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( d_out [ i ] - d_a [ i ] - d_b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , d_out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); /* // Deallocate host memory free(a); free(b); free(out); */ return 0 ; } Compilation and Output Without-unified-memory.cu With-unified-memory // compilation $ nvcc - arch = compute_70 Without - unified - memory . cu - o Without - Unified - Memory // execution $ . / Without - Unified - Memory // output $ . / Without - Unified - Memory out [ 0 ] = 3.000000 PASSED // compilation $ nvcc - arch = compute_70 With - unified - memory . cu - o With - Unified - Memory // execution $ . / With - Unified - Memory // output $ . / With - Unified - Memory out [ 0 ] = 3.000000 PASSED Questions Here in this example, we have used cudaDeviceSynchronize() ; can you remove cudaDeviceSynchronize() and still get a correct solution? if not, why (think)? Please try with different thread blocks and array sizes.","title":"Unified Memory"},{"location":"cuda/exercise-5/#questions-and-solutions","text":"Examples: Unified Memory - Vector Addition Without Unified Memory With Unified Memory - template With Unified Memory-version //-*-C++-*- // Without-unified-memory.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { // Initialize the memory on the host float * a , * b , * out ; // Allocate host memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMalloc (( void ** ) & d_a , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_b , sizeof ( float ) * N ); cudaMalloc (( void ** ) & d_out , sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Transfer data from host to device memory cudaMemcpy ( d_a , a , sizeof ( float ) * N , cudaMemcpyHostToDevice ); cudaMemcpy ( d_b , b , sizeof ( float ) * N , cudaMemcpyHostToDevice ); // Thread organization dim3 dimGrid ( ceil ( N / 32 ), ceil ( N / 32 ), 1 ); dim3 dimBlock ( 32 , 32 , 1 ); // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); // Transfer data back to host memory cudaMemcpy ( out , d_out , sizeof ( float ) * N , cudaMemcpyDeviceToHost ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( out [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); // Deallocate host memory free ( a ); free ( b ); free ( out ); return 0 ; } //-*-C++-*- #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { /* // Initialize the memory on the host float *a, *b, *out; // Allocate host memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); c = (float*)malloc(sizeof(float) * N); */ // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device(unified) memory cudaMallocManaged ...... // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { d_a [ i ] = ... d_b [ i ] = ... } /* // Transfer data from host to device memory cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); */ // Thread organization dim3 dimGrid ... dim3 dimBlock ... // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); // synchronize if needed ...... /* // Transfer data back to host memory cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); */ // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( d_out [ i ] - d_a [ i ] - d_b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , d_out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device(unified) memory cudaFree ... /* // Deallocate host memory free(a); free(b); free(out); */ return 0 ; } //-*-C++-*- // With-unified-memory.cu #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // GPU function that adds two vectors __global__ void vector_add ( float * a , float * b , float * out , int n ) { int i = blockIdx . x * blockDim . x * blockDim . y + threadIdx . y * blockDim . x + threadIdx . x ; // Allow the threads only within the size of N if ( i < n ) { out [ i ] = a [ i ] + b [ i ]; } // Synchronice all the threads __syncthreads (); } int main () { /* // Initialize the memory on the host float *a, *b, *out; // Allocate host memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); c = (float*)malloc(sizeof(float) * N); */ // Initialize the memory on the device float * d_a , * d_b , * d_out ; // Allocate device memory cudaMallocManaged ( & d_a , sizeof ( float ) * N ); cudaMallocManaged ( & d_b , sizeof ( float ) * N ); cudaMallocManaged ( & d_out , sizeof ( float ) * N ); // Initialize host arrays for ( int i = 0 ; i < N ; i ++ ) { d_a [ i ] = 1.0f ; d_b [ i ] = 2.0f ; } /* // Transfer data from host to device memory cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); */ // Thread organization dim3 dimGrid ( ceil ( N / 32 ), ceil ( N / 32 ), 1 ); dim3 dimBlock ( 32 , 32 , 1 ); // execute the CUDA kernel function vector_add <<< dimGrid , dimBlock >>> ( d_a , d_b , d_out , N ); cudaDeviceSynchronize (); /* // Transfer data back to host memory cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); */ // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( d_out [ i ] - d_a [ i ] - d_b [ i ]) < MAX_ERR ); } printf ( \"out[0] = %f \\n \" , d_out [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate device memory cudaFree ( d_a ); cudaFree ( d_b ); cudaFree ( d_out ); /* // Deallocate host memory free(a); free(b); free(out); */ return 0 ; } Compilation and Output Without-unified-memory.cu With-unified-memory // compilation $ nvcc - arch = compute_70 Without - unified - memory . cu - o Without - Unified - Memory // execution $ . / Without - Unified - Memory // output $ . / Without - Unified - Memory out [ 0 ] = 3.000000 PASSED // compilation $ nvcc - arch = compute_70 With - unified - memory . cu - o With - Unified - Memory // execution $ . / With - Unified - Memory // output $ . / With - Unified - Memory out [ 0 ] = 3.000000 PASSED Questions Here in this example, we have used cudaDeviceSynchronize() ; can you remove cudaDeviceSynchronize() and still get a correct solution? if not, why (think)? Please try with different thread blocks and array sizes.","title":"Questions and Solutions"},{"location":"cuda/preparation/","text":"1. How to login to MeluXina machine \u00b6 1.1 Please take a look if you are using Windows 1.2 Please take a look if you are using Linux/Mac 2. Use your username to connect to MeluXina \u00b6 2.1 For example the below example shows the user of u100490 $ ssh u100490@login.lxp.lu -p 8822 ### or $ ssh meluxina 3. Once you have logged in \u00b6 3.1 Once you have logged in, you will be in a default home directory [u100490@login02 ~]$ pwd /home/users/u100490 3.2 After that, go to the project directory. [u100490@login02 ~]$ cd /project/home/p200117 [u100490@login02 p200117]$ pwd /project/home/p200117 4. And please create your own working folder under the project directory \u00b6 4.1 For example, here is the user with u100490 : [u100490@login02 p200117]$ mkdir $USER ### or [u100490@login02 p200117]$ mkdir u100490 5. Now it is time to move into your home directory \u00b6 5.1 For example, with user home directory u100490 [u100490@login02 p200117]$cd u100490 6. Now it is time to copy the folder which has examples and source files to your home directory \u00b6 6.1 For example, with user home directory u100490 [u100490@login03 u100490]$ cp -r /project/home/p200117/CUDA . [u100490@login03 u100490]$ cd CUDA/ [u100490@login03 CUDA]$ pwd /project/home/p200117/u100490/CUDA [u100490@login03 CUDA]$ ls -lthr total 20K -rw-r-----. 1 u100490 p200117 51 Mar 13 15:50 module.sh drwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition drwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory ... ... 7. Until now you are in the login node, now its time to do the dry run test \u00b6 7.1 Reserve the interactive node for running/testing CUDA applications $ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job 7.2 You can also check if you got the interactive node for your computations, for example, here with the user u100490 : [u100490@mel2131 ~]$ squeue -u u100490 JOBID PARTITION NAME USER ACCOUNT STATE TIME TIME_LIMIT NODES NODELIST(REASON) 304381 gpu interact u100490 p200117 RUNNING 0:37 01:00:00 1 mel2131 8. Now we need to check simple CUDA application, if that is going to work for you: \u00b6 8.1 Go to folder Dry-run-test [u100490@login03 CUDA]$ cd Dry-run-test/ [u100490@login03 Dry-run-test]$ ls Hello-world.cu module.sh 9. Finally, we need to load the compiler to test the GPU CUDA codes \u00b6 9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code $ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 ### or $ source module.sh check if the module is loaded properly [u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 [u100490@mel2131 ~]$ module list Currently Loaded Modules: 1) env/release/2022.1 (S) 6) numactl/2.0.14-GCCcore-11.3.0 11) libpciaccess/0.16-GCCcore-11.3.0 16) GDRCopy/2.3-GCCcore-11.3.0 21) knem/1.1.4.90-GCCcore-11.3.0 2) lxp-tools/myquota/0.3.1 (S) 7) CUDA/11.7.0 12) hwloc/2.7.1-GCCcore-11.3.0 17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0 22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 3) GCCcore/11.3.0 8) NVHPC/22.7-CUDA-11.7.0 13) OpenSSL/1.1 18) libfabric/1.15.1-GCCcore-11.3.0 4) zlib/1.2.12-GCCcore-11.3.0 9) XZ/5.2.5-GCCcore-11.3.0 14) libevent/2.1.12-GCCcore-11.3.0 19) PMIx/4.2.2-GCCcore-11.3.0 5) binutils/2.38-GCCcore-11.3.0 10) libxml2/2.9.13-GCCcore-11.3.0 15) UCX/1.13.1-GCCcore-11.3.0 20) xpmem/2.6.5-36-GCCcore-11.3.0 Where: S: Module is Sticky, requires --force to unload or purge 10. Please compile and test your CUDA application \u00b6 For example, Dry-run-test // compilation $ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU // execution $ ./Hello-World-GPU // output $ Hello World from GPU! Hello World from GPU! Hello World from GPU! Hello World from GPU! 11. Similarly for the hands-on session, we need to do the node reservation: \u00b6 $ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job 12. We will continue with our Hands on exercise \u00b6 12.1 For example, Hello World example, we do the following steps: [u100490@mel2063 CUDA]$ pwd /project/home/p200117/u100490/CUDA [u100490@mel2063 CUDA]$ ls [u100490@mel2063 CUDA]$ ls Dry-run-test Matrix-multiplication Profiling Unified-memory Hello-world module.sh Shared-memory Vector-addition [u100490@mel2063 CUDA]$ source module.sh [u100490@mel2063 CUDA]$ cd Hello-world // compilation [u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU // execution [u100490@mel2063 CUDA]$ ./Hello-World-GPU // output [u100490@mel2063 CUDA]$ Hello World from GPU","title":"Preparation"},{"location":"cuda/preparation/#1-how-to-login-to-meluxina-machine","text":"1.1 Please take a look if you are using Windows 1.2 Please take a look if you are using Linux/Mac","title":"1. How to login to MeluXina machine"},{"location":"cuda/preparation/#2-use-your-username-to-connect-to-meluxina","text":"2.1 For example the below example shows the user of u100490 $ ssh u100490@login.lxp.lu -p 8822 ### or $ ssh meluxina","title":"2. Use your username to connect to MeluXina"},{"location":"cuda/preparation/#3-once-you-have-logged-in","text":"3.1 Once you have logged in, you will be in a default home directory [u100490@login02 ~]$ pwd /home/users/u100490 3.2 After that, go to the project directory. [u100490@login02 ~]$ cd /project/home/p200117 [u100490@login02 p200117]$ pwd /project/home/p200117","title":"3. Once you have logged in"},{"location":"cuda/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","text":"4.1 For example, here is the user with u100490 : [u100490@login02 p200117]$ mkdir $USER ### or [u100490@login02 p200117]$ mkdir u100490","title":"4. And please create your own working folder under the project directory"},{"location":"cuda/preparation/#5-now-it-is-time-to-move-into-your-home-directory","text":"5.1 For example, with user home directory u100490 [u100490@login02 p200117]$cd u100490","title":"5. Now it is time to move into your home directory"},{"location":"cuda/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","text":"6.1 For example, with user home directory u100490 [u100490@login03 u100490]$ cp -r /project/home/p200117/CUDA . [u100490@login03 u100490]$ cd CUDA/ [u100490@login03 CUDA]$ pwd /project/home/p200117/u100490/CUDA [u100490@login03 CUDA]$ ls -lthr total 20K -rw-r-----. 1 u100490 p200117 51 Mar 13 15:50 module.sh drwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition drwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory ... ...","title":"6. Now it is time to copy the folder which has examples and source files to your home directory"},{"location":"cuda/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","text":"7.1 Reserve the interactive node for running/testing CUDA applications $ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job 7.2 You can also check if you got the interactive node for your computations, for example, here with the user u100490 : [u100490@mel2131 ~]$ squeue -u u100490 JOBID PARTITION NAME USER ACCOUNT STATE TIME TIME_LIMIT NODES NODELIST(REASON) 304381 gpu interact u100490 p200117 RUNNING 0:37 01:00:00 1 mel2131","title":"7. Until now you are in the login node, now its time to do the dry run test"},{"location":"cuda/preparation/#8-now-we-need-to-check-simple-cuda-application-if-that-is-going-to-work-for-you","text":"8.1 Go to folder Dry-run-test [u100490@login03 CUDA]$ cd Dry-run-test/ [u100490@login03 Dry-run-test]$ ls Hello-world.cu module.sh","title":"8. Now we need to check simple CUDA application, if that is going to work for you:"},{"location":"cuda/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-cuda-codes","text":"9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code $ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 ### or $ source module.sh check if the module is loaded properly [u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 [u100490@mel2131 ~]$ module list Currently Loaded Modules: 1) env/release/2022.1 (S) 6) numactl/2.0.14-GCCcore-11.3.0 11) libpciaccess/0.16-GCCcore-11.3.0 16) GDRCopy/2.3-GCCcore-11.3.0 21) knem/1.1.4.90-GCCcore-11.3.0 2) lxp-tools/myquota/0.3.1 (S) 7) CUDA/11.7.0 12) hwloc/2.7.1-GCCcore-11.3.0 17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0 22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 3) GCCcore/11.3.0 8) NVHPC/22.7-CUDA-11.7.0 13) OpenSSL/1.1 18) libfabric/1.15.1-GCCcore-11.3.0 4) zlib/1.2.12-GCCcore-11.3.0 9) XZ/5.2.5-GCCcore-11.3.0 14) libevent/2.1.12-GCCcore-11.3.0 19) PMIx/4.2.2-GCCcore-11.3.0 5) binutils/2.38-GCCcore-11.3.0 10) libxml2/2.9.13-GCCcore-11.3.0 15) UCX/1.13.1-GCCcore-11.3.0 20) xpmem/2.6.5-36-GCCcore-11.3.0 Where: S: Module is Sticky, requires --force to unload or purge","title":"9. Finally, we need to load the compiler to test the GPU CUDA codes"},{"location":"cuda/preparation/#10-please-compile-and-test-your-cuda-application","text":"For example, Dry-run-test // compilation $ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU // execution $ ./Hello-World-GPU // output $ Hello World from GPU! Hello World from GPU! Hello World from GPU! Hello World from GPU!","title":"10. Please compile and test your CUDA application"},{"location":"cuda/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","text":"$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job","title":"11. Similarly for the hands-on session, we need to do the node reservation:"},{"location":"cuda/preparation/#12-we-will-continue-with-our-hands-on-exercise","text":"12.1 For example, Hello World example, we do the following steps: [u100490@mel2063 CUDA]$ pwd /project/home/p200117/u100490/CUDA [u100490@mel2063 CUDA]$ ls [u100490@mel2063 CUDA]$ ls Dry-run-test Matrix-multiplication Profiling Unified-memory Hello-world module.sh Shared-memory Vector-addition [u100490@mel2063 CUDA]$ source module.sh [u100490@mel2063 CUDA]$ cd Hello-world // compilation [u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU // execution [u100490@mel2063 CUDA]$ ./Hello-World-GPU // output [u100490@mel2063 CUDA]$ Hello World from GPU","title":"12. We will continue with our Hands on exercise"},{"location":"cuda/profiling/","text":"Time measurement \u00b6 In CUDA, the execution time can be measured by using the cuda events. CUDA API events shall be created using cudaEvent_t , for example, cudaEvent_t start, stop; . And thereafter, it can be initiated by cudaEventCreate(&start) for start and similarly for stop, it can be created as cudaEventCreate(&stop) . CUDA API cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start,0); And it can be initialised to measure the timing as cudaEventRecord(start,0) and cudaEventRecord(stop,0) . Then the timings can be measured as float, for example, cudaEventElapsedTime(&time, start, stop) . Finally, all the events should be destroyed using cudaEventDestroy , for example, cudaEventDestroy(start) and cudaEventDestroy(start) . CUDA API cudaEventRecord(stop); cudaEventSynchronize(stop); float time; cudaEventElapsedTime(&time, start, stop); cudaEventDestroy(start); cudaEventDestroy(stop); The following example shows how to measure your GPU kernel call in a CUDA application: Example cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start); // Device function call matrix_mul<<<Grid_dim, Block_dim>>>(d_a, d_b, d_c, N); //use CUDA API to stop the measuring time cudaEventRecord(stop); cudaEventSynchronize(stop); float time; cudaEventElapsedTime(&time, start, stop); cudaEventDestroy(start); cudaEventDestroy(stop); cout << \" time taken for the GPU kernel\" << time << endl; Nvidia system-wide performance analysis \u00b6 Nvidia profiling tools help to analyse the code when it is being spent on the given architecture. Whether it is communication or computation, we can get helpful information through traces and events. This will help the programmer optimise the code performance on the given architecture. For this, Nvidia offers three kinds of profiling options, they are: Nsight Compute : CUDA application interactive kernel profiler : This will give traces and events of the kernel calls; this further provides both visual profile-GUI and Command Line Interface (CLI) profiling options. ncu -o profile Application.exe command will create an output file profile.ncu-rep which can be opened using ncu-ui . Example $ ncu ./a.out matrix_mul(float *, float *, float *, int), 2023-Mar-12 20:20:45, Context 1, Stream 7 Section: GPU Speed Of Light Throughput ---------------------------------------------------------------------- --------------- ------------------------------ DRAM Frequency cycle/usecond 874.24 SM Frequency cycle/nsecond 1.31 Elapsed Cycles cycle 241109 Memory [%] % 13.68 DRAM Throughput % 0.07 Duration usecond 184.35 L1/TEX Cache Throughput % 82.39 L2 Cache Throughput % 13.68 SM Active Cycles cycle 30531.99 Compute (SM) [%] % 1.84 ---------------------------------------------------------------------- --------------- ------------------------------ WRN This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details. Section: Launch Statistics ---------------------------------------------------------------------- --------------- ------------------------------ Block Size 1024 Function Cache Configuration cudaFuncCachePreferNone Grid Size 16 Registers Per Thread register/thread 26 Shared Memory Configuration Size byte 0 Driver Shared Memory Per Block byte/block 0 Dynamic Shared Memory Per Block byte/block 0 Static Shared Memory Per Block byte/block 0 Threads thread 16384 Waves Per SM 0.10 ---------------------------------------------------------------------- --------------- ------------------------------ WRN The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations. Section: Occupancy ---------------------------------------------------------------------- --------------- ------------------------------ Block Limit SM block 32 Block Limit Registers block 2 Block Limit Shared Mem block 32 Block Limit Warps block 2 Theoretical Active Warps per SM warp 64 Theoretical Occupancy % 100 Achieved Occupancy % 45.48 Achieved Active Warps Per SM warp 29.11 ---------------------------------------------------------------------- --------------- ------------------------------ WRN This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy. Nsight Graphics : Graphics application frame debugger and profiler: This is quite useful for analysing the profiling results through GUI. Nsight Systems : System-wide performance analysis tool: It is needed when we try to do heterogeneous computation profiling, for example, mixing MPI and OpenMP with CUDA. This will profile the system-wide application, that is, both CPU and GPU. To learn more about the command line options, please use $ nsys profile --help Example $ nsys profile -t nvtx,cuda --stats=true ./a.out Generating '/scratch_local/nsys-report-ddd1.qdstrm' [1/7] [========================100%] report1.nsys-rep [2/7] [========================100%] report1.sqlite [3/7] Executing 'nvtxsum' stats report SKIPPED: /m100/home/userexternal/ekrishna/Teaching/report1.sqlite does not contain NV Tools Extension (NVTX) data. [4/7] Executing 'cudaapisum' stats report Time (%) Total Time (ns) Num Calls Avg (ns) Med (ns) Min (ns) Max (ns) StdDev (ns) Name -------- --------------- --------- ----------- -------- -------- --------- ----------- ---------------- 99.7 398381310 3 132793770.0 8556.0 6986 398365768 229992096.8 cudaMalloc 0.2 714256 3 238085.3 29993.0 24944 659319 364807.8 cudaFree 0.1 312388 3 104129.3 43405.0 37692 231291 110162.3 cudaMemcpy 0.0 51898 1 51898.0 51898.0 51898 51898 0.0 cudaLaunchKernel [5/7] Executing 'gpukernsum' stats report Time (%) Total Time (ns) Instances Avg (ns) Med (ns) Min (ns) Max (ns) StdDev (ns) GridXYZ BlockXYZ Name -------- --------------- --------- -------- -------- -------- -------- ----------- -------------- -------------- ------------------------------------------ 100.0 181949 1 181949.0 181949.0 181949 181949 0.0 4 4 1 32 32 1 matrix_mul(float *, float *, float *, int) [6/7] Executing 'gpumemtimesum' stats report Time (%) Total Time (ns) Count Avg (ns) Med (ns) Min (ns) Max (ns) StdDev (ns) Operation -------- --------------- ----- -------- -------- -------- -------- ----------- ------------------ 75.0 11520 2 5760.0 5760.0 5760 5760 0.0 [CUDA memcpy HtoD] 25.0 3840 1 3840.0 3840.0 3840 3840 0.0 [CUDA memcpy DtoH] [7/7] Executing 'gpumemsizesum' stats report Total (MB) Count Avg (MB) Med (MB) Min (MB) Max (MB) StdDev (MB) Operation ---------- ----- -------- -------- -------- -------- ----------- ------------------ 0.080 2 0.040 0.040 0.040 0.040 0.000 [CUDA memcpy HtoD] 0.040 1 0.040 0.040 0.040 0.040 0.000 [CUDA memcpy DtoH] Generated: /m100/home/userexternal/ekrishna/Teaching/report1.nsys-rep /m100/home/userexternal/ekrishna/Teaching/report1.sqlite Occupancy \u00b6 The CUDA Occupancy Calculator allows you to compute the multiprocessor occupancy of a Nvidia GPU microarchitecture by a given CUDA kernel. The multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU. \\(Occupancy = \\frac{Active\\ warps\\ per\\ SM}{ Max.\\ warps\\ per\\ SM}\\) Examples Occupancy CUDA Compilation and results //-*-C++-*- #include <iostream> // Device code __global__ void MyKernel ( int * d , int * a , int * b ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; d [ idx ] = a [ idx ] * b [ idx ]; } // Host code int main () { // set your numBlocks and blockSize to get 100% occupancy int numBlocks = 32 ; // Occupancy in terms of active blocks int blockSize = 128 ; // These variables are used to convert occupancy to warps int device ; cudaDeviceProp prop ; int activeWarps ; int maxWarps ; cudaGetDevice ( & device ); cudaGetDeviceProperties ( & prop , device ); cudaOccupancyMaxActiveBlocksPerMultiprocessor ( & numBlocks , MyKernel , blockSize , 0 ); activeWarps = numBlocks * blockSize / prop . warpSize ; maxWarps = prop . maxThreadsPerMultiProcessor / prop . warpSize ; std :: cout << \"Max # of Blocks : \" << numBlocks << std :: endl ; std :: cout << \"ActiveWarps : \" << activeWarps << std :: endl ; std :: cout << \"MaxWarps : \" << maxWarps << std :: endl ; std :: cout << \"Occupancy: \" << ( double ) activeWarps / maxWarps * 100 << \"%\" << std :: endl ; return 0 ; } // compilation $ nvcc - arch = compute_70 occupancy . cu - o Occupancy - GPU // execution $ . / Occupancy - GPU // output Max number of Blocks : 16 ActiveWarps : 64 MaxWarps : 64 Occupancy : 100 % Questions Occupancy: can you change numBlocks and blockSize in Occupancy.cu code and check how it affects or predicts the occupancy of the given Nvidia microarchitecture? Profiling: run your Matrix-multiplication.cu and Vector-addition.cu code and observe what you notice? for example, how to improve the occupancy? Or maximise a GPU utilization? Timing: using CUDA events API can you measure your GPU kernel execution, and compare how fast is your GPU computation compared to CPU computation?","title":"Profiling and Performance"},{"location":"cuda/profiling/#time-measurement","text":"In CUDA, the execution time can be measured by using the cuda events. CUDA API events shall be created using cudaEvent_t , for example, cudaEvent_t start, stop; . And thereafter, it can be initiated by cudaEventCreate(&start) for start and similarly for stop, it can be created as cudaEventCreate(&stop) . CUDA API cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start,0); And it can be initialised to measure the timing as cudaEventRecord(start,0) and cudaEventRecord(stop,0) . Then the timings can be measured as float, for example, cudaEventElapsedTime(&time, start, stop) . Finally, all the events should be destroyed using cudaEventDestroy , for example, cudaEventDestroy(start) and cudaEventDestroy(start) . CUDA API cudaEventRecord(stop); cudaEventSynchronize(stop); float time; cudaEventElapsedTime(&time, start, stop); cudaEventDestroy(start); cudaEventDestroy(stop); The following example shows how to measure your GPU kernel call in a CUDA application: Example cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start); // Device function call matrix_mul<<<Grid_dim, Block_dim>>>(d_a, d_b, d_c, N); //use CUDA API to stop the measuring time cudaEventRecord(stop); cudaEventSynchronize(stop); float time; cudaEventElapsedTime(&time, start, stop); cudaEventDestroy(start); cudaEventDestroy(stop); cout << \" time taken for the GPU kernel\" << time << endl;","title":"Time measurement"},{"location":"cuda/profiling/#nvidia-system-wide-performance-analysis","text":"Nvidia profiling tools help to analyse the code when it is being spent on the given architecture. Whether it is communication or computation, we can get helpful information through traces and events. This will help the programmer optimise the code performance on the given architecture. For this, Nvidia offers three kinds of profiling options, they are: Nsight Compute : CUDA application interactive kernel profiler : This will give traces and events of the kernel calls; this further provides both visual profile-GUI and Command Line Interface (CLI) profiling options. ncu -o profile Application.exe command will create an output file profile.ncu-rep which can be opened using ncu-ui . Example $ ncu ./a.out matrix_mul(float *, float *, float *, int), 2023-Mar-12 20:20:45, Context 1, Stream 7 Section: GPU Speed Of Light Throughput ---------------------------------------------------------------------- --------------- ------------------------------ DRAM Frequency cycle/usecond 874.24 SM Frequency cycle/nsecond 1.31 Elapsed Cycles cycle 241109 Memory [%] % 13.68 DRAM Throughput % 0.07 Duration usecond 184.35 L1/TEX Cache Throughput % 82.39 L2 Cache Throughput % 13.68 SM Active Cycles cycle 30531.99 Compute (SM) [%] % 1.84 ---------------------------------------------------------------------- --------------- ------------------------------ WRN This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details. Section: Launch Statistics ---------------------------------------------------------------------- --------------- ------------------------------ Block Size 1024 Function Cache Configuration cudaFuncCachePreferNone Grid Size 16 Registers Per Thread register/thread 26 Shared Memory Configuration Size byte 0 Driver Shared Memory Per Block byte/block 0 Dynamic Shared Memory Per Block byte/block 0 Static Shared Memory Per Block byte/block 0 Threads thread 16384 Waves Per SM 0.10 ---------------------------------------------------------------------- --------------- ------------------------------ WRN The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations. Section: Occupancy ---------------------------------------------------------------------- --------------- ------------------------------ Block Limit SM block 32 Block Limit Registers block 2 Block Limit Shared Mem block 32 Block Limit Warps block 2 Theoretical Active Warps per SM warp 64 Theoretical Occupancy % 100 Achieved Occupancy % 45.48 Achieved Active Warps Per SM warp 29.11 ---------------------------------------------------------------------- --------------- ------------------------------ WRN This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy. Nsight Graphics : Graphics application frame debugger and profiler: This is quite useful for analysing the profiling results through GUI. Nsight Systems : System-wide performance analysis tool: It is needed when we try to do heterogeneous computation profiling, for example, mixing MPI and OpenMP with CUDA. This will profile the system-wide application, that is, both CPU and GPU. To learn more about the command line options, please use $ nsys profile --help Example $ nsys profile -t nvtx,cuda --stats=true ./a.out Generating '/scratch_local/nsys-report-ddd1.qdstrm' [1/7] [========================100%] report1.nsys-rep [2/7] [========================100%] report1.sqlite [3/7] Executing 'nvtxsum' stats report SKIPPED: /m100/home/userexternal/ekrishna/Teaching/report1.sqlite does not contain NV Tools Extension (NVTX) data. [4/7] Executing 'cudaapisum' stats report Time (%) Total Time (ns) Num Calls Avg (ns) Med (ns) Min (ns) Max (ns) StdDev (ns) Name -------- --------------- --------- ----------- -------- -------- --------- ----------- ---------------- 99.7 398381310 3 132793770.0 8556.0 6986 398365768 229992096.8 cudaMalloc 0.2 714256 3 238085.3 29993.0 24944 659319 364807.8 cudaFree 0.1 312388 3 104129.3 43405.0 37692 231291 110162.3 cudaMemcpy 0.0 51898 1 51898.0 51898.0 51898 51898 0.0 cudaLaunchKernel [5/7] Executing 'gpukernsum' stats report Time (%) Total Time (ns) Instances Avg (ns) Med (ns) Min (ns) Max (ns) StdDev (ns) GridXYZ BlockXYZ Name -------- --------------- --------- -------- -------- -------- -------- ----------- -------------- -------------- ------------------------------------------ 100.0 181949 1 181949.0 181949.0 181949 181949 0.0 4 4 1 32 32 1 matrix_mul(float *, float *, float *, int) [6/7] Executing 'gpumemtimesum' stats report Time (%) Total Time (ns) Count Avg (ns) Med (ns) Min (ns) Max (ns) StdDev (ns) Operation -------- --------------- ----- -------- -------- -------- -------- ----------- ------------------ 75.0 11520 2 5760.0 5760.0 5760 5760 0.0 [CUDA memcpy HtoD] 25.0 3840 1 3840.0 3840.0 3840 3840 0.0 [CUDA memcpy DtoH] [7/7] Executing 'gpumemsizesum' stats report Total (MB) Count Avg (MB) Med (MB) Min (MB) Max (MB) StdDev (MB) Operation ---------- ----- -------- -------- -------- -------- ----------- ------------------ 0.080 2 0.040 0.040 0.040 0.040 0.000 [CUDA memcpy HtoD] 0.040 1 0.040 0.040 0.040 0.040 0.000 [CUDA memcpy DtoH] Generated: /m100/home/userexternal/ekrishna/Teaching/report1.nsys-rep /m100/home/userexternal/ekrishna/Teaching/report1.sqlite","title":"Nvidia system-wide performance analysis"},{"location":"cuda/profiling/#occupancy","text":"The CUDA Occupancy Calculator allows you to compute the multiprocessor occupancy of a Nvidia GPU microarchitecture by a given CUDA kernel. The multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU. \\(Occupancy = \\frac{Active\\ warps\\ per\\ SM}{ Max.\\ warps\\ per\\ SM}\\) Examples Occupancy CUDA Compilation and results //-*-C++-*- #include <iostream> // Device code __global__ void MyKernel ( int * d , int * a , int * b ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; d [ idx ] = a [ idx ] * b [ idx ]; } // Host code int main () { // set your numBlocks and blockSize to get 100% occupancy int numBlocks = 32 ; // Occupancy in terms of active blocks int blockSize = 128 ; // These variables are used to convert occupancy to warps int device ; cudaDeviceProp prop ; int activeWarps ; int maxWarps ; cudaGetDevice ( & device ); cudaGetDeviceProperties ( & prop , device ); cudaOccupancyMaxActiveBlocksPerMultiprocessor ( & numBlocks , MyKernel , blockSize , 0 ); activeWarps = numBlocks * blockSize / prop . warpSize ; maxWarps = prop . maxThreadsPerMultiProcessor / prop . warpSize ; std :: cout << \"Max # of Blocks : \" << numBlocks << std :: endl ; std :: cout << \"ActiveWarps : \" << activeWarps << std :: endl ; std :: cout << \"MaxWarps : \" << maxWarps << std :: endl ; std :: cout << \"Occupancy: \" << ( double ) activeWarps / maxWarps * 100 << \"%\" << std :: endl ; return 0 ; } // compilation $ nvcc - arch = compute_70 occupancy . cu - o Occupancy - GPU // execution $ . / Occupancy - GPU // output Max number of Blocks : 16 ActiveWarps : 64 MaxWarps : 64 Occupancy : 100 % Questions Occupancy: can you change numBlocks and blockSize in Occupancy.cu code and check how it affects or predicts the occupancy of the given Nvidia microarchitecture? Profiling: run your Matrix-multiplication.cu and Vector-addition.cu code and observe what you notice? for example, how to improve the occupancy? Or maximise a GPU utilization? Timing: using CUDA events API can you measure your GPU kernel execution, and compare how fast is your GPU computation compared to CPU computation?","title":"Occupancy"},{"location":"openacc/","text":"Introduction to OpenACC for Heterogeneous Computing \u00b6 Participants from this course will learn GPU programming using the OpenACC programming model, such as compute constructs, loop constructs and data clauses. Furthermore, understanding the GPU architecture and how parallel threads blocks are created and used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the OpenACC programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the OpenACC programming model with mentors' guidance later in the hands-on tutorial part. Learning outcomes \u00b6 After this course, participants will be able to: \u00b6 Understanding the GPU architecture (and also the difference between GPU and CPU) Streaming architecture Threads blocks Implement OpenACC programming model Compute constructs Loop constructs Data clauses Efficient handling of memory management Host to Device Unified memory Apply the OpenACC programming knowledge to accelerate examples from science and engineering: Iterative solvers from science and engineering Vector multiplication, vector addition, etc. Prerequisites \u00b6 Priority will be given to users with good experience with C/C++ and/or FORTRAN. No GPU programming knowledge is required; however, knowing the OpenMP programming model is advantageous. GPU Compute Resource \u00b6 Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide .","title":"Introduction"},{"location":"openacc/#introduction-to-openacc-for-heterogeneous-computing","text":"Participants from this course will learn GPU programming using the OpenACC programming model, such as compute constructs, loop constructs and data clauses. Furthermore, understanding the GPU architecture and how parallel threads blocks are created and used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the OpenACC programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the OpenACC programming model with mentors' guidance later in the hands-on tutorial part.","title":"Introduction to OpenACC for Heterogeneous Computing"},{"location":"openacc/#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"openacc/#after-this-course-participants-will-be-able-to","text":"Understanding the GPU architecture (and also the difference between GPU and CPU) Streaming architecture Threads blocks Implement OpenACC programming model Compute constructs Loop constructs Data clauses Efficient handling of memory management Host to Device Unified memory Apply the OpenACC programming knowledge to accelerate examples from science and engineering: Iterative solvers from science and engineering Vector multiplication, vector addition, etc.","title":"After this course, participants will be able to:"},{"location":"openacc/#prerequisites","text":"Priority will be given to users with good experience with C/C++ and/or FORTRAN. No GPU programming knowledge is required; however, knowing the OpenMP programming model is advantageous.","title":"Prerequisites"},{"location":"openacc/#gpu-compute-resource","text":"Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide .","title":"GPU Compute Resource"},{"location":"openmp/","text":"Introduction to OpenMP Programming for Shared Memory Parallel Architecture \u00b6 Participants from this course will learn Multicore (shared memory) CPU programming using the OpenMP programming model, such as parallel region, environmental routines, and data sharing. Furthermore, understanding the multicore shared memory architecture and how parallel threads blocks are used to parallelise the computational task. Since we deal with multicores and parallel threads, proper parallel work sharing and the synchronisation of the parallel calls are to be studied in detail. Finally, participants will also learn to use the OpenMP programming model to accelerate linear algebra (routines) and iterative solvers on the Multicore CPU. Participants will learn theories first and implement the OpenMP programming model with mentors' guidance later in the hands-on tutorial part. Learning outcomes \u00b6 After this course, participants will be able to: \u00b6 Understanding the shared memory architecture Unified Memory Access (UMA) and Non-Unified Memory Access (NUMA) Hybrid distributed shared memory architecture Implement OpenMP programming model Parallel region Environment routines Data sharing Efficient handling of OpenMP constructs Work sharing Synchronisation constructs Single Instruction Multiple Data (SIMD) directive Apply the OpenMP programming knowledge to parallelise examples from science and engineering: Iterative solvers from science and engineering Vector multiplication, vector addition, etc. Prerequisites \u00b6 Priority will be given to users with good experience with C/C++ and/or FORTRAN. No prior parallel programming experience is needed. GPU Compute Resource \u00b6 Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide .","title":"Introduction"},{"location":"openmp/#introduction-to-openmp-programming-for-shared-memory-parallel-architecture","text":"Participants from this course will learn Multicore (shared memory) CPU programming using the OpenMP programming model, such as parallel region, environmental routines, and data sharing. Furthermore, understanding the multicore shared memory architecture and how parallel threads blocks are used to parallelise the computational task. Since we deal with multicores and parallel threads, proper parallel work sharing and the synchronisation of the parallel calls are to be studied in detail. Finally, participants will also learn to use the OpenMP programming model to accelerate linear algebra (routines) and iterative solvers on the Multicore CPU. Participants will learn theories first and implement the OpenMP programming model with mentors' guidance later in the hands-on tutorial part.","title":"Introduction to OpenMP Programming for Shared Memory Parallel Architecture"},{"location":"openmp/#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"openmp/#after-this-course-participants-will-be-able-to","text":"Understanding the shared memory architecture Unified Memory Access (UMA) and Non-Unified Memory Access (NUMA) Hybrid distributed shared memory architecture Implement OpenMP programming model Parallel region Environment routines Data sharing Efficient handling of OpenMP constructs Work sharing Synchronisation constructs Single Instruction Multiple Data (SIMD) directive Apply the OpenMP programming knowledge to parallelise examples from science and engineering: Iterative solvers from science and engineering Vector multiplication, vector addition, etc.","title":"After this course, participants will be able to:"},{"location":"openmp/#prerequisites","text":"Priority will be given to users with good experience with C/C++ and/or FORTRAN. No prior parallel programming experience is needed.","title":"Prerequisites"},{"location":"openmp/#gpu-compute-resource","text":"Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide .","title":"GPU Compute Resource"},{"location":"openmp/exercise-1/","text":"Parallel Construct \u00b6 In this exercise, we will create a parallel region and execute the computational content in parallel. First, however, this exercise is to create a parallel region and understand the threads' behaviour in parallel. In later exercises, we will study how to parallelise the computational task within the parallel region. To create a parallel region, we use the following parallel constructs: Parallel Constructs C/C++ FORTRAN #pragma omp parallel !$omp parallel The above figure illustrates the parallel region behaviour; as we notice, within the parallel region, we get parallel threads. This means parallel threads can be executed independently of each other, and there is no order of execution. At the same time, in order to enable OpenMP constructs, clauses, and environment variables. etc., we need to include the OpenMP library as follows: OpenMP library C/C++ FORTRAN #include<omp.h> use omp_lib Compilers \u00b6 The following compilers would support the OpenMP programming model. GNU - It is an opensource and can be used for Intel and AMD CPUs Intel - It is from Intel and only optimized for Intel CPUs AOOC - Suitable for AMD CPUs, especially \u201cZen\u201d core architecture. Examples (GNU, Intel and AMD): Compilation GNU Intel AOOC $ gcc test . c - fopenmp $ g ++ test . cc - fopenmp $ gfortran test . f90 - fopenmp $ icc test . c - qopenmp $ icpc test . cc - qopenmp $ ifort test . f90 - qopenmp $ clang test . c - fopenmp $ clang ++ test . cc - fopenmp $ flang test . f90 - fopenmp Questions and Solutions \u00b6 Examples: Hello World Serial-version (C/C++) Serial-version (FORTRAN) OpenMP-version (C/C++) OpenMP-version (FORTRAN) #include <iostream> using namespace std ; int main () { cout << endl ; cout << \"Hello world from master thread\" << endl ; cout << endl ; return 0 ; } program Hello_world_Serial print * , 'Hello world from master thread' end program #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { cout << \"Hello world from parallel region \" << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib print * , 'Hello world from master thread' !$omp parallel print * , 'Hello world from parallel region' !$omp end parallel print * , 'end of the programme from master thread' end program Compilation and Output Serial-version (C/C++) Serial-version (FORTRAN) OpenMP-version (C/C++) OpenMP-version (FORTRAN) // compilation $ g ++ Hello - world - Serial . cc - o Hello - World - Serial // execution $ . / Hello - World - Serial // output $ Hello world from master thread // compilation $ gfortran Hello - world - Serial . f90 - o Hello - World - Serial // execution $ . / Hello - World - Serial // output $ Hello world from master thread // compilation $ g ++ - fopenmp Hello - world - OpenMP . cc - o Hello - World - OpenMP // execution $ . / Hello - World - OpenMP // output $ Hello world from parallel region Hello world from parallel region .. .. Hello world from parallel region end of the programme from master thread // compilation $ gfortran - fopenmp Hello - world - OpenMP . f90 - o Hello - World - OpenMP // execution $ . / Hello - World - OpenMP // output $ Hello world from master thread Hello world from parallel region .. .. Hello world from parallel region end of the programme from master thread Questions What do you notice from those examples? Can you control parallel region printout, that is, how many times it should be printed or executed? What happens if you do not use the OpenMP library, #include<omp.h> or use omp_lib ? Although creating a parallel region would allow us to do the parallel computation, however, at the same time, we should have control over the threads being created in the parallel region, for example, how many threads are needed for a particular computation, thread number, etc. For this, we need to know a few of the important environment routines which are provided by OpenMP. The below list shows a few of the most important environment routines that should be known by the programmer for optimised OpenMP coding. Environment Routines (important) \u00b6 Define number of threads to be used within the parallel region (C/C++): void omp_set_num_threads(int num_threads); (FORTRAN): subroutine omp_set_num_threads(num_threads) integer num_threads To get number of threads in the current parallel region (C/C++): int omp_get_num_threads(void); (FORTRAN): integer function omp_get_num_threads() To get available maximum threads (system default) (c/c++): int omp_get_max_threads(void); (FORTRAN): integer function omp_get_max_threads() To get thread numbers (e.g., 1, 4, etc.) (c/c+): int omp_get_thread_num(void); (FORTRAN): integer function omp_get_thread_num() To know number processors available to the device (c/c++): int omp_get_num_procs(void); (FROTRAN): integer function omp_get_num_procs() Questions and Solutions \u00b6 Questions How can you identify the thread numbers within the parallel region? What happens if you not set omp_set_num_threads() , for example, omp_set_num_threads(5)|call omp_set_num_threads(5) , what do you notice? Alternatively, you can also set a number of threads to be used in the application while the compilation export OMP_NUM_THREADS ; what do you see? Question (C/C++) Question (FORTRAN) Answer (C/C++) Answer (FORTRAN) Answer Solution Output (C/C++) Solution Output (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { //cout << \"Hello world from thread id \" << \" from the team size of \" << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib !$omp parallel !! print *, !$omp end parallel end program #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { cout << \"Hello world from thread id \" << omp_get_thread_num () << \" from the team size of \" << omp_get_num_threads () << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib !$omp parallel print * , 'Hello world from thread id ' , omp_get_thread_num (), 'from the team size of' , omp_get_num_threads () !$omp end parallel end program $ export OMP_NUM_THREADS = 10 // or $ setenv OMP_NUM_THREADS 4 // or $ OMP NUM THREADS = 4 . / omp code . exe ead id Hello world from thread id Hello world from thread id 3 from the team size of 9 from the team size of 52 from the team size of from the team size of 10 0 from the team size of 10 10 10 10 7 from the team size of 10 4 from the team size of 10 8 from the team size of 10 1 from the team size of 10 6 from the team size of 10 Hello world from thread id 0 from the team size of 10 Hello world from thread id 4 from the team size of 10 Hello world from thread id 5 from the team size of 10 Hello world from thread id 9 from the team size of 10 Hello world from thread id 2 from the team size of 10 Hello world from thread id 3 from the team size of 10 Hello world from thread id 7 from the team size of 10 Hello world from thread id 6 from the team size of 10 Hello world from thread id 8 from the team size of 10 Hello world from thread id 1 from the team size of 10 Utilities \u00b6 The main aim is to do the parallel computation to speed up computation on a given parallel architecture. Therefore, measuring the timing and comparing the solution between serial and parallel code is very important. In order to measure the timing, OpenMP provides an environmental variable, omp_get_wtime() . Time measuring C/C++ FORTRAN double start; double end; start = omp_get_wtime(); ... work to be timed ... end = omp_get_wtime(); printf(\"Work took %f seconds\\n\", end - start); DOUBLE PRECISION START, END START = omp_get_wtime() ... work to be timed ... END = omp_get_wtime() PRINT *, \"Work took\", END - START, \"seconds\"","title":"Parallel Region"},{"location":"openmp/exercise-1/#parallel-construct","text":"In this exercise, we will create a parallel region and execute the computational content in parallel. First, however, this exercise is to create a parallel region and understand the threads' behaviour in parallel. In later exercises, we will study how to parallelise the computational task within the parallel region. To create a parallel region, we use the following parallel constructs: Parallel Constructs C/C++ FORTRAN #pragma omp parallel !$omp parallel The above figure illustrates the parallel region behaviour; as we notice, within the parallel region, we get parallel threads. This means parallel threads can be executed independently of each other, and there is no order of execution. At the same time, in order to enable OpenMP constructs, clauses, and environment variables. etc., we need to include the OpenMP library as follows: OpenMP library C/C++ FORTRAN #include<omp.h> use omp_lib","title":"Parallel Construct"},{"location":"openmp/exercise-1/#compilers","text":"The following compilers would support the OpenMP programming model. GNU - It is an opensource and can be used for Intel and AMD CPUs Intel - It is from Intel and only optimized for Intel CPUs AOOC - Suitable for AMD CPUs, especially \u201cZen\u201d core architecture. Examples (GNU, Intel and AMD): Compilation GNU Intel AOOC $ gcc test . c - fopenmp $ g ++ test . cc - fopenmp $ gfortran test . f90 - fopenmp $ icc test . c - qopenmp $ icpc test . cc - qopenmp $ ifort test . f90 - qopenmp $ clang test . c - fopenmp $ clang ++ test . cc - fopenmp $ flang test . f90 - fopenmp","title":"Compilers"},{"location":"openmp/exercise-1/#questions-and-solutions","text":"Examples: Hello World Serial-version (C/C++) Serial-version (FORTRAN) OpenMP-version (C/C++) OpenMP-version (FORTRAN) #include <iostream> using namespace std ; int main () { cout << endl ; cout << \"Hello world from master thread\" << endl ; cout << endl ; return 0 ; } program Hello_world_Serial print * , 'Hello world from master thread' end program #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { cout << \"Hello world from parallel region \" << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib print * , 'Hello world from master thread' !$omp parallel print * , 'Hello world from parallel region' !$omp end parallel print * , 'end of the programme from master thread' end program Compilation and Output Serial-version (C/C++) Serial-version (FORTRAN) OpenMP-version (C/C++) OpenMP-version (FORTRAN) // compilation $ g ++ Hello - world - Serial . cc - o Hello - World - Serial // execution $ . / Hello - World - Serial // output $ Hello world from master thread // compilation $ gfortran Hello - world - Serial . f90 - o Hello - World - Serial // execution $ . / Hello - World - Serial // output $ Hello world from master thread // compilation $ g ++ - fopenmp Hello - world - OpenMP . cc - o Hello - World - OpenMP // execution $ . / Hello - World - OpenMP // output $ Hello world from parallel region Hello world from parallel region .. .. Hello world from parallel region end of the programme from master thread // compilation $ gfortran - fopenmp Hello - world - OpenMP . f90 - o Hello - World - OpenMP // execution $ . / Hello - World - OpenMP // output $ Hello world from master thread Hello world from parallel region .. .. Hello world from parallel region end of the programme from master thread Questions What do you notice from those examples? Can you control parallel region printout, that is, how many times it should be printed or executed? What happens if you do not use the OpenMP library, #include<omp.h> or use omp_lib ? Although creating a parallel region would allow us to do the parallel computation, however, at the same time, we should have control over the threads being created in the parallel region, for example, how many threads are needed for a particular computation, thread number, etc. For this, we need to know a few of the important environment routines which are provided by OpenMP. The below list shows a few of the most important environment routines that should be known by the programmer for optimised OpenMP coding.","title":"Questions and Solutions"},{"location":"openmp/exercise-1/#environment-routines-important","text":"Define number of threads to be used within the parallel region (C/C++): void omp_set_num_threads(int num_threads); (FORTRAN): subroutine omp_set_num_threads(num_threads) integer num_threads To get number of threads in the current parallel region (C/C++): int omp_get_num_threads(void); (FORTRAN): integer function omp_get_num_threads() To get available maximum threads (system default) (c/c++): int omp_get_max_threads(void); (FORTRAN): integer function omp_get_max_threads() To get thread numbers (e.g., 1, 4, etc.) (c/c+): int omp_get_thread_num(void); (FORTRAN): integer function omp_get_thread_num() To know number processors available to the device (c/c++): int omp_get_num_procs(void); (FROTRAN): integer function omp_get_num_procs()","title":"Environment Routines (important)"},{"location":"openmp/exercise-1/#questions-and-solutions_1","text":"Questions How can you identify the thread numbers within the parallel region? What happens if you not set omp_set_num_threads() , for example, omp_set_num_threads(5)|call omp_set_num_threads(5) , what do you notice? Alternatively, you can also set a number of threads to be used in the application while the compilation export OMP_NUM_THREADS ; what do you see? Question (C/C++) Question (FORTRAN) Answer (C/C++) Answer (FORTRAN) Answer Solution Output (C/C++) Solution Output (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { //cout << \"Hello world from thread id \" << \" from the team size of \" << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib !$omp parallel !! print *, !$omp end parallel end program #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { cout << \"Hello world from thread id \" << omp_get_thread_num () << \" from the team size of \" << omp_get_num_threads () << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib !$omp parallel print * , 'Hello world from thread id ' , omp_get_thread_num (), 'from the team size of' , omp_get_num_threads () !$omp end parallel end program $ export OMP_NUM_THREADS = 10 // or $ setenv OMP_NUM_THREADS 4 // or $ OMP NUM THREADS = 4 . / omp code . exe ead id Hello world from thread id Hello world from thread id 3 from the team size of 9 from the team size of 52 from the team size of from the team size of 10 0 from the team size of 10 10 10 10 7 from the team size of 10 4 from the team size of 10 8 from the team size of 10 1 from the team size of 10 6 from the team size of 10 Hello world from thread id 0 from the team size of 10 Hello world from thread id 4 from the team size of 10 Hello world from thread id 5 from the team size of 10 Hello world from thread id 9 from the team size of 10 Hello world from thread id 2 from the team size of 10 Hello world from thread id 3 from the team size of 10 Hello world from thread id 7 from the team size of 10 Hello world from thread id 6 from the team size of 10 Hello world from thread id 8 from the team size of 10 Hello world from thread id 1 from the team size of 10","title":"Questions and Solutions"},{"location":"openmp/exercise-1/#utilities","text":"The main aim is to do the parallel computation to speed up computation on a given parallel architecture. Therefore, measuring the timing and comparing the solution between serial and parallel code is very important. In order to measure the timing, OpenMP provides an environmental variable, omp_get_wtime() . Time measuring C/C++ FORTRAN double start; double end; start = omp_get_wtime(); ... work to be timed ... end = omp_get_wtime(); printf(\"Work took %f seconds\\n\", end - start); DOUBLE PRECISION START, END START = omp_get_wtime() ... work to be timed ... END = omp_get_wtime() PRINT *, \"Work took\", END - START, \"seconds\"","title":"Utilities"},{"location":"openmp/exercise-2/","text":"Shared variable \u00b6 All the threads have access to the shared variable. By default in the parallel region, all the variables are considered as a shared variable expect the loop iteration counter variables. Note Shared variables should be handled carefully; otherwise it causes race conditions in the program. Examples: Shared variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { // Array size int N = 10 ; // Initialize the variables float * a ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); //#pragma omp parallel for // or #pragma omp parallel for shared(a) for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = a [ i ] + i ; cout << \"value of a in the parallel region\" << a [ i ] << endl ; } for ( int i = 0 ; i < N ; i ++ ) cout << \"value of a after the parallel region \" << a [ i ] << endl ; return 0 ; } program main use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a integer :: n , i n = 10 ! Allocate memory for vector allocate ( a ( n )) ! $omp parallel shared ( a ) ! $omp do do i = 1 , n a ( i ) = a ( i ) + i print * , ' value of a in the parallel region ' , a ( i ) end do ! $omp end do ! $omp end parallel do i = 1 , n a ( i ) = a ( i ) + i print * , ' value of a after the parallel region ' , a ( i ) end do ! Delete the memory deallocate ( a ) end program main Question Does the value of vector a change after the parallel loop, if not why, think? Do we really need to mention shared(a) , is it neccessary? Private variable \u00b6 Each thread will have its own copy of the private variable. And the private variable is only accessible within the parallel region, not outside of the parallel region. By default, the loop iteration counters are considered as a private. A change made by one thread is not visible to other threads. Examples: Private variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { // Array size int N = 10 ; // Initialize the variables float * a , b , c ; b = 1.0 ; c = 2.0 ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); #pragma omp parallel for private(b,c) for ( int i = 0 ; i < N ; i ++ ) { b = a [ i ] + i ; c = b + 10 * i ; cout << \"value of c in the parallel region \" << c << endl ; } cout << \"value of c after the parallel region \" << c << endl ; return 0 ; } program main use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ) :: b , c integer :: n , i n = 10 b = 1.0 c = 2.0 ! Allocate memory for vector allocate ( a ( n )) ! $omp parallel private ( b , c ) shared ( a ) ! $omp do do i = 1 , n b = a ( i ) + i c = b + 10 * i print * , ' value of c in the parallel region ' , c end do ! $omp end do ! $omp end parallel print * , ' value of c after the parallel region ' , c ! Delete the memory deallocate ( a ) end program main Questions What is the value of the varible a in the parallel region and after the parallel region? After the parallel region, does variable a has been updated or not? Lastprivate \u00b6 lastprivate: is also similar to a private clause But each thread will have an uninitialized copy of the variables passed as lastprivate At the end of the parallel loop or sections, the final variable value will be the last thread accessed value in the section or in a parallel loop. Examples: Lastprivate variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int n = 10 ; int var = 5 ; omp_set_num_threads ( 10 ); #pragma omp parallel for lastprivate(var) for ( int i = 0 ; i < n ; i ++ ) { var += omp_get_thread_num (); cout << \" lastprivate in the parallel region \" << var << endl ; } /*-- End of parallel region --*/ cout << \"lastprivate after the parallel region \" << var << endl ; return 0 ; } program main use omp_lib implicit none ! Initialise the variable real ( 8 ) :: var integer :: n , i n = 10 var = 5 call omp_set_num_threads ( 10 ) ! $omp parallel ! $omp do lastprivate ( var ) do i = 1 , n var = var + omp_get_thread_num () print * , ' lastprivate in the parallel region ' , var end do ! $omp end do ! $omp end parallel print * , ' lastprivate after the parallel region ' , var end program main Questions What is the value of the varible var in the parallel region and after the parallel region? Do you think the initial value of varibale var is been considered within the parallel region? Firstprivate \u00b6 firstprivate: is similar to a private clause But each thread will have an initialized copy of the variables passed as firstprivate Available for parallel constructs, loop, sections and single constructs Examples: Firstprivate variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int n = 10 ; int var = 5 ; omp_set_num_threads ( 10 ); #pragma omp parallel for firstprivate(var) for ( int i = 0 ; i < n ; i ++ ) { var += omp_get_thread_num (); cout << \" lastprivate in the parallel region \" << var << endl ; } /*-- End of parallel region --*/ cout << \"lastprivate after the parallel region \" << var << endl ; return 0 ; } program main use omp_lib implicit none ! Initialise the variable real ( 8 ) :: var integer :: n , i n = 10 var = 5 call omp_set_num_threads ( 10 ) ! $omp parallel ! $omp do firstprivate ( var ) do i = 1 , n var = var + omp_get_thread_num () print * , ' lastprivate in the parallel region ' , var end do ! $omp end do ! $omp end parallel print * , ' lastprivate after the parallel region ' , var end program main Questions What is the value of the varible var in the parallel region and after the parallel region? Is variable var has been updated after the parallel region, if not why, think?","title":"Data Sharing Attribute"},{"location":"openmp/exercise-2/#shared-variable","text":"All the threads have access to the shared variable. By default in the parallel region, all the variables are considered as a shared variable expect the loop iteration counter variables. Note Shared variables should be handled carefully; otherwise it causes race conditions in the program. Examples: Shared variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { // Array size int N = 10 ; // Initialize the variables float * a ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); //#pragma omp parallel for // or #pragma omp parallel for shared(a) for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = a [ i ] + i ; cout << \"value of a in the parallel region\" << a [ i ] << endl ; } for ( int i = 0 ; i < N ; i ++ ) cout << \"value of a after the parallel region \" << a [ i ] << endl ; return 0 ; } program main use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a integer :: n , i n = 10 ! Allocate memory for vector allocate ( a ( n )) ! $omp parallel shared ( a ) ! $omp do do i = 1 , n a ( i ) = a ( i ) + i print * , ' value of a in the parallel region ' , a ( i ) end do ! $omp end do ! $omp end parallel do i = 1 , n a ( i ) = a ( i ) + i print * , ' value of a after the parallel region ' , a ( i ) end do ! Delete the memory deallocate ( a ) end program main Question Does the value of vector a change after the parallel loop, if not why, think? Do we really need to mention shared(a) , is it neccessary?","title":"Shared variable"},{"location":"openmp/exercise-2/#private-variable","text":"Each thread will have its own copy of the private variable. And the private variable is only accessible within the parallel region, not outside of the parallel region. By default, the loop iteration counters are considered as a private. A change made by one thread is not visible to other threads. Examples: Private variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { // Array size int N = 10 ; // Initialize the variables float * a , b , c ; b = 1.0 ; c = 2.0 ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); #pragma omp parallel for private(b,c) for ( int i = 0 ; i < N ; i ++ ) { b = a [ i ] + i ; c = b + 10 * i ; cout << \"value of c in the parallel region \" << c << endl ; } cout << \"value of c after the parallel region \" << c << endl ; return 0 ; } program main use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ) :: b , c integer :: n , i n = 10 b = 1.0 c = 2.0 ! Allocate memory for vector allocate ( a ( n )) ! $omp parallel private ( b , c ) shared ( a ) ! $omp do do i = 1 , n b = a ( i ) + i c = b + 10 * i print * , ' value of c in the parallel region ' , c end do ! $omp end do ! $omp end parallel print * , ' value of c after the parallel region ' , c ! Delete the memory deallocate ( a ) end program main Questions What is the value of the varible a in the parallel region and after the parallel region? After the parallel region, does variable a has been updated or not?","title":"Private variable"},{"location":"openmp/exercise-2/#lastprivate","text":"lastprivate: is also similar to a private clause But each thread will have an uninitialized copy of the variables passed as lastprivate At the end of the parallel loop or sections, the final variable value will be the last thread accessed value in the section or in a parallel loop. Examples: Lastprivate variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int n = 10 ; int var = 5 ; omp_set_num_threads ( 10 ); #pragma omp parallel for lastprivate(var) for ( int i = 0 ; i < n ; i ++ ) { var += omp_get_thread_num (); cout << \" lastprivate in the parallel region \" << var << endl ; } /*-- End of parallel region --*/ cout << \"lastprivate after the parallel region \" << var << endl ; return 0 ; } program main use omp_lib implicit none ! Initialise the variable real ( 8 ) :: var integer :: n , i n = 10 var = 5 call omp_set_num_threads ( 10 ) ! $omp parallel ! $omp do lastprivate ( var ) do i = 1 , n var = var + omp_get_thread_num () print * , ' lastprivate in the parallel region ' , var end do ! $omp end do ! $omp end parallel print * , ' lastprivate after the parallel region ' , var end program main Questions What is the value of the varible var in the parallel region and after the parallel region? Do you think the initial value of varibale var is been considered within the parallel region?","title":"Lastprivate"},{"location":"openmp/exercise-2/#firstprivate","text":"firstprivate: is similar to a private clause But each thread will have an initialized copy of the variables passed as firstprivate Available for parallel constructs, loop, sections and single constructs Examples: Firstprivate variable (C/C++) (FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int n = 10 ; int var = 5 ; omp_set_num_threads ( 10 ); #pragma omp parallel for firstprivate(var) for ( int i = 0 ; i < n ; i ++ ) { var += omp_get_thread_num (); cout << \" lastprivate in the parallel region \" << var << endl ; } /*-- End of parallel region --*/ cout << \"lastprivate after the parallel region \" << var << endl ; return 0 ; } program main use omp_lib implicit none ! Initialise the variable real ( 8 ) :: var integer :: n , i n = 10 var = 5 call omp_set_num_threads ( 10 ) ! $omp parallel ! $omp do firstprivate ( var ) do i = 1 , n var = var + omp_get_thread_num () print * , ' lastprivate in the parallel region ' , var end do ! $omp end do ! $omp end parallel print * , ' lastprivate after the parallel region ' , var end program main Questions What is the value of the varible var in the parallel region and after the parallel region? Is variable var has been updated after the parallel region, if not why, think?","title":"Firstprivate"},{"location":"openmp/exercise-3/","text":"Serial version discussion \u00b6 To begin to understand the work-sharing constructs, we need to learn how to parallelise the for - C/C++ or do - FORTRAN loop. For this, we will learn simple vector addition examples. As we can see from the above figure, the two vectors should be added to get a single vector. This is done by iterating over the elements and adding them together. For this, we use for - C/C++ or do - FORTRAN . Since there are no data dependencies, the loop indexes do not have any data dependency on the other indexes. Therefore, it is easy to parallelise. Examples: Loop Serial(C/C++) Serial(FORTRAN) for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do Note FORTRAN has a column-major order and C/C++ has a row-major order Fortran array index starts from 1 C/C++ arrray index starts from 0 Parallel version discussion \u00b6 Now we will look into the how to parallelise the for - C/C++ or do - FORTRAN loops. For this, we just need to add below syntax (OpenMP directives). Functionality Syntax in C/C++ Syntax in FORTRAN Distribute iterations over the threads #pragma omp for !$omp do With the help of the above syntax the loops can be easily parallelised. The figure below shows an example of how the loops are parallelised. As we can notice here, we set the omp_set_num_threads(5) for the number of parallel threads that should be used within the loops. Furthermore, the loop index goes from 0 to 9 ; in total, we need to iterate 10 elements. In this example, using 5 threads would divide 10 iterations by two . Therefore, each thread will handle 2 iterations. In total, 5 threads will do just 2 iterations in parallel for 10 elements. Examples: Loops parallelisation Serial(C/C++) FORTRAN(C/C++) #pragma omp parallel for for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } //or #pragma omp parallel #pragma omp for for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } ! $omp parallel do do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end parallel do //or ! $omp parallel ! $omp do do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do ! $omp end parallel Form understating loop parallelisation, we will continue with vector operations in parallel, that is, adding two vectors. It is very simple, and we just need to add the #pragma omp parallel for for C/C++, !$omp parallel do for FORTRAN. Could you try this by yourself? The serial code, templates and compilation command have been provided as follows. Questions and Solutions \u00b6 Examples: Vector Addition Serial(C/C++) Serial(FORTRAN) Template(C/C++) Template(FORTRAN) Solution(C/C++) Solution(FORTRAN) #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) { for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) { // ADD YOUR PARALLEL REGION FOR THE LOOP for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // ADD YOUR PARALLEL REGION HERE // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n !! ADD YOUR PARALLEL DO LOOP do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo !! ADD YOUR PARALLEL REGION ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) #pragma omp for // ADD YOUR PARALLE for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); #pragma omp parallel // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n ! $omp do do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! $omp parallel ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) ! $omp end parallel !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main Compilation and Output Serial(C/C++) Serial(FORTRAN) Solution(C/C++) Solution(FORTRAN) // compilation $ gcc Vector - addition - Serial . c - o Vector - addition - Serial // execution $ . / Vector - addition - Serial // output $ . / Vector - addition - Serial // compilation $ gfortran Vector - addition - Serial . f90 - o Vector - addition - Serial // execution $ . / Vector - addition - Serial // output $ . / Vector - addition - Serial // compilation $ gcc - fopennmp Vector - addition - OpenMP - solution . c - o Vector - addition - Solution // execution $ . / Vector - addition - Solution // output $ . / Vector - addition - Solution // compilation $ gfortran - fopenmp Vector - addition - OpenMP - solution . f90 - o Vector - addition - Solution // execution $ . / Vector - addition - Solution // output $ . / Vector - addition - Solution Questions Can you measure the performance speedup for parallelising loop? Do you see any speedup? For example, can you create more threads to speed up the computation? If yer or not, why?","title":"Work Sharing Constructs(loop)"},{"location":"openmp/exercise-3/#serial-version-discussion","text":"To begin to understand the work-sharing constructs, we need to learn how to parallelise the for - C/C++ or do - FORTRAN loop. For this, we will learn simple vector addition examples. As we can see from the above figure, the two vectors should be added to get a single vector. This is done by iterating over the elements and adding them together. For this, we use for - C/C++ or do - FORTRAN . Since there are no data dependencies, the loop indexes do not have any data dependency on the other indexes. Therefore, it is easy to parallelise. Examples: Loop Serial(C/C++) Serial(FORTRAN) for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do Note FORTRAN has a column-major order and C/C++ has a row-major order Fortran array index starts from 1 C/C++ arrray index starts from 0","title":"Serial version discussion"},{"location":"openmp/exercise-3/#parallel-version-discussion","text":"Now we will look into the how to parallelise the for - C/C++ or do - FORTRAN loops. For this, we just need to add below syntax (OpenMP directives). Functionality Syntax in C/C++ Syntax in FORTRAN Distribute iterations over the threads #pragma omp for !$omp do With the help of the above syntax the loops can be easily parallelised. The figure below shows an example of how the loops are parallelised. As we can notice here, we set the omp_set_num_threads(5) for the number of parallel threads that should be used within the loops. Furthermore, the loop index goes from 0 to 9 ; in total, we need to iterate 10 elements. In this example, using 5 threads would divide 10 iterations by two . Therefore, each thread will handle 2 iterations. In total, 5 threads will do just 2 iterations in parallel for 10 elements. Examples: Loops parallelisation Serial(C/C++) FORTRAN(C/C++) #pragma omp parallel for for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } //or #pragma omp parallel #pragma omp for for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } ! $omp parallel do do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end parallel do //or ! $omp parallel ! $omp do do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do ! $omp end parallel Form understating loop parallelisation, we will continue with vector operations in parallel, that is, adding two vectors. It is very simple, and we just need to add the #pragma omp parallel for for C/C++, !$omp parallel do for FORTRAN. Could you try this by yourself? The serial code, templates and compilation command have been provided as follows.","title":"Parallel version discussion"},{"location":"openmp/exercise-3/#questions-and-solutions","text":"Examples: Vector Addition Serial(C/C++) Serial(FORTRAN) Template(C/C++) Template(FORTRAN) Solution(C/C++) Solution(FORTRAN) #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) { for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) { // ADD YOUR PARALLEL REGION FOR THE LOOP for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // ADD YOUR PARALLEL REGION HERE // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n !! ADD YOUR PARALLEL DO LOOP do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo !! ADD YOUR PARALLEL REGION ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) #pragma omp for // ADD YOUR PARALLE for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); #pragma omp parallel // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n ! $omp do do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! $omp parallel ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) ! $omp end parallel !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main Compilation and Output Serial(C/C++) Serial(FORTRAN) Solution(C/C++) Solution(FORTRAN) // compilation $ gcc Vector - addition - Serial . c - o Vector - addition - Serial // execution $ . / Vector - addition - Serial // output $ . / Vector - addition - Serial // compilation $ gfortran Vector - addition - Serial . f90 - o Vector - addition - Serial // execution $ . / Vector - addition - Serial // output $ . / Vector - addition - Serial // compilation $ gcc - fopennmp Vector - addition - OpenMP - solution . c - o Vector - addition - Solution // execution $ . / Vector - addition - Solution // output $ . / Vector - addition - Solution // compilation $ gfortran - fopenmp Vector - addition - OpenMP - solution . f90 - o Vector - addition - Solution // execution $ . / Vector - addition - Solution // output $ . / Vector - addition - Solution Questions Can you measure the performance speedup for parallelising loop? Do you see any speedup? For example, can you create more threads to speed up the computation? If yer or not, why?","title":"Questions and Solutions"},{"location":"openmp/exercise-4/","text":"Loop scheduling \u00b6 However, the above example is very simple. Because, in most cases, we would end up doing a large list of arrays with complex computations within the loop. Therefore, the work loading should be optimally distributed among the threads in those cases. To handle those considerations, OpenMP has provided the following loop-sharing clauses. They are: Static , Dynamic , Guided , Auto , and, Runtime . Example - Loop scheduling clauses Serial(C/C++) FORTRAN(C/C++) #pragma omp parallel for schedule(static) for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } //or #pragma omp parallel #pragma omp for schedule(static) for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } ! $omp parallel do schedule ( static ) do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end parallel do //or ! $omp parallel ! $omp do schedule ( static ) do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do ! $omp end parallel Static \u00b6 The number of iterations are divided by chunksize. If the chunksize is not provided, a number of iterations will be divided by the size of the team of threads. e.g., n=100, numthreads=5; each thread will execute the 20 iterations in parallel. This is useful when the computational cost is similar to each iteration. Examples and Question: static OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(static) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( static ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id 0 Thread id 0 Thread id 4 Thread id 4 Thread id 3 Thread id 3 Thread id 2 Thread id 2 Thread id 1 Thread id 1 What happens if you would set the chunksize, for example, schedule(static,4) ? What do you notice? Dynamic \u00b6 The number of iterations are divided by chunksize. If the chunksize is not provided, the default value will be considered 1. This is useful when the computational cost is different in the iteration. This will quickly place the chunk of data in the queue. Examples and Question: dynamic OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(dynamic) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( dynamic ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id Thread id 20 Thread id 4 Thread id 2 Thread id 2 Thread id 2 Thread id 2 Thread id 2 Thread id Thread id 1 3 What happens if you would set the chunksize, for example, schedule(dynamic,4)? What do you notice? Do you notice if the iterations are divided by the chunksize that we set? Guided \u00b6 Similar to dynamic scheduling, the number of iterations are divided by chunksize. But the chunk of the data size is decreasing, which is proportional to the number of unsigned iterations divided by the number of threads. If the chunksize is not provided, the default value will be considered 1. This is useful when there is poor load balancing at the end of the iteration. Examples and Question: guided OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(guided) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( guided ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id Thread id Thread id0 41 Thread id 0 Thread id 4 Thread id 4 Thread id 2 Thread id 2 Thread id 3 Thread id Are there any differences between auto and guided or dynamic ? Auto \u00b6 Here the compiler chooses the best combination of the chunksize to be used. Examples and Question: auto OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(auto) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( auto ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id Thread id Thread id Thread id0 34 Thread id Thread id 0 1 Thread id 1 Thread id 3 2 Thread id 2 Thread id 4 What would you choose for your application, auto, dynamic, guided, or static? If you are going to choose either one of them, then have a valid reason. Runtime \u00b6 During the compilation, we simply set the loop scheduling concept. Example:Loop scheduling clauses - runtime Compilation setenv OMP_SCHEDULE = \"guided,4\" setenv OMP_SCHEDULE = \"dynamic\" setenv OMP_SCHEDULE = \"nonmonotonic:dynamic,4\" // or export OMP_SCHEDULE = \"guided,4\" export OMP_SCHEDULE = \"dynamic\" export OMP_SCHEDULE = \"nonmonotonic:dynamic,4\" Examples and Question: runtime OpenMP(C/C++) OpenMP(FORTRAN) Compilation #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(runtime) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( runtime ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main export OMP_SCHEDULE = \"dynamic,3\" // check if you have exported the environment value $ env | grep OMP_SCHEDULE $ OMP_SCHEDULE = dynamic , 3 // if you want to unset $ unset OMP_SCHEDULE $ env | grep OMP_SCHEDULE // it(OMP_SCHEDULE=dynamic,3) will be removed","title":"Work Sharing Constructs(loop-scheduling)"},{"location":"openmp/exercise-4/#loop-scheduling","text":"However, the above example is very simple. Because, in most cases, we would end up doing a large list of arrays with complex computations within the loop. Therefore, the work loading should be optimally distributed among the threads in those cases. To handle those considerations, OpenMP has provided the following loop-sharing clauses. They are: Static , Dynamic , Guided , Auto , and, Runtime . Example - Loop scheduling clauses Serial(C/C++) FORTRAN(C/C++) #pragma omp parallel for schedule(static) for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } //or #pragma omp parallel #pragma omp for schedule(static) for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } ! $omp parallel do schedule ( static ) do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end parallel do //or ! $omp parallel ! $omp do schedule ( static ) do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do ! $omp end parallel","title":"Loop scheduling"},{"location":"openmp/exercise-4/#static","text":"The number of iterations are divided by chunksize. If the chunksize is not provided, a number of iterations will be divided by the size of the team of threads. e.g., n=100, numthreads=5; each thread will execute the 20 iterations in parallel. This is useful when the computational cost is similar to each iteration. Examples and Question: static OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(static) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( static ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id 0 Thread id 0 Thread id 4 Thread id 4 Thread id 3 Thread id 3 Thread id 2 Thread id 2 Thread id 1 Thread id 1 What happens if you would set the chunksize, for example, schedule(static,4) ? What do you notice?","title":"Static"},{"location":"openmp/exercise-4/#dynamic","text":"The number of iterations are divided by chunksize. If the chunksize is not provided, the default value will be considered 1. This is useful when the computational cost is different in the iteration. This will quickly place the chunk of data in the queue. Examples and Question: dynamic OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(dynamic) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( dynamic ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id Thread id 20 Thread id 4 Thread id 2 Thread id 2 Thread id 2 Thread id 2 Thread id 2 Thread id Thread id 1 3 What happens if you would set the chunksize, for example, schedule(dynamic,4)? What do you notice? Do you notice if the iterations are divided by the chunksize that we set?","title":"Dynamic"},{"location":"openmp/exercise-4/#guided","text":"Similar to dynamic scheduling, the number of iterations are divided by chunksize. But the chunk of the data size is decreasing, which is proportional to the number of unsigned iterations divided by the number of threads. If the chunksize is not provided, the default value will be considered 1. This is useful when there is poor load balancing at the end of the iteration. Examples and Question: guided OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(guided) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( guided ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id Thread id Thread id0 41 Thread id 0 Thread id 4 Thread id 4 Thread id 2 Thread id 2 Thread id 3 Thread id Are there any differences between auto and guided or dynamic ?","title":"Guided"},{"location":"openmp/exercise-4/#auto","text":"Here the compiler chooses the best combination of the chunksize to be used. Examples and Question: auto OpenMP(C/C++) OpenMP(FORTRAN) Output #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(auto) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( auto ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main Thread id Thread id Thread id Thread id0 34 Thread id Thread id 0 1 Thread id 1 Thread id 3 2 Thread id 2 Thread id 4 What would you choose for your application, auto, dynamic, guided, or static? If you are going to choose either one of them, then have a valid reason.","title":"Auto"},{"location":"openmp/exercise-4/#runtime","text":"During the compilation, we simply set the loop scheduling concept. Example:Loop scheduling clauses - runtime Compilation setenv OMP_SCHEDULE = \"guided,4\" setenv OMP_SCHEDULE = \"dynamic\" setenv OMP_SCHEDULE = \"nonmonotonic:dynamic,4\" // or export OMP_SCHEDULE = \"guided,4\" export OMP_SCHEDULE = \"dynamic\" export OMP_SCHEDULE = \"nonmonotonic:dynamic,4\" Examples and Question: runtime OpenMP(C/C++) OpenMP(FORTRAN) Compilation #include <iostream> #include <omp.h> int main () { int N = 10 ; omp_set_num_threads ( 5 ); #pragma omp parallel for schedule(runtime) for ( int i = 0 ; i < N ; i ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } return 0 ; } program main use omp_lib implicit none integer :: n , i n = 10 call omp_set_num_threads ( 5 ) ! $omp parallel ! $omp do schedule ( runtime ) do i = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do ! $omp end do ! $omp end parallel end program main export OMP_SCHEDULE = \"dynamic,3\" // check if you have exported the environment value $ env | grep OMP_SCHEDULE $ OMP_SCHEDULE = dynamic , 3 // if you want to unset $ unset OMP_SCHEDULE $ env | grep OMP_SCHEDULE // it(OMP_SCHEDULE=dynamic,3) will be removed","title":"Runtime"},{"location":"openmp/exercise-5/","text":"Most of the time, we end up having more than one loop, a nested loop, where two or three loops will be next to each other. OpenMP provides a clause for handling this kind of situation with collapse . To understand this, we will now study Matrix multiplication, which involves a nested loop. Again, most of the time, we might do computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future. Collapse \u00b6 The collapse clause can be used for the nested loop; an entire part of the iteration will be divided by an available number of threads. If the outer loop is equal to the available threads, then the outer loop will be divided number of threads. The figure below shows an example of not using a collapse clause. Therefore, only the outer loop is parallelised; each outer loop index will have N number of inner loop iterations. This is not what we want. Instead, with the available threads, we would like to parallelise the loops as efficiently as we could. Moreover, most of the time, we might have more threads available on a machine; for example, on MeluXina, we can have up to 256 threads. Therefore, when adding the collapse clause, we notice that the available threads execute every single iteration, as seen in the figure below. Collapse C/C++ FORTRAN #pragma omp parallel #pragma omp for collapse(2) for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } } // Or #pragma omp parallel for collapse(2) for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } } ! $omp parallel ! $omp do collapse ( 2 ) do i = 1 , n do j = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do end do ! $omp end do ! $omp end parallel !! Or ! $omp parallel do collapse ( 2 ) do i = 1 , n do j = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do end do ! $omp end parallel do Examples and Questions: Collapse OpenMP(C/C++) OpenMP(FORTRAN) Output(FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int N = 5 ; #pragma omp parallel #pragma omp for collapse(2) for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << \"Outer loop id \" << i << \" Inner loop id \" << j << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } } return 0 ; } program main use omp_lib implicit none integer :: n , i , j n = 5 ! $omp parallel ! $omp do collapse ( 2 ) do i = 1 , n do j = 1 , n print * , ' Outer loop id ' , i , ' Inner loop id ' , j , ' Thread id ' , omp_get_thread_num () end do end do ! $omp end do ! $omp end parallel end program main Outer loop id 4 Inner loop id 2 Thread id 16 Outer loop id 1 Inner loop id 4 Thread id 3 Outer loop id 5 Inner loop id 1 Thread id 20 Outer loop id 4 Inner loop id 1 Thread id 15 Outer loop id 2 Inner loop id 1 Thread id 5 Outer loop id 3 Inner loop id 1 Thread id 10 Outer loop id 3 Inner loop id 4 Thread id 13 Outer loop id 4 Inner loop id 4 Thread id 18 Outer loop id 4 Inner loop id 3 Thread id 17 Outer loop id 3 Inner loop id 3 Thread id 12 Outer loop id 1 Inner loop id 2 Thread id 1 Outer loop id 2 Inner loop id 3 Thread id 7 Outer loop id 1 Inner loop id 5 Thread id 4 Outer loop id 2 Inner loop id 2 Thread id 6 Outer loop id 3 Inner loop id 2 Thread id 11 Outer loop id 2 Inner loop id 5 Thread id 9 Outer loop id 3 Inner loop id 5 Thread id 14 Outer loop id 5 Inner loop id 3 Thread id 22 Outer loop id 5 Inner loop id 4 Thread id 23 Outer loop id 5 Inner loop id 5 Thread id 24 Outer loop id 2 Inner loop id 4 Thread id 8 Outer loop id 1 Inner loop id 3 Thread id 2 Outer loop id 4 Inner loop id 5 Thread id 19 Outer loop id 1 Inner loop id 1 Thread id 0 Outer loop id 5 Inner loop id 2 Thread id 21 Can you add here any of the scheduling clauses, for example, static, dynamic, etc? Is it really necessary to them when you use collapse , or is it dependent on other factors, such as the nature of the computation and available threads? Reduction \u00b6 The reduction clauses are data-sharing attribute clauses that can be used to perform some forms of repetition calculations in the parallel region. it can be used for arithmetic reductions: +,*,-,max,min and also with logical operator reductions in C: & && | || \u02c6 Reduction C/C++ FORTRAN #pragma omp parallel #pragma omp for reduction(+:sum) for ( int i = 0 ; i < N ; i ++ ) { sum += a [ i ]; } // Or #pragma omp parallel for reduction(+:sum) for ( int i = 0 ; i < N ; i ++ ) { sum += a [ i ]; } ! $omp parallel ! $omp do reduction ( +: sum ) do i = 1 , n sum = sum + a ( i ) end do ! $omp end do ! $omp end parallel !! Or ! $omp parallel do reduction ( +: sum ) do i = 1 , n sum = sum + a ( i ) end do ! $omp end parallel do Examples and Question: Reduction OpenMP(C/C++) OpenMP(FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int sum , N = 10 ; float * a = ( float * ) malloc ( sizeof ( float ) * N ); #pragma omp parallel for reduction(+:sum) for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = i ; sum += a [ i ]; } cout << \"Sum is \" << sum << endl ; return 0 ; } program main use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a integer :: n , i , sum n = 10 ! Allocate memory for vector allocate ( a ( n )) ! $omp parallel do reduction ( +: sum ) do i = 1 , n a ( i ) = i sum = sum + a ( i ) end do ! $omp end parallel do print * , ' Sum is ' , sum end program main What happens if you do not use the reduction clause? Do we still get the correct answer? Matrix Multiplication \u00b6 In this example, we consider a square matrix; M=N is equal for both A and B matrices. Even though we deal here with a 2D matrix, we create a 1D array to represent a 2D matrix. In this example, we must use collapse clause since matrix multiplication deals with 3 loops. The first 2 outer loops will take rows of the A matrix and columns of the B matrix. Therefore, these two loops can be easily parallelised. But then we need to sum the value of the those two outer loops value finally; this is where we should use the reduction clause. matrix multiplication function call C/C++ FORTRAN for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo Questions and Solutions \u00b6 Examples: Matrix Multiplication Serial(C/C++) Serial(FORTRAN) Template(C/C++) Template(FORTRAN) Solution(C/C++) Solution(FORTRAN) #include <stdio.h> #include <stdlib.h> #include <omp.h> void Matrix_Multiplication ( float * a , float * b , float * c , int width ) { float sum = 0 ; for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } } int main () { printf ( \"Programme assumes that matrix size is N*N \\n \" ); printf ( \"Please enter the N size number \\n \" ); int N = 0 ; scanf ( \"%d\" , & N ); // Initialize the memory float * a , * b , * c ; // Allocate memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Fuction call Matrix_Multiplication ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { printf ( \"%f \" , c [ j ]); } printf ( \" \\n \" ); } // Deallocate memory free ( a ); free ( b ); free ( c ); return 0 ; } module Matrix_Multiplication_Mod implicit none contains subroutine Matrix_Multiplication ( a , b , c , width ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c real ( 8 ) :: sum = 0 integer :: i , row , col , width do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo end subroutine Matrix_Multiplication end module Matrix_Multiplication_Mod program main use Matrix_Multiplication_Mod use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n * n )) allocate ( b ( n * n )) allocate ( c ( n * n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n * n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! Call the vector add subroutine call Matrix_Multiplication ( a , b , c , n ) !! Verification do i = 1 , n * n print * , c ( i ) enddo ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <omp.h> void Matrix_Multiplication ( float * a , float * b , float * c , int width ) { float sum = 0 ; for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } } int main () { printf ( \"Programme assumes that matrix size is N*N \\n \" ); printf ( \"Please enter the N size number \\n \" ); int N = 0 ; scanf ( \"%d\" , & N ); // Initialize the memory float * a , * b , * c ; // Allocate memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Fuction call Matrix_Multiplication ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { printf ( \"%f \" , c [ j ]); } printf ( \" \\n \" ); } // Deallocate memory free ( a ); free ( b ); free ( c ); return 0 ; } module Matrix_Multiplication_Mod implicit none contains subroutine Matrix_Multiplication ( a , b , c , width ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c real ( 8 ) :: sum = 0 integer :: i , row , col , width !!! ADD LOOP PARALLELISATION do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo end subroutine Matrix_Multiplication end module Matrix_Multiplication_Mod program main use Matrix_Multiplication_Mod use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n * n )) allocate ( b ( n * n )) allocate ( c ( n * n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n * n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo !!!! ADD PARALLEL REGION ! Call the vector add subroutine call Matrix_Multiplication ( a , b , c , n ) !! Verification do i = 1 , n * n print * , c ( i ) enddo ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <omp.h> void Matrix_Multiplication ( float * a , float * b , float * c , int width ) { float sum = 0 ; #pragma for loop collapse(2) reduction (+:sum) for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } } int main () { printf ( \"Programme assumes that matrix size is N*N \\n \" ); printf ( \"Please enter the N size number \\n \" ); int N = 0 ; scanf ( \"%d\" , & N ); // Initialize the memory float * a , * b , * c ; // Allocate memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } #pragma omp parallel // Fuction call Matrix_Multiplication ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { printf ( \"%f \" , c [ j ]); } printf ( \" \\n \" ); } // Deallocate memory free ( a ); free ( b ); free ( c ); return 0 ; } module Matrix_Multiplication_Mod implicit none contains subroutine Matrix_Multiplication ( a , b , c , width ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c real ( 8 ) :: sum = 0 integer :: i , row , col , width ! $omp do collapse ( 2 ) reduction ( +: sum ) do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo ! $omp end do end subroutine Matrix_Multiplication end module Matrix_Multiplication_Mod program main use Matrix_Multiplication_Mod use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n * n )) allocate ( b ( n * n )) allocate ( c ( n * n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n * n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! $omp parallel ! Call the vector add subroutine call Matrix_Multiplication ( a , b , c , n ) ! $omp end parallel !! Verification do i = 1 , n * n print * , c ( i ) enddo ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main Compilation and Output Serial-version(C/C++) Serial-version(FORTRAN) OpenMP(C/C++) OpenMP(FORTRAN) // compilation $ gcc Matrix - multiplication . c - o Matrix - Multiplication - Serial // execution $ . / Matrix - Multiplication - Serial Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 // compilation $ gfortran Matrix - multiplication . f90 - o Matrix - Multiplication - Serial // execution $ . / Matrix - Multiplication - Serial Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 // compilation $ gcc - fopenmp Matrix - multiplication - Solution . c - o Matrix - Multiplication - Solution // execution $ . / Matrix - Multiplication - Solution Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 // compilation $ gfortran - fopenmp Matrix - multiplication - Solution . f90 - o Matrix - Multiplication - Solution // execution $ . / Matrix - Multiplication - Solution Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 Questions Right now, we are dealing with square matrices. Could you write a code for a different matrix size while still fulfilling the matrix multiplication condition? Could you use any one of the loop scheduling, for example, dynamic or static ? Do you see any performance gain?","title":"Worksharing Constructs(others)"},{"location":"openmp/exercise-5/#collapse","text":"The collapse clause can be used for the nested loop; an entire part of the iteration will be divided by an available number of threads. If the outer loop is equal to the available threads, then the outer loop will be divided number of threads. The figure below shows an example of not using a collapse clause. Therefore, only the outer loop is parallelised; each outer loop index will have N number of inner loop iterations. This is not what we want. Instead, with the available threads, we would like to parallelise the loops as efficiently as we could. Moreover, most of the time, we might have more threads available on a machine; for example, on MeluXina, we can have up to 256 threads. Therefore, when adding the collapse clause, we notice that the available threads execute every single iteration, as seen in the figure below. Collapse C/C++ FORTRAN #pragma omp parallel #pragma omp for collapse(2) for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } } // Or #pragma omp parallel for collapse(2) for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } } ! $omp parallel ! $omp do collapse ( 2 ) do i = 1 , n do j = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do end do ! $omp end do ! $omp end parallel !! Or ! $omp parallel do collapse ( 2 ) do i = 1 , n do j = 1 , n print * , ' Thread id ' , omp_get_thread_num () end do end do ! $omp end parallel do Examples and Questions: Collapse OpenMP(C/C++) OpenMP(FORTRAN) Output(FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int N = 5 ; #pragma omp parallel #pragma omp for collapse(2) for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { cout << \"Outer loop id \" << i << \" Inner loop id \" << j << \" Thread id\" << \" \" << omp_get_thread_num () << endl ; } } return 0 ; } program main use omp_lib implicit none integer :: n , i , j n = 5 ! $omp parallel ! $omp do collapse ( 2 ) do i = 1 , n do j = 1 , n print * , ' Outer loop id ' , i , ' Inner loop id ' , j , ' Thread id ' , omp_get_thread_num () end do end do ! $omp end do ! $omp end parallel end program main Outer loop id 4 Inner loop id 2 Thread id 16 Outer loop id 1 Inner loop id 4 Thread id 3 Outer loop id 5 Inner loop id 1 Thread id 20 Outer loop id 4 Inner loop id 1 Thread id 15 Outer loop id 2 Inner loop id 1 Thread id 5 Outer loop id 3 Inner loop id 1 Thread id 10 Outer loop id 3 Inner loop id 4 Thread id 13 Outer loop id 4 Inner loop id 4 Thread id 18 Outer loop id 4 Inner loop id 3 Thread id 17 Outer loop id 3 Inner loop id 3 Thread id 12 Outer loop id 1 Inner loop id 2 Thread id 1 Outer loop id 2 Inner loop id 3 Thread id 7 Outer loop id 1 Inner loop id 5 Thread id 4 Outer loop id 2 Inner loop id 2 Thread id 6 Outer loop id 3 Inner loop id 2 Thread id 11 Outer loop id 2 Inner loop id 5 Thread id 9 Outer loop id 3 Inner loop id 5 Thread id 14 Outer loop id 5 Inner loop id 3 Thread id 22 Outer loop id 5 Inner loop id 4 Thread id 23 Outer loop id 5 Inner loop id 5 Thread id 24 Outer loop id 2 Inner loop id 4 Thread id 8 Outer loop id 1 Inner loop id 3 Thread id 2 Outer loop id 4 Inner loop id 5 Thread id 19 Outer loop id 1 Inner loop id 1 Thread id 0 Outer loop id 5 Inner loop id 2 Thread id 21 Can you add here any of the scheduling clauses, for example, static, dynamic, etc? Is it really necessary to them when you use collapse , or is it dependent on other factors, such as the nature of the computation and available threads?","title":"Collapse"},{"location":"openmp/exercise-5/#reduction","text":"The reduction clauses are data-sharing attribute clauses that can be used to perform some forms of repetition calculations in the parallel region. it can be used for arithmetic reductions: +,*,-,max,min and also with logical operator reductions in C: & && | || \u02c6 Reduction C/C++ FORTRAN #pragma omp parallel #pragma omp for reduction(+:sum) for ( int i = 0 ; i < N ; i ++ ) { sum += a [ i ]; } // Or #pragma omp parallel for reduction(+:sum) for ( int i = 0 ; i < N ; i ++ ) { sum += a [ i ]; } ! $omp parallel ! $omp do reduction ( +: sum ) do i = 1 , n sum = sum + a ( i ) end do ! $omp end do ! $omp end parallel !! Or ! $omp parallel do reduction ( +: sum ) do i = 1 , n sum = sum + a ( i ) end do ! $omp end parallel do Examples and Question: Reduction OpenMP(C/C++) OpenMP(FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { int sum , N = 10 ; float * a = ( float * ) malloc ( sizeof ( float ) * N ); #pragma omp parallel for reduction(+:sum) for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = i ; sum += a [ i ]; } cout << \"Sum is \" << sum << endl ; return 0 ; } program main use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a integer :: n , i , sum n = 10 ! Allocate memory for vector allocate ( a ( n )) ! $omp parallel do reduction ( +: sum ) do i = 1 , n a ( i ) = i sum = sum + a ( i ) end do ! $omp end parallel do print * , ' Sum is ' , sum end program main What happens if you do not use the reduction clause? Do we still get the correct answer?","title":"Reduction"},{"location":"openmp/exercise-5/#matrix-multiplication","text":"In this example, we consider a square matrix; M=N is equal for both A and B matrices. Even though we deal here with a 2D matrix, we create a 1D array to represent a 2D matrix. In this example, we must use collapse clause since matrix multiplication deals with 3 loops. The first 2 outer loops will take rows of the A matrix and columns of the B matrix. Therefore, these two loops can be easily parallelised. But then we need to sum the value of the those two outer loops value finally; this is where we should use the reduction clause. matrix multiplication function call C/C++ FORTRAN for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo","title":"Matrix Multiplication"},{"location":"openmp/exercise-5/#questions-and-solutions","text":"Examples: Matrix Multiplication Serial(C/C++) Serial(FORTRAN) Template(C/C++) Template(FORTRAN) Solution(C/C++) Solution(FORTRAN) #include <stdio.h> #include <stdlib.h> #include <omp.h> void Matrix_Multiplication ( float * a , float * b , float * c , int width ) { float sum = 0 ; for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } } int main () { printf ( \"Programme assumes that matrix size is N*N \\n \" ); printf ( \"Please enter the N size number \\n \" ); int N = 0 ; scanf ( \"%d\" , & N ); // Initialize the memory float * a , * b , * c ; // Allocate memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Fuction call Matrix_Multiplication ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { printf ( \"%f \" , c [ j ]); } printf ( \" \\n \" ); } // Deallocate memory free ( a ); free ( b ); free ( c ); return 0 ; } module Matrix_Multiplication_Mod implicit none contains subroutine Matrix_Multiplication ( a , b , c , width ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c real ( 8 ) :: sum = 0 integer :: i , row , col , width do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo end subroutine Matrix_Multiplication end module Matrix_Multiplication_Mod program main use Matrix_Multiplication_Mod use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n * n )) allocate ( b ( n * n )) allocate ( c ( n * n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n * n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! Call the vector add subroutine call Matrix_Multiplication ( a , b , c , n ) !! Verification do i = 1 , n * n print * , c ( i ) enddo ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <omp.h> void Matrix_Multiplication ( float * a , float * b , float * c , int width ) { float sum = 0 ; for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } } int main () { printf ( \"Programme assumes that matrix size is N*N \\n \" ); printf ( \"Please enter the N size number \\n \" ); int N = 0 ; scanf ( \"%d\" , & N ); // Initialize the memory float * a , * b , * c ; // Allocate memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Fuction call Matrix_Multiplication ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { printf ( \"%f \" , c [ j ]); } printf ( \" \\n \" ); } // Deallocate memory free ( a ); free ( b ); free ( c ); return 0 ; } module Matrix_Multiplication_Mod implicit none contains subroutine Matrix_Multiplication ( a , b , c , width ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c real ( 8 ) :: sum = 0 integer :: i , row , col , width !!! ADD LOOP PARALLELISATION do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo end subroutine Matrix_Multiplication end module Matrix_Multiplication_Mod program main use Matrix_Multiplication_Mod use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n * n )) allocate ( b ( n * n )) allocate ( c ( n * n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n * n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo !!!! ADD PARALLEL REGION ! Call the vector add subroutine call Matrix_Multiplication ( a , b , c , n ) !! Verification do i = 1 , n * n print * , c ( i ) enddo ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <omp.h> void Matrix_Multiplication ( float * a , float * b , float * c , int width ) { float sum = 0 ; #pragma for loop collapse(2) reduction (+:sum) for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { sum = 0 ; for ( int i = 0 ; i < width ; ++ i ) { sum += a [ row * width + i ] * b [ i * width + col ]; } c [ row * width + col ] = sum ; } } } int main () { printf ( \"Programme assumes that matrix size is N*N \\n \" ); printf ( \"Please enter the N size number \\n \" ); int N = 0 ; scanf ( \"%d\" , & N ); // Initialize the memory float * a , * b , * c ; // Allocate memory a = ( float * ) malloc ( sizeof ( float ) * ( N * N )); b = ( float * ) malloc ( sizeof ( float ) * ( N * N )); c = ( float * ) malloc ( sizeof ( float ) * ( N * N )); // Initialize arrays for ( int i = 0 ; i < ( N * N ); i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } #pragma omp parallel // Fuction call Matrix_Multiplication ( a , b , c , N ); // Verification for ( int i = 0 ; i < N ; i ++ ) { for ( int j = 0 ; j < N ; j ++ ) { printf ( \"%f \" , c [ j ]); } printf ( \" \\n \" ); } // Deallocate memory free ( a ); free ( b ); free ( c ); return 0 ; } module Matrix_Multiplication_Mod implicit none contains subroutine Matrix_Multiplication ( a , b , c , width ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c real ( 8 ) :: sum = 0 integer :: i , row , col , width ! $omp do collapse ( 2 ) reduction ( +: sum ) do row = 0 , width -1 do col = 0 , width -1 sum = 0 do i = 0 , width -1 sum = sum + ( a (( row * width ) + i + 1 ) * b (( i * width ) + col + 1 )) enddo c ( row * width + col + 1 ) = sum enddo enddo ! $omp end do end subroutine Matrix_Multiplication end module Matrix_Multiplication_Mod program main use Matrix_Multiplication_Mod use omp_lib implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n * n )) allocate ( b ( n * n )) allocate ( c ( n * n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n * n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! $omp parallel ! Call the vector add subroutine call Matrix_Multiplication ( a , b , c , n ) ! $omp end parallel !! Verification do i = 1 , n * n print * , c ( i ) enddo ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main Compilation and Output Serial-version(C/C++) Serial-version(FORTRAN) OpenMP(C/C++) OpenMP(FORTRAN) // compilation $ gcc Matrix - multiplication . c - o Matrix - Multiplication - Serial // execution $ . / Matrix - Multiplication - Serial Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 // compilation $ gfortran Matrix - multiplication . f90 - o Matrix - Multiplication - Serial // execution $ . / Matrix - Multiplication - Serial Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 // compilation $ gcc - fopenmp Matrix - multiplication - Solution . c - o Matrix - Multiplication - Solution // execution $ . / Matrix - Multiplication - Solution Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 // compilation $ gfortran - fopenmp Matrix - multiplication - Solution . f90 - o Matrix - Multiplication - Solution // execution $ . / Matrix - Multiplication - Solution Programme assumes that matrix ( square matrix ) size is N * N Please enter the N size number 4 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 Questions Right now, we are dealing with square matrices. Could you write a code for a different matrix size while still fulfilling the matrix multiplication condition? Could you use any one of the loop scheduling, for example, dynamic or static ? Do you see any performance gain?","title":"Questions and Solutions"},{"location":"openmp/exercise-6/","text":"In this exercise, we will try to add the simd classes to our existing problems, for example, vector addition. Examples and Question: SIMD - Vector Addition Serial(C/C++) Serial(FORTRAN) Template(C/C++) Template(FORTRAN) Solution(C/C++) Solution(FORTRAN) #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) { for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) { // ADD YOUR PARALLEL REGION FOR THE LOOP SIMD for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } // Start measuring time clock_t start = clock (); // ADD YOUR PARALLEL REGION HERE // Executing vector addtion function Vector_Add ( a , b , c , N ); // Stop measuring time and calculate the elapsed time clock_t end = clock (); double elapsed = ( double )( end - start ) / CLOCKS_PER_SEC ; printf ( \"Time measured: %.3f seconds. \\n \" , elapsed ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n !! ADD YOUR PARALLEL DO LOOP WITH SIMD do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo !! ADD YOUR PARALLEL REGION ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main #include <stdio.h> #include <stdlib.h> #include <math.h> #include <assert.h> #include <time.h> #define N 5120 #define MAX_ERR 1e-6 // CPU function that adds two vector float * Vector_Add ( float * a , float * b , float * c , int n ) #pragma omp for simd // ADD YOUR PARALLE SIMD for ( int i = 0 ; i < n ; i ++ ) { c [ i ] = a [ i ] + b [ i ]; } return c ; } int main () { // Initialize the variables float * a , * b , * c ; // Allocate the memory a = ( float * ) malloc ( sizeof ( float ) * N ); b = ( float * ) malloc ( sizeof ( float ) * N ); c = ( float * ) malloc ( sizeof ( float ) * N ); // Initialize the arrays for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = 1.0f ; b [ i ] = 2.0f ; } double start = omp_get_wtime (); #pragma omp parallel // Executing vector addtion function Vector_Add ( a , b , c , N ); double end = omp_get_wtime (); printf ( \"Work took %f seconds \\n \" , end - start ); // Verification for ( int i = 0 ; i < N ; i ++ ) { assert ( fabs ( c [ i ] - a [ i ] - b [ i ]) < MAX_ERR ); } printf ( \"c[0] = %f \\n \" , c [ 0 ]); printf ( \"PASSED \\n \" ); // Deallocate the memory free ( a ); free ( b ); free ( c ); return 0 ; } module Vector_Addition_Mod implicit none contains subroutine Vector_Addition ( a , b , c , n ) use omp_lib ! Input vectors real ( 8 ), intent ( in ), dimension ( : ) :: a real ( 8 ), intent ( in ), dimension ( : ) :: b real ( 8 ), intent ( out ), dimension ( : ) :: c integer :: i , n ! $omp do simd do i = 1 , n c ( i ) = a ( i ) + b ( i ) end do ! $omp end do simd end subroutine Vector_Addition end module Vector_Addition_Mod program main use Vector_Addition_Mod implicit none ! Input vectors real ( 8 ), dimension ( : ), allocatable :: a real ( 8 ), dimension ( : ), allocatable :: b ! Output vector real ( 8 ), dimension ( : ), allocatable :: c ! real ( 8 ) :: sum = 0 double precision :: start , end integer :: n , i print * , \"This program does the addition of two vectors \" print * , \"Please specify the vector size = \" read * , n ! Allocate memory for vector allocate ( a ( n )) allocate ( b ( n )) allocate ( c ( n )) ! Initialize content of input vectors , ! vector a [ i ] = sin ( i ) ^ 2 vector b [ i ] = cos ( i ) ^ 2 do i = 1 , n a ( i ) = sin ( i * 1 D0 ) * sin ( i * 1 D0 ) b ( i ) = cos ( i * 1 D0 ) * cos ( i * 1 D0 ) enddo start = omp_get_wtime () ! $omp parallel ! Call the vector add subroutine call Vector_Addition ( a , b , c , n ) ! $omp end parallel end = omp_get_wtime () PRINT * , \"Work took\" , end - start , \"seconds\" !! Verification do i = 1 , n if ( abs ( c ( i ) - ( a ( i ) + b ( i )) == 0.00000 )) then else print * , \"FAIL\" endif enddo print * , \"PASS\" ! Delete the memory deallocate ( a ) deallocate ( b ) deallocate ( c ) end program main Please try the examples without the simd clause. Do you notice any performance differences? Critical, Single, and Master \u00b6 We will explore how single, master and critical are working in the OpenMP programming model. For this, we consider the following simple examples. Examples and Question: Critical, Single and Master (C/C++) FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { cout << \"Hello world from thread id \" << omp_get_thread_num () << \" from the team size of \" << omp_get_num_threads () << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib !$omp parallel print * , 'Hello world from thread id ' , omp_get_thread_num (), 'from the team size of' , omp_get_num_threads () !$omp end parallel end program Try single clause Try master clause Try critical clause","title":"SIMD and Others"},{"location":"openmp/exercise-6/#critical-single-and-master","text":"We will explore how single, master and critical are working in the OpenMP programming model. For this, we consider the following simple examples. Examples and Question: Critical, Single and Master (C/C++) FORTRAN) #include <iostream> #include <omp.h> using namespace std ; int main () { cout << \"Hello world from master thread \" << endl ; cout << endl ; // creating the parallel region (with N number of threads) #pragma omp parallel { cout << \"Hello world from thread id \" << omp_get_thread_num () << \" from the team size of \" << omp_get_num_threads () << endl ; } // parallel region is closed cout << endl ; cout << \"end of the programme from master thread\" << endl ; return 0 ; } program Hello_world_OpenMP use omp_lib !$omp parallel print * , 'Hello world from thread id ' , omp_get_thread_num (), 'from the team size of' , omp_get_num_threads () !$omp end parallel end program Try single clause Try master clause Try critical clause","title":"Critical, Single, and Master"},{"location":"openmp/preparation/","text":"1. How to login to MeluXina machine \u00b6 1.1 Please take a look if you are using Windows 1.2 Please take a look if you are using Linux/Mac 2. Use your username to connect to MeluXina \u00b6 2.1 For example the below example shows the user of u100490 $ ssh u100490@login.lxp.lu -p 8822 ### or $ ssh meluxina 3. Once you have logged in \u00b6 3.1 Once you have logged in, you will be in a default home directory [u100490@login02 ~]$ pwd /home/users/u100490 3.2 After that, go to the project directory. [u100490@login02 ~]$ cd /project/home/p200117 [u100490@login02 p200117]$ pwd /project/home/p200117 4. And please create your own working folder under the project directory \u00b6 4.1 For example, here is the user with u100490 : [u100490@login02 p200117]$ mkdir $USER ### or [u100490@login02 p200117]$ mkdir u100490 5. Now it is time to move into your home directory \u00b6 5.1 For example, with user home directory u100490 [u100490@login02 p200117]$cd u100490 6. Now it is time to copy the folder which has examples and source files to your home directory \u00b6 6.1 For example, with user home directory u100490 [u100490@login03 u100490]$ cp -r /project/home/p200117/OpenMP . [u100490@login03 u100490]$ cd OpenMP/ [u100490@login03 OpenMP]$ pwd /project/home/p200117/u100490/OpenMP [u100490@login03 OpenMP]$ ls -lthr drwxr-s---. 2 u100490 p200117 4.0K May 27 21:42 Data-Sharing-Attribute drwxr-s---. 2 u100490 p200117 4.0K May 28 00:35 Parallel-Region drwxr-s---. 2 u100490 p200117 4.0K May 30 18:26 Dry-run-test ... ... 7. Until now you are in the login node, now its time to do the dry run test \u00b6 7.1 Reserve the interactive node for running/testing OpenMP applications $ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 01:00:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 01:00:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job 7.2 You can also check if you got the interactive node for your computations, for example, here with the user u100490 : [u100490@mel2131 ~]$ squeue -u u100490 JOBID PARTITION NAME USER ACCOUNT STATE TIME TIME_LIMIT NODES NODELIST(REASON) 304381 cpu interact u100490 p200117 RUNNING 0:37 01:00:00 1 mel2131 8. Now we need to check simple OpenMP application, if that is going to work for you: \u00b6 8.1 Go to folder Dry-run-test [u100490@login03 OpenMP]$ cd Dry-run-test/ [u100490@login03 Dry-run-test]$ ls source.sh Test.cc Test.f90 9. Finally, we need to load the compiler to test the our OpenMP codes \u00b6 9.1 We will work with GNU compiler $ source module.sh check if the module is loaded properly [u100490@mel2131 ~]$ module list currently Loaded Modules: 1) env/release/2022.1 (S) 19) libpciaccess/0.16-GCCcore-11.3.0 37) jbigkit/2.1-GCCcore-11.3.0 55) VTune/2022.3.0 73) NSS/3.79-GCCcore-11.3.0 2) lxp-tools/myquota/0.3.1 (S) 20) X11/20220504-GCCcore-11.3.0 38) gzip/1.12-GCCcore-11.3.0 56) numactl/2.0.14-GCCcore-11.3.0 74) snappy/1.1.9-GCCcore-11.3.0 3) GCCcore/11.3.0 21) Arm-Forge/22.0.4-GCC-11.3.0 39) lz4/1.9.3-GCCcore-11.3.0 57) hwloc/2.7.1-GCCcore-11.3.0 75) JasPer/2.0.33-GCCcore-11.3.0 4) zlib/1.2.12-GCCcore-11.3.0 22) libglvnd/1.4.0-GCCcore-11.3.0 40) zstd/1.5.2-GCCcore-11.3.0 58) OpenSSL/1.1 76) nodejs/16.15.1-GCCcore-11.3.0 5) binutils/2.38-GCCcore-11.3.0 23) AMD-uProf/3.6.449 41) libdeflate/1.10-GCCcore-11.3.0 59) libevent/2.1.12-GCCcore-11.3.0 77) Qt5/5.15.5-GCCcore-11.3.0 6) ncurses/6.3-GCCcore-11.3.0 24) Advisor/2022.1.0 42) LibTIFF/4.3.0-GCCcore-11.3.0 60) UCX/1.13.1-GCCcore-11.3.0 78) CubeGUI/4.7-GCCcore-11.3.0 Where: S: Module is Sticky, requires --force to unload or purge 10. Please compile and test your OpenMP application \u00b6 For example, Dry-run-test // compilation (C/C++) $ g++ Test.cc -fopenmp // compilation (FORTRAN) $ gfortran Test.f90 -fopenmp // execution $ ./a.out // output $ Hello world from master thread Hello world from thread id Hello world from thread id Hello world from thread id Hello world from thread id Hello world from thread id 4 from the team size of 1 from the team size of 20 from the team size of from the team size of 555 11. Similarly for the hands-on session, we need to do the node reservation: \u00b6 $ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 02:30:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 02:30:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job 12. We will continue with our Hands on exercise \u00b6 12.1 For example, Hello World example, we do the following steps: [u100490@mel2063 OpenMP]$ pwd /project/home/p200117/u100490/OpenMP [u100490@mel2063 OpenMP]$ ls [u100490@mel2063 OpenMP]$ ls drwxr-s---. 2 u100490 p200117 4.0K May 27 21:42 Data-Sharing-Attribute drwxr-s--- 2 u100490 p200117 4.0K May 28 00:35 Parallel-Region drwxr-s--- 2 u100490 p200117 4.0K May 28 23:45 Worksharing-Constructs-Schedule drwxr-s---. 2 u100490 p200117 4.0K May 29 00:57 Worksharing-Constructs-Other drwxr-s---. 2 u100490 p200117 4.0K May 29 18:07 Worksharing-Constructs-Loop drwxr-s---. 2 u100490 p200117 4.0K May 30 18:25 SIMD-Others drwxr-s---. 2 u100490 p200117 4.0K May 30 18:37 Dry-run-test -rw-r----- 1 u100490 p200117 241 May 30 18:41 module.sh [u100490@mel2063 OpenMP]$ source module.sh","title":"Preparation"},{"location":"openmp/preparation/#1-how-to-login-to-meluxina-machine","text":"1.1 Please take a look if you are using Windows 1.2 Please take a look if you are using Linux/Mac","title":"1. How to login to MeluXina machine"},{"location":"openmp/preparation/#2-use-your-username-to-connect-to-meluxina","text":"2.1 For example the below example shows the user of u100490 $ ssh u100490@login.lxp.lu -p 8822 ### or $ ssh meluxina","title":"2. Use your username to connect to MeluXina"},{"location":"openmp/preparation/#3-once-you-have-logged-in","text":"3.1 Once you have logged in, you will be in a default home directory [u100490@login02 ~]$ pwd /home/users/u100490 3.2 After that, go to the project directory. [u100490@login02 ~]$ cd /project/home/p200117 [u100490@login02 p200117]$ pwd /project/home/p200117","title":"3. Once you have logged in"},{"location":"openmp/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","text":"4.1 For example, here is the user with u100490 : [u100490@login02 p200117]$ mkdir $USER ### or [u100490@login02 p200117]$ mkdir u100490","title":"4. And please create your own working folder under the project directory"},{"location":"openmp/preparation/#5-now-it-is-time-to-move-into-your-home-directory","text":"5.1 For example, with user home directory u100490 [u100490@login02 p200117]$cd u100490","title":"5. Now it is time to move into your home directory"},{"location":"openmp/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","text":"6.1 For example, with user home directory u100490 [u100490@login03 u100490]$ cp -r /project/home/p200117/OpenMP . [u100490@login03 u100490]$ cd OpenMP/ [u100490@login03 OpenMP]$ pwd /project/home/p200117/u100490/OpenMP [u100490@login03 OpenMP]$ ls -lthr drwxr-s---. 2 u100490 p200117 4.0K May 27 21:42 Data-Sharing-Attribute drwxr-s---. 2 u100490 p200117 4.0K May 28 00:35 Parallel-Region drwxr-s---. 2 u100490 p200117 4.0K May 30 18:26 Dry-run-test ... ...","title":"6. Now it is time to copy the folder which has examples and source files to your home directory"},{"location":"openmp/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","text":"7.1 Reserve the interactive node for running/testing OpenMP applications $ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 01:00:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 01:00:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job 7.2 You can also check if you got the interactive node for your computations, for example, here with the user u100490 : [u100490@mel2131 ~]$ squeue -u u100490 JOBID PARTITION NAME USER ACCOUNT STATE TIME TIME_LIMIT NODES NODELIST(REASON) 304381 cpu interact u100490 p200117 RUNNING 0:37 01:00:00 1 mel2131","title":"7. Until now you are in the login node, now its time to do the dry run test"},{"location":"openmp/preparation/#8-now-we-need-to-check-simple-openmp-application-if-that-is-going-to-work-for-you","text":"8.1 Go to folder Dry-run-test [u100490@login03 OpenMP]$ cd Dry-run-test/ [u100490@login03 Dry-run-test]$ ls source.sh Test.cc Test.f90","title":"8. Now we need to check simple OpenMP application, if that is going to work for you:"},{"location":"openmp/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-our-openmp-codes","text":"9.1 We will work with GNU compiler $ source module.sh check if the module is loaded properly [u100490@mel2131 ~]$ module list currently Loaded Modules: 1) env/release/2022.1 (S) 19) libpciaccess/0.16-GCCcore-11.3.0 37) jbigkit/2.1-GCCcore-11.3.0 55) VTune/2022.3.0 73) NSS/3.79-GCCcore-11.3.0 2) lxp-tools/myquota/0.3.1 (S) 20) X11/20220504-GCCcore-11.3.0 38) gzip/1.12-GCCcore-11.3.0 56) numactl/2.0.14-GCCcore-11.3.0 74) snappy/1.1.9-GCCcore-11.3.0 3) GCCcore/11.3.0 21) Arm-Forge/22.0.4-GCC-11.3.0 39) lz4/1.9.3-GCCcore-11.3.0 57) hwloc/2.7.1-GCCcore-11.3.0 75) JasPer/2.0.33-GCCcore-11.3.0 4) zlib/1.2.12-GCCcore-11.3.0 22) libglvnd/1.4.0-GCCcore-11.3.0 40) zstd/1.5.2-GCCcore-11.3.0 58) OpenSSL/1.1 76) nodejs/16.15.1-GCCcore-11.3.0 5) binutils/2.38-GCCcore-11.3.0 23) AMD-uProf/3.6.449 41) libdeflate/1.10-GCCcore-11.3.0 59) libevent/2.1.12-GCCcore-11.3.0 77) Qt5/5.15.5-GCCcore-11.3.0 6) ncurses/6.3-GCCcore-11.3.0 24) Advisor/2022.1.0 42) LibTIFF/4.3.0-GCCcore-11.3.0 60) UCX/1.13.1-GCCcore-11.3.0 78) CubeGUI/4.7-GCCcore-11.3.0 Where: S: Module is Sticky, requires --force to unload or purge","title":"9. Finally, we need to load the compiler to test the our OpenMP codes"},{"location":"openmp/preparation/#10-please-compile-and-test-your-openmp-application","text":"For example, Dry-run-test // compilation (C/C++) $ g++ Test.cc -fopenmp // compilation (FORTRAN) $ gfortran Test.f90 -fopenmp // execution $ ./a.out // output $ Hello world from master thread Hello world from thread id Hello world from thread id Hello world from thread id Hello world from thread id Hello world from thread id 4 from the team size of 1 from the team size of 20 from the team size of from the team size of 555","title":"10. Please compile and test your OpenMP application"},{"location":"openmp/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","text":"$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 02:30:00 check if your reservation is allocated [u100490@login03 ~]$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 02:30:00 salloc: Pending job allocation 296848 salloc: job 296848 queued and waiting for resources salloc: job 296848 has been allocated resources salloc: Granted job allocation 296848 salloc: Waiting for resource configuration salloc: Nodes mel2131 are ready for job","title":"11. Similarly for the hands-on session, we need to do the node reservation:"},{"location":"openmp/preparation/#12-we-will-continue-with-our-hands-on-exercise","text":"12.1 For example, Hello World example, we do the following steps: [u100490@mel2063 OpenMP]$ pwd /project/home/p200117/u100490/OpenMP [u100490@mel2063 OpenMP]$ ls [u100490@mel2063 OpenMP]$ ls drwxr-s---. 2 u100490 p200117 4.0K May 27 21:42 Data-Sharing-Attribute drwxr-s--- 2 u100490 p200117 4.0K May 28 00:35 Parallel-Region drwxr-s--- 2 u100490 p200117 4.0K May 28 23:45 Worksharing-Constructs-Schedule drwxr-s---. 2 u100490 p200117 4.0K May 29 00:57 Worksharing-Constructs-Other drwxr-s---. 2 u100490 p200117 4.0K May 29 18:07 Worksharing-Constructs-Loop drwxr-s---. 2 u100490 p200117 4.0K May 30 18:25 SIMD-Others drwxr-s---. 2 u100490 p200117 4.0K May 30 18:37 Dry-run-test -rw-r----- 1 u100490 p200117 241 May 30 18:41 module.sh [u100490@mel2063 OpenMP]$ source module.sh","title":"12. We will continue with our Hands on exercise"},{"location":"openmp/profiling/","text":"Profiling is an important task to be considered when a computer code is written. Writing parallel code is less challenging, but making it more efficient on a given parallel architecture is challenging. Moreover, from the programming and programmer\u2019s perspective, we want to know where the code spends most of its time. In particular, we would like to know if the code (given algorithm) is compute bound, memory bound, cache misses, memory leak, proper vectorisation, cache misses, register spilling, or hot spot (time-consuming part in the code). Plenty of tools are available to profile a scientific code (computer code for doing arithmetic computing using processors). However, Here, we will focus few of the widely used tools. AMD uProf ARM Forge Intel tools ARM Forge \u00b6 Arm Forge is another standard commercial tool for debugging, profiling, and analysing scientific code on the massively parallel computer architecture. They have a separate toolset for each category with the common environment: DDT for debugging, MAP for profiling, and performance reports for analysis. It also supports the MPI, UPC, CUDA, and OpenMP programming models for a different architecture with different variety of compilers. DDT and MAP will launch the GUI, where we can interactively debug and profile the code. Whereas perf-report will provide the analysis results in .html and .txt files. Example: ARM Forge C/C++ FORTRAN # compilation with debugging tool $ gcc test . c - g - fopenmp # execute and profile the code $ map -- profile -- no - mpi . / a . out # open the profiled result in GUI $ map xyz . map # for debugging $ ddt . / a . out # for profiling $ map . / a . out # for analysis $ perf - report . / a . out # compilation $ gfortran test . f90 - fopenmp # execute and profile the code $ map -- profile -- no - mpi . / a . out # open the profiled result in GUI $ map xyz . map # for debugging $ ddt . / a . out # for profiling $ map . / a . out # for analysis $ perf - report . / a . out Intel tools \u00b6 Intel Application Snapshot \u00b6 Intel Application Performance Snapshot tool helps to find essential performance factors and the metrics of CPU utilisation, memory access efficiency, and vectorisation. aps -help will list out profiling metrics options in APS Example: APS C/C++ FORTRAN # compilation $ icc - qopenmp test . c # code execution $ aps -- collection - mode = all - r report_output . / a . out $ aps - report - g report_output # create a . html file $ firefox report_output_ < postfix > . html # APS GUI in a browser $ aps - report report_output # command line output # compilation $ ifort - qopenmp test . f90 # code execution $ aps -- collection - mode = all - r report_output . / a . out $ aps - report - g report_output # create a . html file $ firefox report_output_ < postfix > . html # APS GUI in a browser $ aps - report report_output # command line output Intel Inspector \u00b6 Intel Inspector detects and locates the memory, deadlocks, and data races in the code. For example, memory access and memory leaks can be found. Example: Intel Inspector C/C++ FORTRAN # compile the code $ icc - qopenmp example . c # execute and profile the code $ inspxe - cl - collect mi1 - result - dir mi1 -- . / a . out $ cat inspxe - cl . txt # open the file to see if there is any memory leak === Start : [ 2020 / 12 / 12 01 : 19 : 59 ] === 0 new problem ( s ) found === End : [ 2020 / 12 / 12 01 : 20 : 25 ] === # compile the code $ ifort - qopenmp test . f90 # execute and profile the code $ inspxe - cl - collect mi1 - result - dir mi1 -- . / a . out $ cat inspxe - cl . txt # open the file to see if there is any memory leak === Start : [ 2023 / 05 / 10 01 : 19 : 59 ] === 0 new problem ( s ) found === End : [ 2020 / 05 / 10 01 : 20 : 25 ] === Intel Advisor \u00b6 Intel Advisor: set of collection tools for the metrics and traces that can be used for further tunning in the code. survey : analyse and explore an idea about where to add efficient vectorisation. Example: Intel Advisor C/C++ FORTRAN # compile the code $ icc - qopenmp test . c # collect the survey metrics $ advixe - cl - collect survey - project - dir result -- . / a . out # collect the report $ advixe - cl - report survey - project - dir result # open the gui for report visualization $ advixe - gui # compile the code $ ifort - qopenmp test .90 # collect the survey metrics $ advixe - cl - collect survey - project - dir result -- . / a . out # collect the report $ advixe - cl - report survey - project - dir result # open the gui for report visualization $ advixe - gui Intel VTune \u00b6 Identifying the time consuming part in the code. And also the identify the cache misses and latency. Example: Intel VTune C/C++ FORTRAN # compile the code $ icc - qopenmp test . c # execute the code and collect the hotspots $ amplxe - cl - collect hotspots - r amplifier_result . / a . out $ amplxe - gui # open the GUI of VTune amplifier # compile the code $ ifort - qopenmp test .90 # execute the code and collect the hotspots $ amplxe - cl - collect hotspots - r amplifier_result . / a . out $ amplxe - gui # open the GUI of VTune amplifier amplxe-cl will list out the analysis types and amplxe-cl -hlep report will list out available reports in VTune. AMD uProf \u00b6 AMD uProf profiler follows a statistical sampling-based approach to collect profile data to identify the performance bottlenecks in the application. Example: AMD uProf C/C++ FORTRAN # compile the code $ clang - fopenmp test . c $ AMDuProfCLI collect -- trace openmp -- config tbp -- output - dir solution . / a . out - d 1 # compile the code $ flang - fopenmp test .90 $ AMDuProfCLI collect -- trace openmp -- config tbp -- output - dir solution . / a . out - d 1","title":"Profiling and Performance"},{"location":"openmp/profiling/#arm-forge","text":"Arm Forge is another standard commercial tool for debugging, profiling, and analysing scientific code on the massively parallel computer architecture. They have a separate toolset for each category with the common environment: DDT for debugging, MAP for profiling, and performance reports for analysis. It also supports the MPI, UPC, CUDA, and OpenMP programming models for a different architecture with different variety of compilers. DDT and MAP will launch the GUI, where we can interactively debug and profile the code. Whereas perf-report will provide the analysis results in .html and .txt files. Example: ARM Forge C/C++ FORTRAN # compilation with debugging tool $ gcc test . c - g - fopenmp # execute and profile the code $ map -- profile -- no - mpi . / a . out # open the profiled result in GUI $ map xyz . map # for debugging $ ddt . / a . out # for profiling $ map . / a . out # for analysis $ perf - report . / a . out # compilation $ gfortran test . f90 - fopenmp # execute and profile the code $ map -- profile -- no - mpi . / a . out # open the profiled result in GUI $ map xyz . map # for debugging $ ddt . / a . out # for profiling $ map . / a . out # for analysis $ perf - report . / a . out","title":"ARM Forge"},{"location":"openmp/profiling/#intel-tools","text":"","title":"Intel tools"},{"location":"openmp/profiling/#intel-application-snapshot","text":"Intel Application Performance Snapshot tool helps to find essential performance factors and the metrics of CPU utilisation, memory access efficiency, and vectorisation. aps -help will list out profiling metrics options in APS Example: APS C/C++ FORTRAN # compilation $ icc - qopenmp test . c # code execution $ aps -- collection - mode = all - r report_output . / a . out $ aps - report - g report_output # create a . html file $ firefox report_output_ < postfix > . html # APS GUI in a browser $ aps - report report_output # command line output # compilation $ ifort - qopenmp test . f90 # code execution $ aps -- collection - mode = all - r report_output . / a . out $ aps - report - g report_output # create a . html file $ firefox report_output_ < postfix > . html # APS GUI in a browser $ aps - report report_output # command line output","title":"Intel Application Snapshot"},{"location":"openmp/profiling/#intel-inspector","text":"Intel Inspector detects and locates the memory, deadlocks, and data races in the code. For example, memory access and memory leaks can be found. Example: Intel Inspector C/C++ FORTRAN # compile the code $ icc - qopenmp example . c # execute and profile the code $ inspxe - cl - collect mi1 - result - dir mi1 -- . / a . out $ cat inspxe - cl . txt # open the file to see if there is any memory leak === Start : [ 2020 / 12 / 12 01 : 19 : 59 ] === 0 new problem ( s ) found === End : [ 2020 / 12 / 12 01 : 20 : 25 ] === # compile the code $ ifort - qopenmp test . f90 # execute and profile the code $ inspxe - cl - collect mi1 - result - dir mi1 -- . / a . out $ cat inspxe - cl . txt # open the file to see if there is any memory leak === Start : [ 2023 / 05 / 10 01 : 19 : 59 ] === 0 new problem ( s ) found === End : [ 2020 / 05 / 10 01 : 20 : 25 ] ===","title":"Intel Inspector"},{"location":"openmp/profiling/#intel-advisor","text":"Intel Advisor: set of collection tools for the metrics and traces that can be used for further tunning in the code. survey : analyse and explore an idea about where to add efficient vectorisation. Example: Intel Advisor C/C++ FORTRAN # compile the code $ icc - qopenmp test . c # collect the survey metrics $ advixe - cl - collect survey - project - dir result -- . / a . out # collect the report $ advixe - cl - report survey - project - dir result # open the gui for report visualization $ advixe - gui # compile the code $ ifort - qopenmp test .90 # collect the survey metrics $ advixe - cl - collect survey - project - dir result -- . / a . out # collect the report $ advixe - cl - report survey - project - dir result # open the gui for report visualization $ advixe - gui","title":"Intel Advisor"},{"location":"openmp/profiling/#intel-vtune","text":"Identifying the time consuming part in the code. And also the identify the cache misses and latency. Example: Intel VTune C/C++ FORTRAN # compile the code $ icc - qopenmp test . c # execute the code and collect the hotspots $ amplxe - cl - collect hotspots - r amplifier_result . / a . out $ amplxe - gui # open the GUI of VTune amplifier # compile the code $ ifort - qopenmp test .90 # execute the code and collect the hotspots $ amplxe - cl - collect hotspots - r amplifier_result . / a . out $ amplxe - gui # open the GUI of VTune amplifier amplxe-cl will list out the analysis types and amplxe-cl -hlep report will list out available reports in VTune.","title":"Intel VTune"},{"location":"openmp/profiling/#amd-uprof","text":"AMD uProf profiler follows a statistical sampling-based approach to collect profile data to identify the performance bottlenecks in the application. Example: AMD uProf C/C++ FORTRAN # compile the code $ clang - fopenmp test . c $ AMDuProfCLI collect -- trace openmp -- config tbp -- output - dir solution . / a . out - d 1 # compile the code $ flang - fopenmp test .90 $ AMDuProfCLI collect -- trace openmp -- config tbp -- output - dir solution . / a . out - d 1","title":"AMD uProf"}]}