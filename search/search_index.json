{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NCC Supercomputing Luxembourg","text":"<p> Luxinnovation, the University of Luxembourg and LuxProvide are jointly managing the National Competence Centre Supercomputing in Luxembourg. Its mission is to promote the use of HPC linked to computing, data analytics and artificial intelligence and to support stakeholders such as industry \u2013 including SMEs and startups \u2013 academia and public administration to navigate the national and european HPC ecosystem. </p> <p>Luxembourg has a unique HPC infrastructure that is open to companies as well as to researchers.</p> <p>The Luxembourg National Competence Centre in HPC offers a wide portfolio of services to help you to set up and implement your HPC-enabled projects.</p> <p>Visit our webisite: National Competence Centre Supercomputing Luxembourg</p>"},{"location":"#meluxina-open-for-commercial-use","title":"MeluXina: open for commercial use","text":"<p>Inaugurated recently in June 2021, MeluXina is open for commercial use. It has received top marks by ranking organisation TOP500, which places it 1st in the EU and 4th in the world for its energy efficiency.</p> <p>Unlike most HPC systems that are pure research infrastructures, 65% of MeluXina\u2019s capacity is available for start-ups, SMEs and large companies. The system has been built on an efficient platform and is meant to serve a large variety of complex, data-driven computational workloads. Based on the Modular Supercomputing Architecture, its forward-looking design responds to the convergence of simulation, modelling, data analytics and artificial intelligence, and enables simulation driven by predictive analytics.</p>"},{"location":"#hpc-thursdays-webinar-series","title":"HPC Thursdays webinar series","text":"<p>The \u201cHPC Thursdays\u201d webinar series provides an understanding of the benefits of using a supercomputer. It also looks at what a supercomputer is, how it works, and the skills needed to use one. Each webinar presents concrete examples of applications in different domains to help participants understand the value added of using a supercomputer, and to inspire future users in order to boost their innovation activities.</p> <p>The HPC Thursdays webinars are designed for newcomers in the supercomputing world, from all sectors of industry and all disciplines of research as well as from public administration. This is the perfect exploratory path to understand the basics of HPC.</p>"},{"location":"ai/introduction/","title":"Introduction","text":"<p>The Luxembourg Competence Centre in High-Performance Computing (HPC), in collaboration with NVIDIA  and OpenACC.org, is hosting online the AI for Science and Engineering Bootcamp during 2 half-days. The first part will be dedicated to theory, and the second part will focus on hands-on challenges on GPU accelerators of the MeluXina supercomputer. For whom? </p> <p>Both current or prospective users of large hybrid CPU/GPU clusters, which develop HPC and AI applications and could benefit from GPU acceleration, are encouraged to participate! What will you learn and how? </p> <p>During this online Bootcamp, participants will learn how to apply AI tools, techniques, and algorithms to real-life problems. Participants will be introduced to the critical concepts of Deep Neural Networks, how to build Deep Learning models, and how to measure and improve the accuracy of their models. Participants will also learn essential data pre-processing techniques to ensure a robust machine-learning pipeline. The Bootcamp is a hands-on learning experience where mentors guide participants. Learning outcomes</p>"},{"location":"ai/introduction/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<pre><code>Apply Deep Convolutional Neural Networks for science and engineering applications\nUnderstand the Classification (multi-class classification) methodology in AI\nImplement AI algorithms using Keras (e.g. TensorFlow)\nUse an efficient usage of the GPU for AI algorithms (e.g. CNN) with handling large data set\nRun AI applications in the Jupyter notebook environment (and understand singularity containers)\n</code></pre>"},{"location":"ai/introduction/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with basic experience with Python. No GPU programming knowledge is required. GPU Compute Resource</p> <p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer\u202fduring the hackathon. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide. Agenda</p> <p>This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email.</p> <p>Day 1 \u2013 Thursday, February 9th 2023: 01:30 PM \u2013 05:00 PM</p> <pre><code>01:30 PM \u2013 01:45 PM: Welcome (Moderator)\n01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture)\n02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture)\n03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab)\n</code></pre> <p>Day 2 \u2013 Friday, February 10th  2023: 01:30 PM \u2013 05:00 PM</p> <pre><code>01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge)\n04:45 PM \u2013 05:00 PM: Wrap up and QA\n</code></pre>"},{"location":"cuda/","title":"Introduction to GPU programming using CUDA","text":"<p>Participants from this course will learn GPU programming using the CUDA programming model, such as synchronisation, memory allocation and device and host calls. Furthermore, understanding the GPU architecture and how parallel threads blocks are used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the CUDA programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the CUDA programming model with mentors\u2019 guidance later in the hands-on tutorial part.</p>"},{"location":"cuda/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"cuda/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture</li> <li>Threads blocks</li> </ul> </li> <li>Implement CUDA programming model<ul> <li>Programming structure</li> <li>Device calls (threads block organisation)</li> <li>Host calls</li> </ul> </li> <li>Efficient handling of memory management<ul> <li>Host to Device</li> <li>Unified memory</li> </ul> </li> <li>Apply the CUDA programming knowledge to accelerate examples from science and engineering<ul> <li>Iterative solvers from science and engineering</li> <li>Matrix multiplication, vector addition, etc</li> </ul> </li> </ul>"},{"location":"cuda/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++. No GPU programming knowledge is required. However, knowing some basic parallel programming concepts are advantage but not necessary. </p>"},{"location":"cuda/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"cuda/exercise-1/","title":"Hello World","text":"<p>Now our first exercise would be to print out the hello world from GPU. To do that, we need to do the following things:</p> <ul> <li>Run a part or entire application on the GPU</li> <li>Call cuda function on device</li> <li>It should be called using function qualifier <code>__global__</code></li> <li>Calling the device function on the main program:</li> <li>C/C++ example, <code>c_function()</code></li> <li>CUDA example, <code>cuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;()</code> (just using 1 thread)</li> <li><code>&lt;&lt;&lt; &gt;&gt;&gt;</code>, specify the threads blocks within the bracket</li> <li>Make sure to synchronize the threads</li> <li><code>__syncthreads()</code> synchronizes all the threads within a thread block</li> <li><code>CudaDeviceSynchronize()</code> synchronizes a kernel call in host</li> <li>Most of the CUDA APIs are synchronized calls by default (but sometimes    it is good to call explicit synchronized calls to avoid errors    in the computation)</li> </ul>"},{"location":"cuda/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-versionCUDA-version <pre><code>//-*-C++-*-\n// Hello-world.c\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\nvoid c_function()\n{\nprintf(\"Hello World!\\n\");\n}\nint main()\n{\nc_function();\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Hello-world.cu\n#include&lt;studio.h&gt;\n#include&lt;cuda.h&gt;\n// device function will be executed on device (GPU) \n__global__ void cuda_function()\n{\nprintf(\"Hello World from GPU!\\n\");\n// synchronize all the threads\n__syncthreads();\n}\nint main()\n{\n// call the kernel function \ncuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;();\n// synchronize the device kernel call\ncudaDeviceSynchronize();\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Hello-world.c -o Hello-World-CPU\n// execution \n$ ./Hello-World-CPU\n// output\n$ Hello World from CPU!\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n// execution\n$ ./Hello-World-GPU\n// output\n$ Hello World from GPU!\n</code></pre> Question <p>Right now, you are printing just one <code>Hello World from GPU</code>, but what if you would like to print more <code>Hello World from GPU</code>? How can you do that?</p> QuestionAnswerSolution Output <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n__global__ void cuda_function()\n{\nprintf(\"Hello World from GPU!\\n\");\n__syncthreads();\n}\nint main()\n{\n// define your thread block here\ncuda_function&lt;&lt;&lt;&gt;&gt;&gt;();\ncudaDeviceSynchronize();\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n__global__ void cuda_function()\n{\nprintf(\"Hello World from GPU!\\n\");\n__syncthreads();\n}\nint main()\n{\n// define your thread block here\ncuda_function&lt;&lt;&lt;10,1&gt;&gt;&gt;();\ncudaDeviceSynchronize();\nreturn 0;\n}\n</code></pre> <pre><code>Hello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\n</code></pre>"},{"location":"cuda/exercise-2/","title":"Vector Addition","text":"<p>In this example, we will continue with vector addition in GPU using the CUDA programming model. This is an excellent example to begin with because we usually need to do some arithmetic operations using matrices or vectors. For that, we need to know how to access the indexes of the matrix or vector to do the computation efficiently. In this example, we will practice SIMT computation by adding two vectors.</p> <ul> <li>Memory allocation on both CPU and GPU. Because as discussed before,    GPU is an accelerator and can not act as a host machine    .So therefore, the computation    has to be initiated via CPU. That means, we need to first initialise the data on the host,    that is CPU. At the same time, we also need to initialise the memory allocation on the GPU.    Because, we need to transfer the data from a CPU to GPU.</li> </ul> <p></p> <ul> <li> <p>Allocating the CPU memory for a, b, and out vector <pre><code>// Initialize the memory on the host\nfloat *a, *b, *out;\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout   = (float*)malloc(sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for d_a, d_b, and d_out matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the     arrays a and b.  <pre><code>// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n</code></pre></p> </li> <li> <p>Transfer initialized value from CPU to GPU <pre><code>// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>Creating a 2D thread block <pre><code>// Thread organization \ndim3 dimGrid(1, 1, 1);    dim3 dimBlock(16, 16, 1); </code></pre></p> </li> </ul> Conversion of thread blocks <pre><code>//1D grid of 1D blocks\n__device__ int getGlobalIdx_1D_1D()\n{\nreturn blockIdx.x * blockDim.x + threadIdx.x;\n}\n//1D grid of 2D blocks\n__device__ int getGlobalIdx_1D_2D()\n{\nreturn blockIdx.x * blockDim.x * blockDim.y\n+ threadIdx.y * blockDim.x + threadIdx.x;\n}\n//1D grid of 3D blocks\n__device__ int getGlobalIdx_1D_3D()\n{\nreturn blockIdx.x * blockDim.x * blockDim.y * blockDim.z + threadIdx.z * blockDim.y * blockDim.x\n+ threadIdx.y * blockDim.x + threadIdx.x;\n}\n//2D grid of 1D blocks \n__device__ int getGlobalIdx_2D_1D()\n{\nint blockId   = blockIdx.y * gridDim.x + blockIdx.x;\nint threadId = blockId * blockDim.x + threadIdx.x; return threadId;\n}\n//2D grid of 2D blocks  \n__device__ int getGlobalIdx_2D_2D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * (blockDim.x * blockDim.y) +\n(threadIdx.y * blockDim.x) + threadIdx.x;\nreturn threadId;\n}\n//2D grid of 3D blocks\n__device__ int getGlobalIdx_2D_3D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n+ (threadIdx.z * (blockDim.x * blockDim.y))\n+ (threadIdx.y * blockDim.x)\n+ threadIdx.x;\nreturn threadId;\n}\n//3D grid of 1D blocks\n__device__ int getGlobalIdx_3D_1D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int threadId = blockId * blockDim.x + threadIdx.x;\nreturn threadId;\n}\n//3D grid of 2D blocks\n__device__ int getGlobalIdx_3D_2D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int threadId = blockId * (blockDim.x * blockDim.y)\n+ (threadIdx.y * blockDim.x)\n+ threadIdx.x;\nreturn threadId;\n}\n//3D grid of 3D blocks\n__device__ int getGlobalIdx_3D_3D()\n{\nint blockId = blockIdx.x + blockIdx.y * gridDim.x + gridDim.x * gridDim.y * blockIdx.z; int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n+ (threadIdx.z * (blockDim.x * blockDim.y))\n+ (threadIdx.y * blockDim.x)\n+ threadIdx.x;\nreturn threadId;\n</code></pre> <ul> <li> <p>Calling the kernel function <pre><code>// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n</code></pre></p> </li> <li> <p>Vector addition kernel function call definition</p> </li> <li> vector addition function call Serial-versionCUDA-version <pre><code>// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) {\nfor(int i = 0; i &lt; n; i ++)\n{\nout[i] = a[i] + b[i];\n}\nreturn out;\n}\n</code></pre> <pre><code>// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronize all the threads \n__syncthreads();\n}\n</code></pre> </li> </ul> <p> </p> <ul> <li> <p>Copy back computed value from GPU to CPU <pre><code>// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n// Deallocate host memory\nfree(a); free(b); free(out);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-2/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial-versionCUDA-templateCUDA-version <pre><code>//-*-C++-*-\n// Vector-addition.c\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) {\nfor(int i = 0; i &lt; n; i ++)\n{\nout[i] = a[i] + b[i];\n}\nreturn out;\n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out;       // Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout = (float*)malloc(sizeof(float) * N);\n// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Start measuring time\nclock_t start = clock();\n// Executing CPU function \nVector_Add(a, b, out, N);\n// Stop measuring time and calculate the elapsed time\nclock_t end = clock();\ndouble elapsed = (double)(end - start)/CLOCKS_PER_SEC;\nprintf(\"Time measured: %.3f seconds.\\n\", elapsed);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate host memory\nfree(a); free(b); free(out);\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition-template.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {     // allign your thread id indexes \nint i = ........\n// Allow the   threads only within the size of N\nif------\n{\nout[i] = a[i] + b[i];\n}\n// Synchronize all the threads \n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out;\n// Allocate host memory\na   = (float*)......\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a,......\n// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = ....\nb[i] = ....\n}\n// Transfer data from host to device memory\ncudaMemcpy.....\n// Thread organization \ndim3 dimGrid....  dim3 dimBlock....\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt; &gt;&gt;&gt;....\n// Transfer data back to host memory\ncudaMemcpy....\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree...\n// Deallocate host memory\nfree..\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out;\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout = (float*)malloc(sizeof(float) * N);\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N); // Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n// Thread organization \ndim3 dimGrid(ceil(N/32), ceil(N/32), 1);\ndim3 dimBlock(32, 32, 1); // execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n// Deallocate host memory\nfree(a); free(b); free(out);\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Vector-addition.c -o Vector-Addition-CPU\n// execution \n$ ./Vector-Addition-CPU\n// output\n$ ./Vector-addition-CPU out[0] = 3.000000\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Vector-addition.cu -o Vector-Addition-GPU\n// execution\n$ ./Vector-Addition-GPU\n// output\n$ ./Vector-addition-GPU\nout[0] = 3.000000\nPASSED\n</code></pre> Questions <ul> <li>What happens if you remove the <code>__syncthreads();</code> from the <code>__global__ void vector_add(float *a, float *b,     float *out, int n)</code> function.</li> <li>Can you remove the if condition <code>if(i &lt; n)</code> from the <code>__global__ void vector_add(float *a, float *b,    float *out, int n)</code> function. If so how can you do that?</li> <li>Here we do not use the <code>cudaDeviceSynchronize()</code> in the main application, can you figure out why we     do not need to use it. </li> <li>Can you create a different kinds of threads block for larger number of array?</li> </ul>"},{"location":"cuda/exercise-3/","title":"Matrix Multiplication","text":"<p>We will now look into the basic matrix multiplication. In this example, we will perform the matrix multiplication. Matrix multiplication involves a nested loop. Again, most of the time, we might end up doing computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future. </p> <p></p> b <ul> <li> <p>Allocating the CPU memory for A, B, and C matrix.    Here we notice that the matrix is stored in a    1D array because we want to consider the same function concept for CPU and GPU. <pre><code>// Initialize the memory on the host\nfloat *a, *b, *c;\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for A, B, and C matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_c;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the matrix A and B. <pre><code>// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n</code></pre></p> </li> <li> <p>Transfer initialized A and B matrix from CPU to GPU <pre><code>cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>2D thread block for indexing x and y <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></p> </li> <li> <p>Calling the kernel function <pre><code>// Device function call\nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n</code></pre></p> </li> <li> matrix multiplication function call serialcuda <pre><code>float * matrix_mul(float *h_a, float *h_b, float *h_c, int width)\n{\nfor(int row = 0; row &lt; width ; ++row)\n{\nfor(int col = 0; col &lt; width ; ++col)\n{\nfloat temp = 0;\nfor(int i = 0; i &lt; width ; ++i)\n{\ntemp += h_a[row*width+i] * h_b[i*width+col];\n}\nh_c[row*width+col] = temp;\n}\n}\nreturn h_c;\n}\n</code></pre> <pre><code>__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)\n{\nint row = blockIdx.x * blockDim.x + threadIdx.x;\nint col = blockIdx.y * blockDim.y + threadIdx.y;\nif ((row &lt; width) &amp;&amp; (col &lt; width)) {\nfloat temp = 0;\n// each thread computes one \n// element of the block sub-matrix\nfor (int i = 0; i &lt; width; ++i) {\ntemp += d_a[row*width+i]*d_b[i*width+col];\n}\nd_c[row*width+col] = temp;\n}\n}\n</code></pre> </li> <li> <p>Copy back computed value from GPU to CPU;    transfer the data back to GPU (from device to host).    Here is the C matrix that contains the product of the two matrices. <pre><code>// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial-versionCUDA-templateCUDA-version <pre><code>//-*-C++-*-\n// Matrix-multiplication.c\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\nusing namespace std;\nfloat * matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float temp = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     temp += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = temp;                            }                                                         }   return h_c;           }\nint main()\n{\ncout &lt;&lt; \"Programme assumes that matrix (square matrix )size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N = 0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c;       // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Device function call \nmatrix_mul(a, b, c, N);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\ncout &lt;&lt; c[j] &lt;&lt;\" \";\n}\ncout &lt;&lt; \" \" &lt;&lt;endl;\n}\n// Deallocate host memory\nfree(a); free(b); free(c);\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Matrix-multiplication-template.cu\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\nusing namespace std;\n__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)\n{\n// create a 2d threads block\nint row = ..................\nint col = ....................\n// only allow the threads that are needed for the computation \nif (................................)\n{\nfloat temp = 0;\n// each thread computes one \n// element of the block sub-matrix\nfor (int i = 0; i &lt; width; ++i) {\ntemp += d_a[row*width+i]*d_b[i*width+col];\n}\nd_c[row*width+col] = temp;\n}\n}\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float single_entry = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     single_entry += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = single_entry;                            }                                                         }   return h_c;           }\nint main()\n{\ncout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N = 0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\n...\n...\n// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n...\n...\n// Transfer data from host to device memory\ncudaMemcpy(.........................);\ncudaMemcpy(.........................);\n// Thread organization\nint blockSize = ..............;\ndim3 dimBlock(......................);\ndim3 dimGrid(.......................);\n// Device function call \nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// CPU computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree...\n// Deallocate host memory\nfree...\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Matrix-multiplication.cu\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\nusing namespace std;\n__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)\n{\nint row = blockIdx.x * blockDim.x + threadIdx.x;\nint col = blockIdx.y * blockDim.y + threadIdx.y;\nif ((row &lt; width) &amp;&amp; (col &lt; width)) {\nfloat temp = 0;\n// each thread computes one \n// element of the block sub-matrix\nfor (int i = 0; i &lt; width; ++i) {\ntemp += d_a[row*width+i]*d_b[i*width+col];\n}\nd_c[row*width+col] = temp;\n}\n}\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float single_entry = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     single_entry += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = single_entry;                            }                                                         }   return h_c;           }\nint main()\n{\ncout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N = 0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\nhost_check = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n// Device function call \nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// cpu computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\nfree(host_check);\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-CPU\n// execution \n$ ./Matrix-Multiplication-CPU\n// output\n$ g++ Matrix-multiplication.cc -o Matrix-multiplication\n$ ./Matrix-multiplication\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number 4\n16 16 16 16 16 16 16 16  16 16 16 16  16 16 16 16 </code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Matrix-multiplication.cu -o Matrix-Multiplication-GPU\n// execution\n$ ./Matrix-Multiplication-GPU\nProgramme assumes that matrix (square matrix) size is N*N Please enter the N size number\n$ 256\n// output\n$ Two matrices are equal\n</code></pre> Questions <ul> <li>Right now, we are using the 1D array to represent the matrix. However, you can also do it with the 2D matrix. Can you try with 2D array matrix multiplication with 2D thread block?</li> <li>Can you get the correct soltion if you remove the <code>if ((row &lt; width) &amp;&amp; (col &lt; width))</code> condition from the <code>__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)</code> function?</li> <li>Please try with different thread blocks and different matrix sizes. <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></li> </ul>"},{"location":"cuda/exercise-4/","title":"Shared Memory","text":"<p>In this example, we try shared memory matrix multiplication. This is achieved by blocking the global matrix into a small block matrix (tiled matrix) that can fit into the shared memory of the Nvidia GPU. Shared memory from the GPUs, which has a good bandwidth within the GPUs compared to access to the global memory.</p> <p> </p> <ul> <li>This is very similar to the previous example; however, we just need to allocate the small block matrix into shared memory.  The below example shows the blocking size for a and b matrix respectively for gobal A and B matrix.  <pre><code>  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n</code></pre></li> </ul> <p></p> <ul> <li>Then we need to iterate elements within the block size and, finally with the global index.  These can be achieved with CUDA threads. </li> </ul> <p></p> <ul> <li>You can also increase the shared memory or L1 cache size by using <code>cudaFuncSetCacheConfig</code>. For more information about  CUDA API, please refer to cudaFuncSetCacheConfig.</li> </ul> Tips <pre><code>cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1);\n//cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared);\n\ncudaFuncCachePreferNone: no preference for shared memory or L1 (default)\ncudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache\ncudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory\ncudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory\n\n// simple example usage increasing more shared memory \n#include&lt;stdio.h&gt;\nint main()\n{\n  // example of increasing the shared memory \n  cudaDeviceSetCacheConfig(My_Kernel, cudaFuncCachePreferShared);\n  My_Kernel&lt;&lt;&lt;&gt;&gt;&gt;();\n  cudaDeviceSynchronize(); \n  return 0;\n}\n</code></pre> <ul> <li>Different Nvidia GPUs provides different configuration, for example, Ampere GA102 GPU Architecture, will support the following configuration: <pre><code>128 KB L1 + 0 KB Shared Memory\n120 KB L1 + 8 KB Shared Memory\n112 KB L1 + 16 KB Shared Memory\n96 KB L1 + 32 KB Shared Memory\n64 KB L1 + 64 KB Shared Memory\n28 KB L1 + 100 KB Shared Memory\n</code></pre></li> </ul>"},{"location":"cuda/exercise-4/#questions-and-solutions","title":"Questions and Solutions","text":"Example: Shared Memory - Matrix Multiplication Matrix-multiplication-shared-templateMatrix-multiplication-shared.cu <pre><code>// Matrix-multiplication-shared-template.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n// block size for the matrix \n#define BLOCK_SIZE 16\nusing namespace std;\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, float *d_c, int width)\n{\n// Shared memory allocation for the block matrix  \n__shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n...\n// Indexing for the block matrix\nint tx = threadIdx.x;\n...\n// Indexing global matrix to block matrix \nint row = threadIdx.x+blockDim.x*blockIdx.x;\n...\n// Allow threads only for size of rows and columns (we assume square matrix)\nif ((row &lt; width) &amp;&amp; (col&lt; width))\n{\n// Save temporary value for the particular index\nfloat temp = 0;\nfor(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n{\n// Allign the global matrix to block matrix \na_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\nb_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n// Make sure all the threads are synchronized\n....\n// Multiply the block matrix \nfor(int j = 0; j &lt; BLOCK_SIZE; ++j)\n{\ntemp += a_block[ty][j] * b_block[j][tx];    }\n// Make sure all the threads are synchronized\n...\n}\n// Save block matrix entry to global matrix \n...\n}\n}\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float temp = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     temp += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = temp;                            }                                                         }   return h_c;           }\nint main()\n{  cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N=0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\nhost_check = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n// Thread organization\ndim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                ...\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// Cpu computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"But,two matrices are not equal\" &lt;&lt; endl;\ncout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\nfree(host_check);\nreturn 0;\n}\n</code></pre> <pre><code>// Matrix-multiplication-shared.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n// block size for the matrix \n#define BLOCK_SIZE 16\nusing namespace std;\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, float *d_c, int width)\n{\n// Shared memory allocation for the block matrix  \n__shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n__shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n// Indexing for the block matrix\nint tx = threadIdx.x;\nint ty = threadIdx.y;\n// Indexing global matrix to block matrix \nint row = threadIdx.x+blockDim.x*blockIdx.x;\nint col = threadIdx.y+blockDim.y*blockIdx.y;\n// Allow threads only for size of rows and columns (we assume square matrix)\nif ((row &lt; width) &amp;&amp; (col&lt; width))\n{\n// Save temporary value for the particular index\nfloat temp = 0;\nfor(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n{\n// Allign the global matrix to block matrix \na_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\nb_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n// Make sure all the threads are synchronized\n__syncthreads(); // Multiply the block matrix\nfor(int j = 0; j &lt; BLOCK_SIZE; ++j)\n{\ntemp += a_block[ty][j] * b_block[j][tx];    }\n__syncthreads();\n}\n// Save block matrix entry to global matrix \nd_c[row*width+col] = temp;\n}\n}\n// Host call (matix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   {                                                                 for(int row = 0; row &lt; width ; ++row)                           {                                                             for(int col = 0; col &lt; width ; ++col)                       {                                                         float single_entry = 0;                                       for(int i = 0; i &lt; width ; ++i)                         {                                                     single_entry += h_a[row*width+i] * h_b[i*width+col];      }                                                     h_c[row*width+col] = single_entry;                            }                                                         }   return h_c;           }\nint main()\n{  cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\ncout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\ncout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\nint N=0;\ncin &gt;&gt; N;\n// Initialize the memory on the host\nfloat *a, *b, *c, *host_check;       // Initialize the memory on the device\nfloat *d_a, *d_b, *d_c; // Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\nhost_check = (float*)malloc(sizeof(float) * (N*N));\n// Initialize host arrays\nfor(int i = 0; i &lt; (N*N); i++)\n{\na[i] = 2.0f;\nb[i] = 2.0f;\n}\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n// Thread organization\ndim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                dim3 Grid_dim(ceil(N/BLOCK_SIZE), ceil(N/BLOCK_SIZE), 1);\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n// cpu computation for verification \ncpu_matrix_mul(a,b,host_check,N);\n// Verification\nbool flag=1;\nfor(int i = 0; i &lt; N; i++)\n{\nfor(int j = 0; j &lt; N; j++)\n{\nif(c[j*N+i]!= host_check[j*N+i])\n{\nflag=0;\nbreak;\n}\n}\n}\nif (flag==0)\n{\ncout &lt;&lt;\"But,two matrices are not equal\" &lt;&lt; endl;\ncout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n}\nelse\ncout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n// Deallocate host memory\nfree(a); free(b); free(c);\nfree(host_check);\nreturn 0;\n}\n</code></pre> Compilation and Output CUDA-version <pre><code>// compilation\n$ nvcc -arch=sm_70 Matrix-multiplication-shared.cu -o Matrix-multiplication-shared\n// execution\n$ ./Matrix-multiplication-shared\nProgramme assumes that matrix size is N*N Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\nPlease enter the N size number\n$ 256\n// output\n$ Two matrices are equal\n</code></pre> Questions <ul> <li>Could you resize the <code>BLOCK_SIZE</code> number and check the solution's correctness?</li> <li>Can you also create a different kind of thread block and matrix size and check the solution's correctness?</li> <li>Please try with <code>cudaFuncSetCacheConfig</code> and check if you can successfully execute the application. </li> </ul>"},{"location":"cuda/exercise-5/","title":"Unified Memory","text":"<p>Unified memory simplifies the explicit data movement from host to device by programmers. CUDA API will manage the data transfer between CPU and GPU. In this example, we will look into vector addition in GPU using the unified memory concept.</p> <ul> <li>Just one memory allocation is enough <code>cudaMallocManaged()</code>.  The blow table summerise the required steps needed for the unified memory concept. </li> </ul> <p></p> Without unified memory With unified memory Allocate the host memory Allocate the host memory Allocate the device memory Allocate the device memory Initialize the host value Initialize the host value Transfer the host value to the device memory location Transfer the host value to the device memory location Do the computation using the CUDA kernel Do the computation using the CUDA kernel Transfer the data from the device to host Transfer the data from the device to host Free device memory Free device memory Free host memory Free host memory"},{"location":"cuda/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Unified Memory - Vector Addition Without Unified MemoryWith Unified Memory - templateWith Unified Memory-version <pre><code>//-*-C++-*-\n// Without-unified-memory.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n// Initialize the memory on the host\nfloat *a, *b, *out; // Allocate host memory\na = (float*)malloc(sizeof(float) * N);\nb = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N); // Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\na[i] = 1.0f;\nb[i] = 2.0f;\n}\n// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n// Thread organization \ndim3 dimGrid(ceil(N/32), ceil(N/32), 1);\ndim3 dimBlock(32, 32, 1);\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n// Deallocate host memory\nfree(a); free(b); free(out);\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n/*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device(unified) memory\ncudaMallocManaged......\n// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\nd_a[i] = ...\nd_b[i] = ...\n}\n/*\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n// Thread organization \ndim3 dimGrid...    dim3 dimBlock...\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n// synchronize if needed\n......\n/*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", d_out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device(unified) memory\ncudaFree...\n/*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\nreturn 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// With-unified-memory.cu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#define N 5120\n#define MAX_ERR 1e-6\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, float *out, int n) {\nint i = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;   // Allow the   threads only within the size of N\nif(i &lt; n)\n{\nout[i] = a[i] + b[i];\n}\n// Synchronice all the threads \n__syncthreads();\n}\nint main()\n{\n/*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n// Allocate device memory\ncudaMallocManaged(&amp;d_a, sizeof(float) * N);\ncudaMallocManaged(&amp;d_b, sizeof(float) * N);\ncudaMallocManaged(&amp;d_out, sizeof(float) * N); // Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n{\nd_a[i] = 1.0f;\nd_b[i] = 2.0f;\n}\n/*\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n// Thread organization\ndim3 dimGrid(ceil(N/32), ceil(N/32), 1);\ndim3 dimBlock(32, 32, 1);\n// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\ncudaDeviceSynchronize();\n/*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n// Verification\nfor(int i = 0; i &lt; N; i++)\n{\nassert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n}\nprintf(\"out[0] = %f\\n\", d_out[0]);\nprintf(\"PASSED\\n\");\n// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n/*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\nreturn 0;\n}\n</code></pre> Compilation and Output Without-unified-memory.cuWith-unified-memory <pre><code>// compilation\n$ nvcc -arch=compute_70 Without-unified-memory.cu -o Without-Unified-Memory\n// execution \n$ ./Without-Unified-Memory\n// output\n$ ./Without-Unified-Memory\nout[0] = 3.000000\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 With-unified-memory.cu -o With-Unified-Memory\n// execution\n$ ./With-Unified-Memory\n// output\n$ ./With-Unified-Memory out[0] = 3.000000\nPASSED\n</code></pre> Questions <ul> <li>Here in this example, we have used <code>cudaDeviceSynchronize()</code>; can you remove <code>cudaDeviceSynchronize()</code>   and still get a correct solution? if not, why (think)?</li> <li>Please try with different thread blocks and array sizes. </li> </ul>"},{"location":"cuda/preparation/","title":"Preparation","text":""},{"location":"cuda/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"cuda/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200117]$cd u100490\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200117/CUDA .\n[u100490@login03 u100490]$ cd CUDA/\n[u100490@login03 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@login03 CUDA]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200117   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","title":"7. Until now you are in the login node, now its time to do the dry run test","text":"<ul> <li>7.1 Reserve the interactive node for running/testing CUDA applications    <pre><code>$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></li> <li> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> <li>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490    p200117  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#8-now-we-need-to-check-simple-cuda-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check simple CUDA application, if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 CUDA]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-cuda-codes","title":"9. Finally, we need to load the compiler to test the GPU CUDA codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code  <pre><code>$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n### or\n$ source module.sh\n</code></pre></p> </li> <li> check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n[u100490@mel2131 ~]$ module list\n\nCurrently Loaded Modules:\n1) env/release/2022.1           (S)   6) numactl/2.0.14-GCCcore-11.3.0  11) libpciaccess/0.16-GCCcore-11.3.0  16) GDRCopy/2.3-GCCcore-11.3.0                  21) knem/1.1.4.90-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1      (S)   7) CUDA/11.7.0                    12) hwloc/2.7.1-GCCcore-11.3.0        17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0  22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n3) GCCcore/11.3.0                     8) NVHPC/22.7-CUDA-11.7.0         13) OpenSSL/1.1                       18) libfabric/1.15.1-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0         9) XZ/5.2.5-GCCcore-11.3.0        14) libevent/2.1.12-GCCcore-11.3.0    19) PMIx/4.2.2-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0      10) libxml2/2.9.13-GCCcore-11.3.0  15) UCX/1.13.1-GCCcore-11.3.0         20) xpmem/2.6.5-36-GCCcore-11.3.0\n\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre> </li> </ul>"},{"location":"cuda/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>For example, Dry-run-test  <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly for the hands-on session, we need to do the node reservation:","text":"<pre><code>$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre> <ul> <li> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> </ul>"},{"location":"cuda/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands on exercise","text":"<ul> <li>12.1 For example, <code>Hello World</code> example, we do the following steps:</li> </ul> <pre><code>[u100490@mel2063 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@mel2063 CUDA]$ ls\n[u100490@mel2063 CUDA]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Shared-memory  Vector-addition\n[u100490@mel2063 CUDA]$ source module.sh\n[u100490@mel2063 CUDA]$ cd Hello-world\n// compilation\n[u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n[u100490@mel2063 CUDA]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 CUDA]$ Hello World from GPU\n</code></pre>"},{"location":"cuda/profiling/","title":"Profiling and Performance","text":""},{"location":"cuda/profiling/#time-measurement","title":"Time measurement","text":"<p>In CUDA, the execution time can be measured by using the cuda events. CUDA API events shall be created using <code>cudaEvent_t</code>, for example, <code>cudaEvent_t start, stop;</code>. And thereafter, it can be initiated by <code>cudaEventCreate(&amp;start)</code> for start and similarly for stop, it can be created as <code>cudaEventCreate(&amp;stop)</code>. </p> CUDA API <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start,0);\n</code></pre> <p>And it can be initialised to measure the timing as <code>cudaEventRecord(start,0)</code> and <code>cudaEventRecord(stop,0)</code>. Then the timings can be measured as float, for example, <code>cudaEventElapsedTime(&amp;time, start, stop)</code>. Finally, all the events should be destroyed using <code>cudaEventDestroy</code>, for example, <code>cudaEventDestroy(start)</code> and <code>cudaEventDestroy(start)</code>.</p> CUDA API <pre><code>cudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n</code></pre> <p>The following example shows how to measure your GPU kernel call in a CUDA application:</p> Example <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start);\n\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n//use CUDA API to stop the measuring time\ncudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n\ncout &lt;&lt; \" time taken for the GPU kernel\" &lt;&lt; time &lt;&lt; endl;\n</code></pre>"},{"location":"cuda/profiling/#nvidia-system-wide-performance-analysis","title":"Nvidia system-wide performance analysis","text":"<p>Nvidia profiling tools help to analyse the code when it is being spent on the given architecture. Whether it is communication or computation, we can get helpful information through traces and events. This will help the programmer optimise the code performance on the given architecture. For this, Nvidia offers three kinds of profiling options, they are:</p> <ul> <li> <p>Nsight Compute: CUDA application interactive kernel profiler: This will give traces and events of the kernel calls; this further provides both visual profile-GUI and Command Line Interface (CLI) profiling options. <code>ncu -o profile Application.exe</code> command will create an output file <code>profile.ncu-rep</code> which can be opened using <code>ncu-ui</code>. </p> </li> <li> Example <pre><code>$ ncu ./a.out\nmatrix_mul(float *, float *, float *, int), 2023-Mar-12 20:20:45, Context 1, Stream 7\nSection: GPU Speed Of Light Throughput\n---------------------------------------------------------------------- --------------- ------------------------------\nDRAM Frequency                                                           cycle/usecond                         874.24\nSM Frequency                                                             cycle/nsecond                           1.31\nElapsed Cycles                                                                   cycle                         241109\nMemory [%]                                                                           %                          13.68\nDRAM Throughput                                                                      %                           0.07\nDuration                                                                       usecond                         184.35\nL1/TEX Cache Throughput                                                              %                          82.39\nL2 Cache Throughput                                                                  %                          13.68\nSM Active Cycles                                                                 cycle                       30531.99\nCompute (SM) [%]                                                                     %                           1.84\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n      waves across all SMs. Look at Launch Statistics for more details.                                             \n\nSection: Launch Statistics\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Size                                                                                                       1024\nFunction Cache Configuration                                                                  cudaFuncCachePreferNone\nGrid Size                                                                                                          16\nRegisters Per Thread                                                   register/thread                             26\nShared Memory Configuration Size                                                  byte                              0\nDriver Shared Memory Per Block                                              byte/block                              0\nDynamic Shared Memory Per Block                                             byte/block                              0\nStatic Shared Memory Per Block                                              byte/block                              0\nThreads                                                                         thread                          16384\nWaves Per SM                                                                                                     0.10\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80             \n      multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n      concurrently with other workloads, consider reducing the block size to have at least one block per            \n      multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n      Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n      description for more details on launch configurations.                                                        \n\nSection: Occupancy\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Limit SM                                                                   block                             32\nBlock Limit Registers                                                            block                              2\nBlock Limit Shared Mem                                                           block                             32\nBlock Limit Warps                                                                block                              2\nTheoretical Active Warps per SM                                                   warp                             64\nTheoretical Occupancy                                                                %                            100\nAchieved Occupancy                                                                   %                          45.48\nAchieved Active Warps Per SM                                                      warp                          29.11\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n      theoretical (100.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads   \n      or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n      as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n      (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n      optimizing occupancy.                                                                                         \n</code></pre> </li> <li> <p>Nsight Graphics: Graphics application frame debugger and profiler: This is quite useful for analysing the profiling results through GUI. </p> </li> <li> <p>Nsight Systems: System-wide performance analysis tool: It is needed when we try to do heterogeneous computation profiling, for example, mixing MPI and OpenMP with CUDA. This will profile the system-wide application, that is, both CPU and GPU. To learn more about the command line options, please use <code>$ nsys profile --help</code></p> </li> <li> Example <pre><code>$ nsys profile -t nvtx,cuda --stats=true ./a.out\nGenerating '/scratch_local/nsys-report-ddd1.qdstrm'\n[1/7] [========================100%] report1.nsys-rep\n[2/7] [========================100%] report1.sqlite\n[3/7] Executing 'nvtxsum' stats report\nSKIPPED: /m100/home/userexternal/ekrishna/Teaching/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n[4/7] Executing 'cudaapisum' stats report\n\nTime (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)  Min (ns)  Max (ns)   StdDev (ns)        Name      \n--------  ---------------  ---------  -----------  --------  --------  ---------  -----------  ----------------\n    99.7        398381310          3  132793770.0    8556.0      6986  398365768  229992096.8  cudaMalloc      \n     0.2           714256          3     238085.3   29993.0     24944     659319     364807.8  cudaFree        \n     0.1           312388          3     104129.3   43405.0     37692     231291     110162.3  cudaMemcpy      \n     0.0            51898          1      51898.0   51898.0     51898      51898          0.0  cudaLaunchKernel\n\n[5/7] Executing 'gpukernsum' stats report\n\nTime (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)     GridXYZ         BlockXYZ                        Name                   \n--------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------  --------------  ------------------------------------------\n100.0           181949          1  181949.0  181949.0    181949    181949          0.0     4    4    1    32   32    1  matrix_mul(float *, float *, float *, int)\n\n[6/7] Executing 'gpumemtimesum' stats report\n\nTime (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n--------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n 75.0            11520      2    5760.0    5760.0      5760      5760          0.0  [CUDA memcpy HtoD]\n 25.0             3840      1    3840.0    3840.0      3840      3840          0.0  [CUDA memcpy DtoH]\n\n[7/7] Executing 'gpumemsizesum' stats report\n\nTotal (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n----------  -----  --------  --------  --------  --------  -----------  ------------------\n  0.080      2     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy HtoD]\n  0.040      1     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy DtoH]\n\nGenerated:\n   /m100/home/userexternal/ekrishna/Teaching/report1.nsys-rep\n   /m100/home/userexternal/ekrishna/Teaching/report1.sqlite\n</code></pre> </li> </ul>"},{"location":"cuda/profiling/#occupancy","title":"Occupancy","text":"<p>The CUDA Occupancy Calculator allows you to compute the multiprocessor occupancy of a Nvidia GPU microarchitecture by a given CUDA kernel. The multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU.</p> <p>\\(Occupancy  = \\frac{Active\\ warps\\ per\\ SM}{ Max.\\ warps\\ per\\ SM}\\)</p> <ul> <li> Examples Occupancy CUDACompilation and results <pre><code>//-*-C++-*-\n#include&lt;iostream&gt;\n// Device code\n__global__ void MyKernel(int *d, int *a, int *b)\n{\nint idx = threadIdx.x + blockIdx.x * blockDim.x;\nd[idx] = a[idx] * b[idx];\n}\n// Host code\nint main()\n{\n// set your numBlocks and blockSize to get 100% occupancy\nint numBlocks = 32;        // Occupancy in terms of active blocks\nint blockSize = 128;\n// These variables are used to convert occupancy to warps\nint device;\ncudaDeviceProp prop;\nint activeWarps;\nint maxWarps;\ncudaGetDevice(&amp;device);\ncudaGetDeviceProperties(&amp;prop, device);\ncudaOccupancyMaxActiveBlocksPerMultiprocessor(\n&amp;numBlocks,\nMyKernel,\nblockSize,0);\nactiveWarps = numBlocks * blockSize / prop.warpSize;\nmaxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\nstd::cout &lt;&lt; \"Max # of Blocks : \" &lt;&lt; numBlocks &lt;&lt; std::endl;\nstd::cout &lt;&lt; \"ActiveWarps : \" &lt;&lt; activeWarps &lt;&lt; std::endl;\nstd::cout &lt;&lt; \"MaxWarps : \" &lt;&lt; maxWarps &lt;&lt; std::endl;\nstd::cout &lt;&lt; \"Occupancy: \" &lt;&lt; (double)activeWarps / maxWarps * 100 &lt;&lt; \"%\" &lt;&lt; std::endl;\nreturn 0;\n}\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 occupancy.cu -o Occupancy-GPU\n// execution\n$ ./Occupancy-GPU\n// output\nMax number of Blocks : 16\nActiveWarps : 64\nMaxWarps : 64\nOccupancy: 100%\n</code></pre> </li> </ul> Questions <ul> <li>Occupancy: can you change <code>numBlocks</code> and <code>blockSize</code> in Occupancy.cu code  and check how it affects or predicts the occupancy of the given Nvidia microarchitecture?</li> <li>Profiling: run your <code>Matrix-multiplication.cu</code> and <code>Vector-addition.cu</code> code and observe what you notice?  for example, how to improve the occupancy? Or maximise a GPU utilization?</li> <li>Timing: using CUDA events API can you measure your GPU kernel execution, and compare how fast is your GPU computation compared to CPU computation?</li> </ul>"},{"location":"openacc/","title":"Introduction to OpenACC for Heterogeneous Computing","text":"<p>Participants from this course will learn GPU programming using the OpenACC programming model, such as compute constructs, loop constructs and data clauses. Furthermore, understanding the GPU architecture and how parallel threads blocks are created and used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the OpenACC programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the OpenACC programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openacc/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openacc/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture </li> <li>Threads blocks </li> </ul> </li> <li>Implement OpenACC programming model  <ul> <li>Compute constructs  </li> <li>Loop constructs </li> <li>Data clauses</li> </ul> </li> <li>Efficient handling of memory management  <ul> <li>Host to Device </li> <li>Unified memory </li> </ul> </li> <li>Apply the OpenACC programming knowledge to accelerate examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openacc/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No GPU programming knowledge is required; however, knowing the OpenMP programming model is advantageous. </p>"},{"location":"openacc/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openmp/","title":"Introduction to OpenMP Programming for Shared Memory Parallel Architecture","text":"<p>Participants from this course will learn Multicore (shared memory) CPU programming using the OpenMP programming model, such as parallel region, environmental routines, and data sharing. Furthermore, understanding the multicore shared memory architecture and how parallel threads blocks are used to parallelise the computational task. Since we deal with multicores and parallel threads, proper parallel work sharing and the synchronisation of the parallel calls are to be studied in detail. Finally, participants will also learn to use the OpenMP programming model to accelerate linear algebra (routines) and iterative solvers on the Multicore CPU. Participants will learn theories first and implement the OpenMP programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openmp/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openmp/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the shared memory architecture <ul> <li>Unified Memory Access (UMA) and Non-Unified Memory Access (NUMA)  </li> <li>Hybrid distributed shared memory architecture  </li> </ul> </li> <li>Implement OpenMP programming model  <ul> <li>Parallel region  </li> <li>Environment routines  </li> <li>Data sharing  </li> </ul> </li> <li>Efficient handling of OpenMP constructs  <ul> <li>Work sharing  </li> <li>Synchronisation constructs  </li> <li>Single Instruction Multiple Data (SIMD) directive </li> </ul> </li> <li>Apply the OpenMP programming knowledge to parallelise examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openmp/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No prior parallel programming experience is needed.</p>"},{"location":"openmp/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"}]}