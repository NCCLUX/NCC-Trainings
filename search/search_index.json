{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Trainings organization","text":"<p>Most of our courses are organised online.</p>"},{"location":"#courses","title":"<p>Courses</p>","text":"<p>Courses are primarily designed to be completed in just one day. This format is considered a more effective way of learning than spreading the training over multiple days. The courses focus mainly on parallel programming on CPUs and GPUs and the acceleration of scientific software on supercomputers. Examples include OpenMP, MPI, OpenACC, CUDA, and OpenMP Offloading, using programming languages such as C/C++ and FORTRAN. Additionally, we also focus on Python, R, Matlab, and Julia for parallelizing applications. Regarding software acceleration, courses on GROMACS, CFD and FEM software, material science, bioinformatics, and related topics will be offered.</p> <ul> <li>The one-day training consists of three stages:<ul> <li>Preparation: Pre-preparation to follow up on the lecture and practicals</li> <li>Lecture: Introduction and focus on the course topic</li> <li>Practicals: Hands-on session</li> </ul> </li> </ul>"},{"location":"#bootcamps","title":"<p>Bootcamps</p>","text":"<p>Bootcamps are usually one- or two-day events designed to teach scientists and researchers how to quickly start accelerating codes on modern processors (such as GPUs). Participants will be introduced to available libraries, programming models, and platforms. They will learn the basics of parallel programming (CPU, GPU, and hybrid) through extensive hands-on collaboration using real-life codes based on the parallel programming model.</p>"},{"location":"#hackathons","title":"<p>Hackathons</p>","text":"<p>Hackathons are held over a longer duration, typically lasting one or two months. During the hackathons, participants focus primarily on HPC-related problems (HPC, AI, and HPDA) from industry, particularly local Luxembourg industries. Participants will be divided into multiple groups, and each group will be assigned mentors from the organizers (such as Nvidia) as well as experts from the Supercomputing NCC Luxembourg. At the end of the event, a winner will be selected, and a prize will be awarded for outstanding contributions.</p> <p> </p>"},{"location":"Bootcamps/ai/handson/","title":"Hands-on","text":""},{"location":"Bootcamps/ai/handson/#materials-and-timeline","title":"Materials and timeline","text":"<p>This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email.</p> <p>Day 1 \u2013 Thursday, February 9<sup>th</sup> 2023: 01:30 PM \u2013 05:00 PM</p> <p></p> <pre><code>01:30 PM \u2013 01:45 PM: Welcome (Moderator)\n01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture)\n02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture)\n03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab)\n</code></pre> <p>Day 2 \u2013 Friday, February 10<sup>th</sup>  2023: 01:30 PM \u2013 05:00 PM</p> <p></p> <pre><code>01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge)\n04:45 PM \u2013 05:00 PM: Wrap up and QA\n</code></pre>"},{"location":"Bootcamps/ai/handson/#further-more-information-can-be-found-in-here","title":"Further more information can be found in here!","text":""},{"location":"Bootcamps/ai/introduction/","title":"Introduction","text":"<p>The NCC Supercompuing Luxembourg, in collaboration with NVIDIA  and OpenACC.org, is hosting online the AI for Science and Engineering Bootcamp during 2 half-days. The first part will be dedicated to theory, and the second part will focus on hands-on challenges on GPU accelerators of the MeluXina supercomputer. For whom? </p> <p>Both current or prospective users of large hybrid CPU/GPU clusters, which develop HPC and AI applications and could benefit from GPU acceleration, are encouraged to participate! What will you learn and how? </p> <p>During this online Bootcamp, participants will learn how to apply AI tools, techniques, and algorithms to real-life problems. Participants will be introduced to the critical concepts of Deep Neural Networks, how to build Deep Learning models, and how to measure and improve the accuracy of their models. Participants will also learn essential data pre-processing techniques to ensure a robust machine-learning pipeline. The Bootcamp is a hands-on learning experience where mentors guide participants. Learning outcomes</p>"},{"location":"Bootcamps/ai/introduction/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<pre><code>- Apply Deep Convolutional Neural Networks for science and engineering applications\n- Understand the Classification (multi-class classification) methodology in AI\n- Implement AI algorithms using Keras (e.g. TensorFlow)\n- Use an efficient usage of the GPU for AI algorithms (e.g. CNN) with handling large data set\n- Run AI applications in the Jupyter notebook environment (and understand singularity containers)\n</code></pre>"},{"location":"Bootcamps/ai/introduction/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with basic experience with Python. No GPU programming knowledge is required. GPU Compute Resource</p> <p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer\u202fduring the hackathon. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide. Agenda</p>"},{"location":"Bootcamps/ai/introduction/#materials-and-timeline","title":"Materials and timeline","text":"<p>This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email.</p> <p>Day 1 \u2013 Thursday, February 9<sup>th</sup> 2023: 01:30 PM \u2013 05:00 PM</p> <p></p> <pre><code>01:30 PM \u2013 01:45 PM: Welcome (Moderator)\n01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture)\n02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture)\n03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab)\n</code></pre> <p>Day 2 \u2013 Friday, February 10<sup>th</sup>  2023: 01:30 PM \u2013 05:00 PM</p> <p></p> <pre><code>01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge)\n04:45 PM \u2013 05:00 PM: Wrap up and QA\n</code></pre>"},{"location":"Bootcamps/ai/preparation/","title":"Preparation","text":""},{"location":"Bootcamps/ai/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>Please take a look if you are using Windows</li> <li>Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"Bootcamps/ai/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<p>For exmaple the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n</code></pre></p>"},{"location":"Bootcamps/ai/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory     <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that go to project directory (Nvidia Bootcamp activites).    <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"Bootcamps/ai/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory-for-example-here-it-is-user-with-u100490","title":"4. And please create your own working folder under the project directory, for example, here it is user with <code>u100490</code>:","text":"<pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre> <ul> <li> <p>4.1 Now copy climate.simg and climate.sh from project direcoty to your user directory (for exxample, here is <code>u100490</code>) directory:    <pre><code>[u100490@login02 p200117]$ cp /project/home/p200117/climate.simg /project/home/p200117/u100490\n[u100490@login02 p200117]$ cp /project/home/p200117/climate.sh /project/home/p200117/u100490\n</code></pre></p> </li> <li> <p>4.2 Similary, copy cfd.simg and cfd.sh from project direcoty to your user directory (for example, here is <code>u100490</code>) directory:    <pre><code>[u100490@login02 p200117]$ cp /project/home/p200117/cfd.simg /project/home/p200117/u100490\n[u100490@login02 p200117]$ cp /project/home/p200117/cfd.sh /project/home/p200117/u100490\n</code></pre></p> </li> <li>4.3 Now go to your home (for example, here it is <code>u100490</code>) directory check if all the necessary files are there (.simg and .sh)    <pre><code>[u100490@login02 p200117]$ cd u100490\n[u100490@login02 u100490]$ pwd\n[u100490@login02 u100490]$ /project/home/p200117/u100490\n[u100490@login02 u100490]$ ls -lthr\ntotal 15G\n-rw-r-x---. 1 u100490 p200117  736 Feb  8 18:59 climate.sh\n-rwxr-x---. 1 u100490 p200117 7.2G Feb  8 19:19 climate.simg\n-rwxr-x---. 1 u100490 p200117 6.9G Feb  8 19:21 cfd.simg\n-rw-r-x---. 1 u100490 p200117  723 Feb  8 19:21 cfd.sh\n</code></pre></li> </ul>"},{"location":"Bootcamps/ai/preparation/#5-for-the-dry-run-9th-february-from-1130-1230-please-follow-the-following-steps","title":"5. For the dry run (9<sup>th</sup> February from 11:30-12:30), please follow the following steps:","text":"<pre><code>[u100490@login02 u100490]$ salloc -A p200117 --res gpudev -q dev -N 1 -t 01:00:0\n[u100490@mel2123 u100490]$ mkdir -p $PROJECT/$USER/workspace-climate\n[u100490@mel2123 u100490]$ module load Singularity-CE/3.10.2-GCCcore-11.3.0\n\n[u100490@mel2123 u100490]$ singularity run --bind $PROJECT/$USER $PROJECT/$USER/climate.simg cp -rT /workspace $PROJECT/$USER/workspace-climate\nINFO:    Converting SIF file to temporary sandbox...\nINFO:    Cleaning up image...\n[u100490@mel2123 u100490]$ singularity run --nv --bind $PROJECT/$USER $PROJECT/$USER/climate.simg jupyter lab --notebook-dir=$PROJECT/$USER/workspace-climate/python/jupyter_notebook --port=8888 --ip=0.0.0.0 --no-browser --NotebookApp.token=\"\"\nINFO:    Converting SIF file to temporary sandbox...\nWARNING: underlay of /usr/bin/nvidia-smi required more than 50 (452) bind mounts\n[W 10:10:32.723 LabApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n[I 10:10:33.043 LabApp] jupyter_tensorboard extension loaded.\n[I 10:10:33.047 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.8/dist-packages/jupyterlab\n[I 10:10:33.047 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab\n[I 10:10:33.048 LabApp] [Jupytext Server Extension] NotebookApp.contents_manager_class is (a subclass of) jupytext.TextFileContentsManager already - OK\n[I 10:10:33.048 LabApp] Serving notebooks from local directory: /mnt/tier2/project/p200117/u100490/workspace-climate/python/jupyter_notebook\n[I 10:10:33.048 LabApp] Jupyter Notebook 6.2.0 is running at:\n[I 10:10:33.048 LabApp] http://hostname:8888/\n[I 10:10:33.049 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre> <ul> <li> <p>5.1 Now open a new terminal on your local computer, and again login to MeluXina to access the port</p> </li> <li> <p>5.2 Make sure you use the name NODELIST (for example, here it is <code>mel2123</code> - from <code>squeue</code> command you will get this number)    <pre><code>ssh -L8080:NODELIST:8888 USERNAME@login.lxp.lu -p 8822\nssh -L8080:mel2123:8888 u100490@login.lxp.lu -p 8822\n</code></pre></p> </li> <li> <p>5.3 Keep those terminals open/alive (please do not close them)</p> </li> <li> <p>5.4 Now copy and paste localhost to your browser either to Chrome or FireFox    <pre><code>http://localhost:8080\n</code></pre></p> </li> </ul>"},{"location":"Bootcamps/ai/preparation/#6-for-the-afternoon-session-9th-and-10th-february","title":"6. For the afternoon session (9<sup>th</sup> and 10<sup>th</sup> February)","text":"<p>If have missed the dry run session, please go through the steps from 1-4</p> <ul> <li>6.1 Now it is time to edit your batch script (climate.sh) before launching your Jupyter notebook, please follow the following steps:     <pre><code>[u100490@login02 u100490]$ emacs(emacs -nw)/vim climate.sh\n#!/bin/bash -l\n#SBATCH --partition=gpu \n#SBATCH --ntasks=1\n#SBATCH --nodes=1    \n############  day one ##########\n#######SBATCH --time=02:00:00         ## use this option for day one\n#######SBATCH --res ai_bootcamp_day1   ## use this option for day one\n################################\n\n############  day two ##########\n#SBATCH --time=03:30:00         ## use this option for day two\n#SBATCH --res ai_bootcamp_day2  ## use this option for day two\n################################\n#SBATCH -A p200117\n#SBATCH --qos default\n\nmkdir -p $PROJECT/$USER/workspace-climate\nmodule load Singularity-CE/3.10.2-GCCcore-11.3.0\n\nsingularity run --bind $PROJECT/$USER $PROJECT/$USER/climate.simg cp -rT /workspace $PROJECT/$USER/workspace-climate\nsingularity run --nv --bind $PROJECT/$USER $PROJECT/$USER/climate.simg jupyter lab --notebook-dir=$PROJECT/$USER/workspace-climate/python/jupyter_notebook --port=8888 --ip=0.0.0.0 --no-browser --NotebookApp.token=\"\"\n</code></pre></li> <li>6.2 Once you have modified your climate.sh, please launch your batch script as below:    <pre><code>[u100490@login03 u100490]$ sbatch climate.sh\nSubmitted batch job 276009\n[u100490@login03 u100490]$ squeue \nJOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n276009       gpu climate.  u100490    p200117  RUNNING       0:16        20:00      1 mel2077\n</code></pre></li> <li>6.3 Now you have initiated your singularity container which will help you to open the Jupyter nootebook    <pre><code>[u100490@login03 u100490]$ ls -lthr\ntotal 7.2G\n-rwxr-x---. 1 u100490 p200117 7.2G Feb  3 14:53 climate.simg\n-rw-r-----. 1 u100490 p200117  613 Feb  3 17:06 climate.sh\n-rw-r-x---. 1 u100490 p200117  724 Feb  8 19:41 cfd.sh\n-rwxr-x---. 1 u100490 p200117 6.9G Feb  8 19:42 cfd.simg\n-rw-r--r--. 1 u100490 p200117 1.1K Feb  3 17:58 slurm-276009.out\n</code></pre></li> <li>6.4 You can also check meantime if everything OK by executing the below command and you should get similar output:    <pre><code>[u100490@login03 u100490]$ head -30 slurm-276009.out \nINFO:    Converting SIF file to temporary sandbox...\nINFO:    Cleaning up image...\nINFO:    Converting SIF file to temporary sandbox...\nWARNING: underlay of /usr/bin/nvidia-smi required more than 50 (452) bind mounts\n[W 17:58:37.489 LabApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n[I 17:58:37.807 LabApp] jupyter_tensorboard extension loaded.\n[I 17:58:37.811 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.8/dist-packages/jupyterlab\n[I 17:58:37.811 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab\n[I 17:58:37.813 LabApp] [Jupytext Server Extension] NotebookApp.contents_manager_class is (a subclass of) jupytext.TextFileContentsManager already - OK\n[I 17:58:37.813 LabApp] Serving notebooks from local directory: /mnt/tier2/project/p200117/u100490/workspace-climate/python/jupyter_notebook\n[I 17:58:37.813 LabApp] Jupyter Notebook 6.2.0 is running at:\n[I 17:58:37.813 LabApp] http://hostname:8888/\n[I 17:58:37.813 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre></li> <li>6.5 Now open a new terminal on your local computer, and again login to MeluXina to access the port</li> <li>6.6 Make sure you use the name NODELIST (here it is <code>mel2077</code> - from <code>squeue</code> command you will get this number)    <pre><code>ssh -L8080:NODELIST:8888 USERNAME@login.lxp.lu -p 8822\nssh -L8080:mel2077:8888 u100490@login.lxp.lu -p 8822\n</code></pre></li> <li>6.7 Keep those terminals open/alive (please do not close them)</li> <li>6.8 Now copy and paste localhost to your browser either to Chrome or FireFox    <pre><code>http://localhost:8080\n</code></pre></li> </ul>"},{"location":"Hackathons/hpda/introduction/","title":"Introduction","text":""},{"location":"Hackathons/hpda/introduction/#high-performacne-data-analytics-hpda-with-nvidia-and-ceratizit-2024","title":"High Performacne Data Analytics (HPDA) with Nvidia and CERATIZIT - 2024","text":"<p>This will be organized within the year of 2024.</p>"},{"location":"calendar/future/","title":"Future Events","text":""},{"location":"calendar/future/#2025","title":"2025","text":""},{"location":"calendar/future/#1-introduction-to-gpu-programming-using-cuda-21st-january-2025-registration","title":"1) Introduction to GPU programming using CUDA, 21<sup>st</sup> January, 2025 Registration","text":""},{"location":"calendar/future/#2-introduction-to-use-python-in-the-hpc-30-31st-january-2025-registration","title":"2) Introduction to use Python in the HPC, 30-31<sup>st</sup>, January, 2025 Registration","text":""},{"location":"calendar/future/#3-numerical-libraries-georgios-xxx-2025","title":"3) Numerical Libraries (Georgios), xxx, 2025","text":""},{"location":"calendar/future/#4-mpi-for-python-oscar-xxx-2025","title":"4) MPI for Python (Oscar), xxx, 2025","text":""},{"location":"calendar/future/#5-introduction-to-openacc-programming-model-25th-march-2025-registration","title":"5) Introduction to OpenACC Programming Model, 25<sup>th</sup> March, 2025 Registration","text":""},{"location":"calendar/past/","title":"Past Events","text":""},{"location":"calendar/past/#2023","title":"2023","text":""},{"location":"calendar/past/#1-nvidia-bootcamp-aiml","title":"1) Nvidia Bootcamp: AI/ML","text":""},{"location":"calendar/past/#2-introduction-to-gpu-programming-using-cuda-27th-march-2023","title":"2) Introduction to GPU programming using CUDA, 27<sup>th</sup> March, 2023","text":""},{"location":"calendar/past/#3-introduction-to-openmp-programming-31st-may-2023","title":"3) Introduction to OpenMP Programming, 31<sup>st</sup> May, 2023","text":""},{"location":"calendar/past/#4-introduction-to-openacc-programing-model-19th-september-2023","title":"4) Introduction to OpenACC Programing model, 19<sup>th</sup> September, 2023","text":""},{"location":"calendar/past/#2024","title":"2024","text":""},{"location":"calendar/past/#1-introduction-to-gpu-programming-using-cuda-30th-april-2024","title":"1) Introduction to GPU programming using CUDA, 30<sup>th</sup>, April, 2024","text":""},{"location":"calendar/past/#2-python-in-the-hpc-july-2024","title":"2) Python in the HPC, July, 2024","text":""},{"location":"cuda/","title":"Introduction to GPU programming using CUDA","text":"<p>Participants from this course will learn GPU programming using the CUDA programming model, which includes synchronisation, memory allocation, and device and host calls. Furthermore, understanding the GPU architecture and how parallel threads blocks are used to parallelise the computational task. Moreover, the GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the CUDA programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the CUDA programming model with mentors\u2019 guidance later in the hands-on tutorial part.</p>"},{"location":"cuda/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"cuda/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture</li> <li>Threads blocks</li> </ul> </li> <li>Implement CUDA programming model<ul> <li>Programming structure</li> <li>Device calls (threads block organisation)</li> <li>Host calls</li> </ul> </li> <li>Efficient handling of memory management<ul> <li>Host to Device</li> <li>Unified memory</li> </ul> </li> <li>Apply the CUDA programming knowledge to accelerate examples from science and engineering<ul> <li>Iterative solvers from science and engineering</li> <li>Matrix multiplication, vector addition, etc</li> </ul> </li> </ul>"},{"location":"cuda/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++. No GPU programming knowledge is required. However, knowing some basic parallel programming concepts are advantage but not necessary. </p>"},{"location":"cuda/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"cuda/#course-organization-and-registration","title":"Course Organization and Registration","text":"<p>Format - Online  Previous events: 27<sup>th</sup> March, 2023 and 30<sup>th</sup>, April, 2024  Next Event: 21<sup>st</sup>, January, 2025 and 24<sup>th</sup>, June, 2025</p> <p>Registration: 21<sup>st</sup>, January, 2025</p> <p>Registration: 24<sup>th</sup>, June, 2025</p>"},{"location":"cuda/exercise-1/","title":"Hello World","text":"<p>Our first exercise would be to print out \"Hello World\" from the GPU. To do that, we need to do the following things:</p> <ul> <li>Run a part or the entire application on the GPU.</li> <li>Call the CUDA function on a device.</li> <li>It should be called using the function qualifier <code>__global__</code>.</li> <li>Call the device function in the main program:</li> <li>C/C++ example, <code>c_function()</code>.</li> <li>CUDA example, <code>cuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;()</code> (just using 1 thread).</li> <li><code>&lt;&lt;&lt; &gt;&gt;&gt;</code>, specify the thread blocks within the brackets.</li> <li>Make sure to synchronize the threads.</li> <li><code>__syncthreads()</code> synchronizes all the threads within a thread block.</li> <li><code>CudaDeviceSynchronize()</code> synchronizes a kernel call on the host.</li> <li>Most of the CUDA APIs are synchronized calls by default, but sometimes it is good to call explicit synchronized calls to avoid errors in the computation.</li> </ul>"},{"location":"cuda/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-versionCUDA-version <pre><code>//-*-C++-*-\n// Hello-world.c\n\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\nvoid c_function()\n{\n  printf(\"Hello World!\\n\");\n}\n\nint main()\n{\n  c_function();\n  return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Hello-world.cu\n\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\n// device function will be executed on device (GPU) \n__global__ void cuda_function()\n{\n  printf(\"Hello World from GPU!\\n\");\n\n  // synchronize all the threads\n  __syncthreads();\n}\n\nint main()\n{\n  // call the kernel function \n  cuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;();\n\n  // synchronize the device kernel call\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Hello-world.c -o Hello-World-CPU\n\n// execution \n$ ./Hello-World-CPU\n\n// output\n$ Hello World from CPU!\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n</code></pre> Questions <p>Right now, you are printing just one <code>Hello World from GPU</code>, but what if you would like to print more <code>Hello World from GPU</code>? How can you do that?</p> QuestionAnswerSolution Output <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\n__global__ void cuda_function()\n{\n  printf(\"Hello World from GPU!\\n\");\n  __syncthreads();\n}\n\nint main()\n{\n  // define your thread block here\n  cuda_function&lt;&lt;&lt;&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\n__global__ void cuda_function()\n{\n  printf(\"Hello World from GPU!\\n\");\n  __syncthreads();\n}\n\nint main()\n{\n  // define your thread block here\n  cuda_function&lt;&lt;&lt;10,1&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre> <pre><code>Hello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\n</code></pre>"},{"location":"cuda/exercise-2/","title":"Vector Addition","text":"<p>In this hands-on session, we will delve into the fundamentals of vector addition on the Graphics Processing Unit (GPU) utilizing the CUDA programming model. This tutorial is particularly suitable for those new to parallel computing, as arithmetic operations on matrices and vectors are common tasks in various computational applications. To execute these operations efficiently, it is imperative to understand how to access and manipulate indices within these data structures. Our focus will be on the Single Instruction, Multiple Threads (SIMT) execution model, where we will explore the process of adding two vectors in parallel.</p> <ul> <li>A key aspect of this process involves memory allocation on both the Central Processing Unit (CPU) and the GPU. As addressed earlier, the GPU operates as an accelerator and not as a host machine. Consequently, computation tasks are initiated through the CPU. This means that we need to initialize the necessary data on the host before proceeding. Concurrently, we will allocate memory on the GPU and facilitate the transfer of data from the CPU to the GPU, ensuring that our computations can be carried out effectively.</li> </ul> <p></p> <ul> <li> <p>Allocating the CPU memory for a, b, and out vector <pre><code>// Initialize the memory on the host\nfloat *a, *b, *out;\n\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout   = (float*)malloc(sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for d_a, d_b, and d_out matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Now, we need to fill in the values for the     arrays a and b.  <pre><code>// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n  {\n    a[i] = 1.0f;\n    b[i] = 2.0f;\n  }\n</code></pre></p> </li> <li> <p>Transfer initialized value from CPU to GPU <pre><code>// Transfer data from a host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>Creating a 2D thread block <pre><code>// Thread organization \ndim3 dimGrid(1, 1, 1);    \ndim3 dimBlock(16, 16, 1); \n</code></pre></p> Conversion of thread blocks <pre><code>//1D grid of 1D blocks\n__device__ int getGlobalIdx_1D_1D()\n{\n  return blockIdx.x * blockDim.x + threadIdx.x;\n}\n\n\n//1D grid of 2D blocks\n__device__ int getGlobalIdx_1D_2D()\n{\n  return blockIdx.x * blockDim.x * blockDim.y\n      + threadIdx.y * blockDim.x + threadIdx.x;\n}\n\n//1D grid of 3D blocks\n__device__ int getGlobalIdx_1D_3D()\n{\n  return blockIdx.x * blockDim.x * blockDim.y * blockDim.z \n    + threadIdx.z * blockDim.y * blockDim.x\n    + threadIdx.y * blockDim.x + threadIdx.x;\n}            \n\n//2D grid of 1D blocks \n__device__ int getGlobalIdx_2D_1D()\n{\n  int blockId   = blockIdx.y * gridDim.x + blockIdx.x;\n  int threadId = blockId * blockDim.x + threadIdx.x; \n  return threadId;\n}\n\n//2D grid of 2D blocks  \n __device__ int getGlobalIdx_2D_2D()\n{\n  int blockId = blockIdx.x + blockIdx.y * gridDim.x; \n  int threadId = blockId * (blockDim.x * blockDim.y) +\n    (threadIdx.y * blockDim.x) + threadIdx.x;\n  return threadId;\n}\n\n//2D grid of 3D blocks\n__device__ int getGlobalIdx_2D_3D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x; \n  int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n   + (threadIdx.z * (blockDim.x * blockDim.y))\n   + (threadIdx.y * blockDim.x)\n   + threadIdx.x;\n  return threadId;\n}\n\n//3D grid of 1D blocks\n__device__ int getGlobalIdx_3D_1D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x \n    + gridDim.x * gridDim.y * blockIdx.z; \n  int threadId = blockId * blockDim.x + threadIdx.x;\n  return threadId;\n}\n\n//3D grid of 2D blocks\n__device__ int getGlobalIdx_3D_2D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x \n    + gridDim.x * gridDim.y * blockIdx.z; \n  int threadId = blockId * (blockDim.x * blockDim.y)\n    + (threadIdx.y * blockDim.x)\n    + threadIdx.x;\n  return threadId;\n}\n\n//3D grid of 3D blocks\n__device__ int getGlobalIdx_3D_3D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x \n    + gridDim.x * gridDim.y * blockIdx.z; \n  int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n    + (threadIdx.z * (blockDim.x * blockDim.y))\n    + (threadIdx.y * blockDim.x)\n    + threadIdx.x;\n  return threadId;\n</code></pre> </li> <li> <p>Calling the kernel function <pre><code>// Execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n</code></pre></p> </li> <li> <p>Vector addition kernel function call definition</p> vector addition function call Serial-versionCUDA-version <pre><code>// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      out[i] = a[i] + b[i];\n    }\n  return out;\n}\n</code></pre> <pre><code>// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n       float *out, int n) \n{\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n  __syncthreads();\n}\n</code></pre> </li> </ul> <p> </p> <ul> <li> <p>Copy back computed value from GPU to CPU <pre><code>// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n\n// Deallocate host memory\nfree(a); \nfree(b); \nfree(out);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-2/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial-versionCUDA-templateCUDA-version <pre><code>//-*-C++-*-\n// Vector-addition.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      out[i] = a[i] + b[i];\n    }\n  return out;\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out;       \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  out = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Add(a, b, out, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(out);\n\n  return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition-template.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \nfloat *out, int n) \n{     \n  // Allign your thread id indexes \n  int i = ........\n\n  // Allow the   threads only within the size of N\n  if------\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a   = (float*)......\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a,......\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = ....\n      b[i] = ....\n    }\n\n  // Transfer data from a host to device memory\n  cudaMemcpy.....\n\n  // Thread organization \n  dim3 dimGrid....  \n  dim3 dimBlock....\n\n  // execute the CUDA kernel function \n  vector_add&lt;&lt;&lt; &gt;&gt;&gt;....\n\n  // Transfer data back to host memory\n  cudaMemcpy....\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n     {\n       assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n     }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate device memory\n  cudaFree...\n\n\n  // Deallocate host memory\n  free..\n\n  return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Vector-addition.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \nfloat *out, int n) \n{\n\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  out = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_out, sizeof(float) * N); \n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Transfer data from a host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n\n  // Thread organization \n  dim3 dimGrid(ceil(N/32), ceil(N/32), 1);\n  dim3 dimBlock(32, 32, 1); \n\n  // Execute the CUDA kernel function \n  vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n     {\n       assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n     }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_out);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(out);\n\n  return 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ gcc Vector-addition.c -o Vector-Addition-CPU\n\n// execution \n$ ./Vector-Addition-CPU\n\n// output\n$ ./Vector-addition-CPU \nout[0] = 3.000000\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Vector-addition.cu -o Vector-Addition-GPU\n\n// execution\n$ ./Vector-Addition-GPU\n\n// output\n$ ./Vector-addition-GPU\nout[0] = 3.000000\nPASSED\n</code></pre> Questions <ul> <li>What happens if you remove the syncthreads(); from the __global void vector_add(float *a, float *b, float *out, int n) function?</li> <li>Can you remove the if condition if(i &lt; n) from the global void vector_add(float *a, float *b, float *out, int n) function? If so, how can you do that? Here, we do not use cudaDeviceSynchronize() in the main application. Can you figure out why we do not need it?</li> <li>Can you create a different thread block for a larger number of arrays?</li> </ul>"},{"location":"cuda/exercise-3/","title":"Matrix Multiplication","text":"<p>We will now look into basic matrix multiplication. In this tutorial, we will explore the process of matrix multiplication, a fundamental operation in linear algebra. Matrix multiplication is executed through a nested loop structure, which is a common approach for numerous computational tasks involving matrices. Mastering this example will provide valuable insight and practical experience in handling nested loops, a crucial skill for solving complex problems in future applications. </p> <p></p> b <ul> <li> <p>Allocating the CPU memory for A, B, and C matrices.    Here, we notice that the matrix is stored in a    1D array because we want to consider the same function concept for CPU and GPU. <pre><code>// Initialize the memory on the host\nfloat *a, *b, *c;\n\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for A, B, and C matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_c;\n\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Now, we need to fill in the values for the matrix A and B. <pre><code>// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n   {\n    a[i] = 2.0f;\n    b[i] = 2.0f;\n   }\n</code></pre></p> </li> <li> <p>Transfer initialized A and B matrix from CPU to GPU <pre><code>cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>2D thread block for indexing x and y <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></p> </li> <li> <p>Calling the kernel function <pre><code>// Device function call\nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n</code></pre></p> matrix multiplication function call serialcuda <pre><code>float * matrix_mul(float *h_a, float *h_b, float *h_c, int width)\n{\n  for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          float temp = 0;\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              temp += h_a[row*width+i] * h_b[i*width+col];\n            }\n          h_c[row*width+col] = temp;\n        }\n    }\n  return h_c;\n}\n</code></pre> <pre><code>__global__ void matrix_mul(float* d_a, float* d_b, \nfloat* d_c, int width)\n{\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if ((row &lt; width) &amp;&amp; (col &lt; width)) \n    {\n      float temp = 0;\n      // each thread computes one \n      // element of the block sub-matrix\n      for (int i = 0; i &lt; width; ++i) \n        {\n          temp += d_a[row*width+i]*d_b[i*width+col];\n        }\n      d_c[row*width+col] = temp;\n    }\n}\n</code></pre> </li> <li> <p>Copy back computed value from GPU to CPU;    transfer the data back to GPU (from device to host).    Here is the C matrix that contains the product of the two matrices. <pre><code>// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n\n// Deallocate host memory\nfree(a); \nfree(b); \nfree(c);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial-versionCUDA-templateCUDA-version <pre><code>//-*-C++-*-\n// Matrix-multiplication.cc\n\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\nusing namespace std;\n\nfloat * matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float temp = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              temp += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = temp;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix )size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c;       \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  b   = (float*)malloc(sizeof(float) * (N*N));\n  c   = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  matrix_mul(a, b, c, N);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      for(int j = 0; j &lt; N; j++)\n         {\n          cout &lt;&lt; c[j] &lt;&lt;\" \";\n         }\n      cout &lt;&lt; \" \" &lt;&lt;endl;\n   }\n\n  // Deallocate host memory\n free(a); \n free(b); \n free(c);\n\n return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Matrix-multiplication-template.cu\n\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\nusing namespace std;\n\n__global__ void matrix_mul(float* d_a, float* d_b, \nfloat* d_c, int width)\n{\n\n  // create a 2d threads block\n  int row = ..................\n  int col = ....................\n\n  // only allow the threads that are needed for the computation \n  if (................................)\n    {\n      float temp = 0;\n      // each thread computes one \n      // element of the block sub-matrix\n      for (int i = 0; i &lt; width; ++i) \n        {\n          temp += d_a[row*width+i]*d_b[i*width+col];\n        }\n      d_c[row*width+col] = temp;\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;       \n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_c; \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  ...\n  ...\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n  ...\n  ...\n\n  // Transfer data from a host to device memory\n  cudaMemcpy(.........................);\n  cudaMemcpy(.........................);\n\n  // Thread organization\n  int blockSize = ..............;\n  dim3 dimBlock(......................);\n  dim3 dimGrid(.......................);\n\n  // Device function call \n  matrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n  // CPU computation for verification \n  cpu_matrix_mul(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree...\n\n  // Deallocate host memory\n  free...\n\n  return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// Matrix-multiplication.cu\n\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\nusing namespace std;\n\n__global__ void matrix_mul(float* d_a, float* d_b, \nfloat* d_c, int width)\n{\n\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if ((row &lt; width) &amp;&amp; (col &lt; width)) \n    {\n      float temp = 0;\n      // Each thread computes one \n      // Element of the block sub-matrix\n      for (int i = 0; i &lt; width; ++i) \n        {\n          temp += d_a[row*width+i]*d_b[i*width+col];\n        }\n      d_c[row*width+col] = temp;\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;       \n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_c; \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  b   = (float*)malloc(sizeof(float) * (N*N));\n  c   = (float*)malloc(sizeof(float) * (N*N));\n  host_check = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n\n  // Transfer data from host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n\n  // Thread organization\n  int blockSize = 32;\n  dim3 dimBlock(blockSize,blockSize,1);\n  dim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n\n  // Device function call \n  matrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n  // CPU computation for verification \n  cpu_matrix_mul(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n  return 0;\n}\n</code></pre> Compilation and Output Serial-versionCUDA-version <pre><code>// compilation\n$ g++ Matrix-multiplication.cc -o Matrix-Multiplication-CPU\n\n// execution \n$ ./Matrix-Multiplication-CPU\n\n// output\n$ g++ Matrix-multiplication.cc -o Matrix-multiplication\n$ ./Matrix-multiplication\nThe programme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n16 16 16 16 \n16 16 16 16  \n16 16 16 16  \n16 16 16 16 \n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Matrix-multiplication.cu -o Matrix-Multiplication-GPU\n\n// execution\n$ ./Matrix-Multiplication-GPU\nThe programme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number\n$ 256\n\n// output\n$ Two matrices are equal\n</code></pre> Questions <ul> <li>Right now, we are using the 1D array to represent the matrix. However, you can also do it with the 2D matrix. Can you try 2D array matrix multiplication with a 2D thread block?</li> <li>Can you get the correct solution if you remove the <code>if ((row &lt; width) &amp;&amp; (col &lt; width))</code> condition from the <code>__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)</code> function?</li> <li>Please try using different thread blocks and different matrix sizes. <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></li> </ul>"},{"location":"cuda/exercise-4/","title":"Shared Memory","text":"<p>In this tutorial, we will explore the concept of shared memory matrix multiplication. This technique involves partitioning the global matrix into smaller tiled matrices that are designed to fit into the shared memory of Nvidia GPUs. Utilizing shared memory is advantageous, as it offers significantly higher bandwidth compared to access to global memory, thereby enhancing performance in computational tasks.</p> <p></p> <ul> <li>This is very similar to the previous example; however, we just need to allocate the small block matrix into shared memory.  The example below shows the blocking size for <code>a</code> and <code>b</code> matrices, respectively, for global <code>A</code> and <code>B</code> matrices.  <pre><code>  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n</code></pre></li> </ul> <p></p> <ul> <li>Then, we need to iterate elements within the block size and, finally, with the global index.  These can be achieved with CUDA threads. </li> </ul> <p></p> <ul> <li> <p>You can also increase the shared memory or L1 cache size by using <code>cudaFuncSetCacheConfig</code>. For more information about  CUDA API, please refer to cudaFuncSetCacheConfig.</p> Tips <pre><code>cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1);\n//cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared);\n\ncudaFuncCachePreferNone: no preference for shared memory or L1 (default)\ncudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache\ncudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory\ncudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory\n\n// simple example usage increasing more shared memory \n#include&lt;stdio.h&gt;\nint main()\n{\n  // example of increasing the shared memory \n  cudaDeviceSetCacheConfig(My_Kernel, cudaFuncCachePreferShared);\n  My_Kernel&lt;&lt;&lt;&gt;&gt;&gt;();\n  cudaDeviceSynchronize(); \n  return 0;\n}\n</code></pre> </li> <li> <p>Different Nvidia GPUs provide different configurations; for example, Ampere GA102 GPU Architecture will support the following configuration: <pre><code>128 KB L1 + 0 KB Shared Memory\n120 KB L1 + 8 KB Shared Memory\n112 KB L1 + 16 KB Shared Memory\n96 KB L1 + 32 KB Shared Memory\n64 KB L1 + 64 KB Shared Memory\n28 KB L1 + 100 KB Shared Memory\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-4/#questions-and-solutions","title":"Questions and Solutions","text":"Example: Shared Memory - Matrix Multiplication Matrix-multiplication-shared-templateMatrix-multiplication-shared.cu <pre><code>// Matrix-multiplication-shared-template.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\n// block size for the matrix \n#define BLOCK_SIZE 16\n\nusing namespace std;\n\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, \n         float *d_c, int width)\n {\n  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  ...\n\n  // Indexing for the block matrix\n  int tx = threadIdx.x;\n  ...\n\n  // Indexing global matrix to block matrix \n  int row = threadIdx.x+blockDim.x*blockIdx.x;\n  ...\n\n  // Allow threads only for the size of rows and columns (we assume square matrix)\n  if ((row &lt; width) &amp;&amp; (col&lt; width))\n    {\n      // Save temporary value for the particular index\n      float temp = 0;\n      for(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n        {\n          // Allign the global matrix to the block matrix \n          a_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\n          b_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n\n          // Make sure all the threads are synchronized\n          ....\n\n          // Multiply the block matrix \n          for(int j = 0; j &lt; BLOCK_SIZE; ++j)\n            {\n              temp += a_block[ty][j] * b_block[j][tx];    \n            }\n          // Make sure all the threads are synchronized\n          ...\n       }\n      // Save block matrix entry to the global matrix \n      ...\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float temp = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              temp += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = temp;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\n\nint main()\n{  \n cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\n cout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n int N=0;\n cin &gt;&gt; N;\n\n // Initialize the memory on the host\n float *a, *b, *c, *host_check;       \n\n // Initialize the memory on the device\n float *d_a, *d_b, *d_c; \n\n // Allocate host memory\n a   = (float*)malloc(sizeof(float) * (N*N));\n b   = (float*)malloc(sizeof(float) * (N*N));\n c   = (float*)malloc(sizeof(float) * (N*N));\n host_check = (float*)malloc(sizeof(float) * (N*N));\n\n // Initialize host arrays\n for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n // Allocate device memory\n cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n cudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\n cudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n cudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n\n // Thread organization\n dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                \n ...\n\n // Device function call \n matrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n // Transfer data back to host memory\n cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n // Cpu computation for verification \n cpu_matrix_mul(a,b,host_check,N);\n\n // Verification\n bool flag=1;\n for(int i = 0; i &lt; N; i++)\n   {\n    for(int j = 0; j &lt; N; j++)\n      {\n       if(c[j*N+i]!= host_check[j*N+i])\n         {\n          flag=0;\n          break;\n         }\n      }\n   }\n  if (flag==0)\n    {\n    cout &lt;&lt;\"But, two matrices are not equal\" &lt;&lt; endl;\n    cout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n    }\n    else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n return 0;\n}\n</code></pre> <pre><code>// Matrix-multiplication-shared.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\n// block size for the matrix \n#define BLOCK_SIZE 16\n\nusing namespace std;\n\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, \nfloat *d_c, int width)\n{\n  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n\n  // Indexing for the block matrix\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n\n  // Indexing global matrix to block matrix \n  int row = threadIdx.x+blockDim.x*blockIdx.x;\n  int col = threadIdx.y+blockDim.y*blockIdx.y;\n\n  // Allow threads only for the size of rows and columns (we assume square matrix)\n  if ((row &lt; width) &amp;&amp; (col&lt; width))\n    {\n      // Save temporary value for the particular index\n      float temp = 0;\n      for(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n         {\n          // Allign the global matrix to the block matrix \n          a_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\n          b_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n\n          // Make sure all the threads are synchronized\n          __syncthreads(); \n\n          // Multiply the block matrix\n          for(int j = 0; j &lt; BLOCK_SIZE; ++j)\n            {\n              temp += a_block[ty][j] * b_block[j][tx];    \n            }\n            __syncthreads();\n         }\n      // Save block matrix entry to the global matrix \n      d_c[row*width+col] = temp;\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\n\nint main()\n {  \n   cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\n   cout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n   cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n   int N=0;\n   cin &gt;&gt; N;\n\n   // Initialize the memory on the host\n   float *a, *b, *c, *host_check;       \n\n   // Initialize the memory on the device\n   float *d_a, *d_b, *d_c; \n\n   // Allocate host memory\n   a   = (float*)malloc(sizeof(float) * (N*N));\n   b   = (float*)malloc(sizeof(float) * (N*N));\n   c   = (float*)malloc(sizeof(float) * (N*N));\n   host_check = (float*)malloc(sizeof(float) * (N*N));\n\n   // Initialize host arrays\n   for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n\n  // Transfer data from host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n\n  // Thread organization\n  dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                \n  dim3 Grid_dim(ceil(N/BLOCK_SIZE), ceil(N/BLOCK_SIZE), 1);\n\n  // Device function call \n  matrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n  // CPU computation for verification \n  cpu_matrix_mul(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"But, two matrices are not equal\" &lt;&lt; endl;\n      cout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n return 0;\n}\n</code></pre> Compilation and Output CUDA-version <pre><code>// compilation\n$ nvcc -arch=sm_70 Matrix-multiplication-shared.cu -o Matrix-multiplication-shared\n\n// execution\n$ ./Matrix-multiplication-shared\nThe programme assumes that the matrix size is N*N \nMatrix dimensions are assumed to be multiples of BLOCK_SIZE=16\nPlease enter the N size number\n$ 256\n\n// output\n$ Two matrices are equal\n</code></pre> Questions <ul> <li>Could you resize the <code>BLOCK_SIZE</code> number and check the solution's correctness?</li> <li>Can you also create a different kind of thread block and matrix size and check the solution's correctness?</li> <li>Please try using <code>cudaFuncSetCacheConfig</code> and check if you can successfully execute the application.</li> </ul> <pre><code>cudaFuncCachePreferNone: no preference for shared memory or L1 (default)\ncudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache\ncudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory\ncudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory\n</code></pre>"},{"location":"cuda/exercise-5/","title":"Unified Memory","text":"<p>Unified memory streamlines the data transfer process between the host (CPU) and the device (GPU), minimizing the need for explicit data movement by programmers. The CUDA API facilitates this data management, effectively handling the transfer of data between the CPU and GPU. In this tutorial, we will explore the concept of unified memory through a practical example of vector addition performed on the GPU.</p> <p></p> <ul> <li>Just one memory allocation is enough <code>cudaMallocManaged()</code>.  The table below summarises the required steps needed for the unified memory concept.</li> </ul> Without unified memory With unified memory Allocate the host memory Allocate the host memory Allocate the device memory Allocate the device memory Initialize the host value Initialize the host value Transfer the host value to the device memory location Transfer the host value to the device memory location Do the computation using the CUDA kernel Do the computation using the CUDA kernel Transfer the data from the device to host Transfer the data from the device to host Free device memory Free device memory Free host memory Free host memory"},{"location":"cuda/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Unified Memory - Vector Addition Without Unified MemoryWith Unified Memory - templateWith Unified Memory-version <pre><code>//-*-C++-*-\n// Without-unified-memory.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n        float *out, int n) \n{\n\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n   threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out; \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_out, sizeof(float) * N); \n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Transfer data from a host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n\n  // Thread organization \n  dim3 dimGrid(ceil(N/32), ceil(N/32), 1);\n  dim3 dimBlock(32, 32, 1);\n\n  // Execute the CUDA kernel function \n  vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n     {\n       assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n     }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_out);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(out);\n\n  return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n                           float *out, int n) \n{\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  /*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device(unified) memory\n  cudaMallocManaged......\n\n // Initialize host arrays\n for(int i = 0; i &lt; N; i++)\n   {\n     d_a[i] = ...\n     d_b[i] = ...\n   }\n\n /*\n // Transfer data from a host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n\n // Thread organization \n dim3 dimGrid...    \n dim3 dimBlock...\n\n // execute the CUDA kernel function \n vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n\n // synchronize if needed\n ......\n\n /*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n\n // Verification\n for(int i = 0; i &lt; N; i++)\n   {\n     assert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n   }\n\n printf(\"out[0] = %f\\n\", d_out[0]);\n printf(\"PASSED\\n\");\n\n // Deallocate device(unified) memory\n cudaFree...\n\n\n /*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\n\n return 0;\n}\n</code></pre> <pre><code>//-*-C++-*-\n// With-unified-memory.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n                           float *out, int n) \n{\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  /*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMallocManaged(&amp;d_a, sizeof(float) * N);\n  cudaMallocManaged(&amp;d_b, sizeof(float) * N);\n  cudaMallocManaged(&amp;d_out, sizeof(float) * N); \n\n // Initialize host arrays\n for(int i = 0; i &lt; N; i++)\n   {\n     d_a[i] = 1.0f;\n     d_b[i] = 2.0f;\n   }\n\n /*\n // Transfer data from a host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n\n // Thread organization\n dim3 dimGrid(ceil(N/32), ceil(N/32), 1);\n dim3 dimBlock(32, 32, 1);\n\n // Execute the CUDA kernel function \n vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n cudaDeviceSynchronize();\n /*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n\n // Verification\n for(int i = 0; i &lt; N; i++)\n   {\n     assert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n   }\n\n printf(\"out[0] = %f\\n\", d_out[0]);\n printf(\"PASSED\\n\");\n\n // Deallocate device memory\n cudaFree(d_a);\n cudaFree(d_b);\n cudaFree(d_out);\n\n /*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\n\n return 0;\n}\n</code></pre> Compilation and Output Without-unified-memory.cuWith-unified-memory <pre><code>// compilation\n$ nvcc -arch=compute_70 Without-unified-memory.cu -o Without-Unified-Memory\n\n// execution \n$ ./Without-Unified-Memory\n\n// output\n$ ./Without-Unified-Memory\nout[0] = 3.000000\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 With-unified-memory.cu -o With-Unified-Memory\n\n// execution\n$ ./With-Unified-Memory\n\n// output\n$ ./With-Unified-Memory \nout[0] = 3.000000\nPASSED\n</code></pre> Questions <ul> <li>Here in this example, we have used <code>cudaDeviceSynchronize()</code>; can you remove <code>cudaDeviceSynchronize()</code>   and still get a correct solution? If not, why (think)?</li> <li>Please try using different thread blocks and array sizes. </li> </ul>"},{"location":"cuda/preparation/","title":"Preparation","text":""},{"location":"cuda/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"cuda/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example, the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200301\n[u100490@login02 p200301]$ pwd\n/project/home/p200301\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200301]$ mkdir $USER\n### or \n[u100490@login02 p200301]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200301]$cd u100490\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200301/CUDA .\n[u100490@login03 u100490]$ cd CUDA/\n[u100490@login03 CUDA]$ pwd\n/project/home/p200301/u100490/CUDA\n[u100490@login03 CUDA]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200301   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200301 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200301 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#7-until-now-you-are-in-the-login-node-now-it-is-time-to-do-the-dry-run-test","title":"7. Until now, you are in the login node; now it is time to do the dry run test","text":"<ul> <li> <p>7.1 Reserve the interactive node for running/testing CUDA applications    <pre><code>$ salloc -A p200301 --res gpu-ncc-luxembourg-morning --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></p> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200301 --res gpu-ncc-luxembourg-morning --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> <li> <p>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490   p200301  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></p> </li> </ul>"},{"location":"cuda/preparation/#8-now-we-need-to-check-a-simple-cuda-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check a simple CUDA application if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 CUDA]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-cuda-codes","title":"9. Finally, we need to load the compiler to test the GPU CUDA codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code  <pre><code>$ module load env/staging/2023.1\n$ module load OpenMPI/4.1.5-NVHPC-23.7-CUDA-11.7.0\n$ export NVCC_APPEND_FLAGS='-allow-unsupported-compiler'\n### or\n$ source module.sh\n</code></pre></p> check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module load env/staging/2023.1\n[u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n[u100490@mel2131 ~]$ export NVCC_APPEND_FLAGS='-allow-unsupported-compiler'\n[u100490@mel2131 ~]$ module list\n\nCurrently Loaded Modules:\n1) env/release/2022.1           (S)   6) numactl/2.0.14-GCCcore-11.3.0  11) libpciaccess/0.16-GCCcore-11.3.0  16) GDRCopy/2.3-GCCcore-11.3.0                  21) knem/1.1.4.90-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1      (S)   7) CUDA/11.7.0                    12) hwloc/2.7.1-GCCcore-11.3.0        17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0  22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n3) GCCcore/11.3.0                     8) NVHPC/22.7-CUDA-11.7.0         13) OpenSSL/1.1                       18) libfabric/1.15.1-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0         9) XZ/5.2.5-GCCcore-11.3.0        14) libevent/2.1.12-GCCcore-11.3.0    19) PMIx/4.2.2-GCCcore-11.    3.0\n5) binutils/2.38-GCCcore-11.3.0      10) libxml2/2.9.13-GCCcore-11.3.0  15) UCX/1.13.1-GCCcore-11.3.0         20) xpmem/2.6.5-36-GCCcore-11.3.0\n\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre> </li> </ul>"},{"location":"cuda/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>10.1 For example, Dry-run-test  <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly, for the hands-on session, we need to do the node reservation:","text":"<ul> <li> <p>11.1 For example, reservation  <pre><code>$ salloc -A p200301 --res gpu-ncc-luxembourg-afternoon --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre></p> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200301 --res gpu-ncc-luxembourg-afternoon --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> </ul>"},{"location":"cuda/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands-on exercise","text":"<ul> <li>12.1 For example, in the <code>Hello World</code> example, we do the following steps:  <pre><code>[u100490@mel2063 CUDA]$ pwd\n/project/home/p200301/u100490/CUDA\n[u100490@mel2063 CUDA]$ ls\n[u100490@mel2063 CUDA]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Shared-memory  Vector-addition\n[u100490@mel2063 CUDA]$ source module.sh\n[u100490@mel2063 CUDA]$ cd Hello-world\n// compilation\n[u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU \n\n// execution\n[u100490@mel2063 CUDA]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 CUDA]$ Hello World from GPU\n</code></pre></li> </ul>"},{"location":"cuda/profiling/","title":"Profiling and Performance","text":""},{"location":"cuda/profiling/#time-measurement","title":"Time measurement","text":"<p>In CUDA, the execution time can be measured by using the CUDA events. CUDA API events shall be created using <code>cudaEvent_t</code>, for example, <code>cudaEvent_t start, stop;</code>. Moreover, it can be initiated by <code>cudaEventCreate(&amp;start)</code> for a start, and similarly, for stop, it can be created as <code>cudaEventCreate(&amp;stop)</code>. </p> CUDA API <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start,0);\n</code></pre> <p>And it can be initialised to measure the timing as <code>cudaEventRecord(start,0)</code> and <code>cudaEventRecord(stop,0)</code>. Then the timings can be measured as floats, for example, <code>cudaEventElapsedTime(&amp;time, start, stop)</code>. Finally, all events should be destroyed using <code>cudaEventDestroy</code>, such as <code>cudaEventDestroy(start)</code> and <code>cudaEventDestroy(start)</code>.</p> CUDA API <pre><code>cudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n</code></pre> <p>The following example shows how to measure your GPU kernel call in a CUDA application:</p> Example <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start);\n\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n//use CUDA API to stop the measuring time\ncudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n\ncout &lt;&lt; \" time taken for the GPU kernel\" &lt;&lt; time &lt;&lt; endl;\n</code></pre>"},{"location":"cuda/profiling/#nvidia-system-wide-performance-analysis","title":"Nvidia system-wide performance analysis","text":"<p>Nvidia profiling tools help to analyse the code when it is being spent on the given architecture. Whether it is communication or computation, we can get helpful information through traces and events. This will help the programmer optimise the code performance on the given architecture. For this, Nvidia offers three kinds of profiling options, they are:</p> <ul> <li> <p>Nsight Compute: CUDA application interactive kernel profiler: This will give traces and events of the kernel calls; this further provides both visual profile-GUI and Command Line Interface (CLI) profiling options. <code>ncu -o profile Application.exe</code> command will create an output file <code>profile.ncu-rep</code>, which can be opened using <code>ncu-ui</code>. </p> Example <pre><code>$ ncu ./a.out\nmatrix_mul(float *, float *, float *, int), 2023-Mar-12 20:20:45, Context 1, Stream 7\nSection: GPU Speed Of Light Throughput\n---------------------------------------------------------------------- --------------- ------------------------------\nDRAM Frequency                                                           cycle/usecond                         874.24\nSM Frequency                                                             cycle/nsecond                           1.31\nElapsed Cycles                                                                   cycle                         241109\nMemory [%]                                                                           %                          13.68\nDRAM Throughput                                                                      %                           0.07\nDuration                                                                       usecond                         184.35\nL1/TEX Cache Throughput                                                              %                          82.39\nL2 Cache Throughput                                                                  %                          13.68\nSM Active Cycles                                                                 cycle                       30531.99\nCompute (SM) [%]                                                                     %                           1.84\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n     waves across all SMs. Look at Launch Statistics for more details.                                             \n\nSection: Launch Statistics\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Size                                                                                                       1024\nFunction Cache Configuration                                                                  cudaFuncCachePreferNone\nGrid Size                                                                                                          16\nRegisters Per Thread                                                   register/thread                             26\nShared Memory Configuration Size                                                  byte                              0\nDriver Shared Memory Per Block                                              byte/block                              0\nDynamic Shared Memory Per Block                                             byte/block                              0\nStatic Shared Memory Per Block                                              byte/block                              0\nThreads                                                                         thread                          16384\nWaves Per SM                                                                                                     0.10\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80             \n      multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n      concurrently with other workloads, consider reducing the block size to have at least one block per            \n      multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n      Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n      description for more details on launch configurations.                                                        \n\nSection: Occupancy\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Limit SM                                                                   block                             32\nBlock Limit Registers                                                            block                              2\nBlock Limit Shared Mem                                                           block                             32\nBlock Limit Warps                                                                block                              2\nTheoretical Active Warps per SM                                                   warp                             64\nTheoretical Occupancy                                                                %                            100\nAchieved Occupancy                                                                   %                          45.48\nAchieved Active Warps Per SM                                                      warp                          29.11\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n      theoretical (100.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads   \n      or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n      as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n      (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n      optimizing occupancy.                                                                                         \n</code></pre> </li> <li> <p>Nsight Graphics: Graphics application frame debugger and profiler: This is quite useful for analysing the profiling results through GUI. </p> </li> <li> <p>Nsight Systems: System-wide performance analysis tool: This is needed when we try to do heterogeneous computation profiling, such as mixing MPI and OpenMP with CUDA. This will profile the system-wide application, that is, both CPU and GPU. To learn more about the command line options, please use <code>$ nsys profile --help</code></p> Example <pre><code>$ nsys profile -t nvtx,cuda --stats=true ./a.out\nGenerating '/scratch_local/nsys-report-ddd1.qdstrm'\n[1/7] [========================100%] report1.nsys-rep\n[2/7] [========================100%] report1.sqlite\n[3/7] Executing 'nvtxsum' stats report\nSKIPPED: /m100/home/userexternal/ekrishna/Teaching/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n[4/7] Executing 'cudaapisum' stats report\n\nTime (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)  Min (ns)  Max (ns)   StdDev (ns)        Name      \n--------  ---------------  ---------  -----------  --------  --------  ---------  -----------  ----------------\n    99.7        398381310          3  132793770.0    8556.0      6986  398365768  229992096.8  cudaMalloc      \n     0.2           714256          3     238085.3   29993.0     24944     659319     364807.8  cudaFree        \n     0.1           312388          3     104129.3   43405.0     37692     231291     110162.3  cudaMemcpy      \n     0.0            51898          1      51898.0   51898.0     51898      51898          0.0  cudaLaunchKernel\n\n[5/7] Executing 'gpukernsum' stats report\n\n\nTime (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)     GridXYZ         BlockXYZ                        Name                   \n--------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------  --------------  ------------------------------------------\n100.0           181949          1  181949.0  181949.0    181949    181949          0.0     4    4    1    32   32    1  matrix_mul(float *, float *, float *, int)\n\n[6/7] Executing 'gpumemtimesum' stats report\n\nTime (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n--------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n 75.0            11520      2    5760.0    5760.0      5760      5760          0.0  [CUDA memcpy HtoD]\n 25.0             3840      1    3840.0    3840.0      3840      3840          0.0  [CUDA memcpy DtoH]\n\n[7/7] Executing 'gpumemsizesum' stats report\n\nTotal (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n----------  -----  --------  --------  --------  --------  -----------  ------------------\n  0.080      2     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy HtoD]\n  0.040      1     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy DtoH]\n\nGenerated:\n   /m100/home/userexternal/ekrishna/Teaching/report1.nsys-rep\n   /m100/home/userexternal/ekrishna/Teaching/report1.sqlite\n</code></pre> </li> </ul>"},{"location":"cuda/profiling/#occupancy","title":"Occupancy","text":"<p>The CUDA Occupancy Calculator allows you to compute the multiprocessor occupancy of a Nvidia GPU microarchitecture by a given CUDA kernel. The multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU.</p> <p>\\(Occupancy  = \\frac{Active\\ warps\\ per\\ SM}{ Max.\\ warps\\ per\\ SM}\\)</p> Examples Occupancy CUDACompilation and results <pre><code>//-*-C++-*-\n#include&lt;iostream&gt;\n// Device code\n__global__ void MyKernel(int *d, int *a, int *b)\n{\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  d[idx] = a[idx] * b[idx];\n}\n\n// Host code\nint main()\n{\n  // set your numBlocks and blockSize to get 100% occupancy\n  int numBlocks = 32;        // Occupancy in terms of active blocks\n  int blockSize = 128;\n\n  // These variables are used to convert occupancy to warps\n  int device;\n  cudaDeviceProp prop;\n  int activeWarps;\n  int maxWarps;\n\n  cudaGetDevice(&amp;device);\n  cudaGetDeviceProperties(&amp;prop, device);\n\n  cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n  &amp;numBlocks,\n  MyKernel,\n  blockSize,0);\n\n  activeWarps = numBlocks * blockSize / prop.warpSize;\n  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n\n  std::cout &lt;&lt; \"Max # of Blocks : \" &lt;&lt; numBlocks &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"ActiveWarps : \" &lt;&lt; activeWarps &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"MaxWarps : \" &lt;&lt; maxWarps &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Occupancy: \" &lt;&lt; (double)activeWarps / maxWarps * 100 &lt;&lt; \"%\" &lt;&lt; std::endl;\n\n return 0;\n}\n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 occupancy.cu -o Occupancy-GPU\n\n// execution\n$ ./Occupancy-GPU\n\n// output\nMax number of Blocks : 16\nActiveWarps : 64\nMaxWarps : 64\nOccupancy: 100%\n</code></pre> Questions <ul> <li>Occupancy: can you change <code>numBlocks</code> and <code>blockSize</code> in the occupancy.cu code  and check how it affects or predicts the occupancy of the given Nvidia microarchitecture.</li> <li>Profiling: run your <code>Matrix-multiplication.cu</code> and <code>Vector-addition.cu</code> code and observe what you notice.  For example, how can occupancy be improved? Or maximize GPU utilization?</li> <li>Timing: using CUDA events API, can you measure your GPU kernel execution and compare how fast your GPU computation is compared to CPU computation?</li> </ul>"},{"location":"education/introduction/","title":"Introduction","text":"<p>If you are an educator and would like to offer a course with us, please contact us:, please contact us :</p> <p>Please read the following instructions:</p> <p>The following are the website and Git repository for hosting the training events under EuroCC (Luxembourg).</p> <p>https://ncclux.github.io/NCC-Trainings https://github.com/NCCLUX/NCC-Trainings</p> <p>Please create your own repository and store your training materials there. We will merge with the main branch (or publish your training materials) close to the event or earlier, whenever you are ready.</p> <p>We plan to allocate 2 hours for theory and 2 hours for practical (hands-on) exercises. Typically, the entire event lasts 5 hours; this extra 1 hour is dedicated to a dry-run to ensure registered participants can access the machine (in this case, MeluXina) and test the training materials (hands-on exercises) in advance. For example, <pre><code>11:00-12:00 (dry-run),\n12:00-13:00 (lunch break),\n13:00-15:00 (lecture) and\n15:00-17:00 (hands-on exercise).\n</code></pre> It would be helpful to categorize the hands-on exercises, for example, from basic to intermediate (or advanced) topics. Previous course examples are available here: https://ncclux.github.io/NCC-Trainings/cuda/exercise-1/</p> <p>For example, the Quantum Espresso course falls under Computational Chemistry (if you prefer another name, please let us know). Before the event, for promotional purposes, we need to publish the course introduction and learning outcomes. You might draw inspiration from this existing course: https://ncclux.github.io/NCC-Trainings/openmp/</p> <p>Could you please provide a suitable date as soon as possible (as we need to reserve the HPC machine for the event)? We also need to publish the event info (Luxinnovation handles the landing page) at least 3-4 weeks before the event so that enough participants can register.</p> <p>For the training event, we usually accept 20-30 participants. However, as you might know, in events like this, some participants may not attend at the last minute. Therefore, we usually accept around 50 registrants. Participants typically come from Luxembourg (academic and industry sectors) and other EU countries. However, preference will be given to Luxembourg participants. Importantly, these events are primarily online, offering participants flexibility and avoiding local logistical burdens.</p> <p>During the event, you will be using the MeluXina HPC machine. Once you confirm the date for the event, we can request your account on MeluXina for training material preparation. Regarding participants, access to MeluXina will be granted before the event.</p> <p>Furthermore the following steps would summerize the steps for instructors.</p> <ol> <li> <p>Course Proposal:</p> <ul> <li>Once you have gathered sufficient materials and formulated a planned timeline, submit the course title to Leane BIONDINO.</li> </ul> </li> <li> <p>Announcement:</p> <ul> <li>After receiving approval, the course will be announced at Supercomputing Luxembourg through LuxInnovation.</li> </ul> </li> <li> <p>Event Preparation:</p> <ul> <li>Prepare the training event, which should typically last half a day. Use previous training event formats as inspiration.</li> </ul> </li> <li> <p>Resource Management:</p> <ul> <li>During preparation, ensure that all materials are uploaded to the Git repository. The layout should conform to existing course formats.</li> </ul> </li> <li> <p>Participant Registration:</p> <ul> <li>Confirm access to Inwink or check with LuxInnovation to obtain a list of registered participants.</li> </ul> </li> <li> <p>Technical Support:</p> <ul> <li>Use the LXP service ticket system to request compute node resources, specifying the expected number of participants. Make these requests at least two weeks before the event starts.</li> </ul> </li> </ol> <p>Make sure to follow these guidelines to ensure a successful training event.</p>"},{"location":"education/requirements/","title":"Requirements","text":"<p>In order to install and check you need to have the following dependencies (and it is tested with Python 3.10.12):</p> <pre><code>mkdocs                                    == 1.4.2\nmkdocs-git-revision-date-localized-plugin == 1.2.0\nmkdocs-glightbox                          == 0.3.1\nmkdocs-material                           == 9.1.0\nmkdocs-material-extensions                == 1.1.1\nmkdocs-minify-plugin                      == 0.6.2\n</code></pre> <p>Bfore requesting the merge request, pelase make sure,  you do not have any issues (website layout) with .md files.</p>"},{"location":"environment/","title":"An introduction to Conda and environment management","text":"<p>Copyright \u00a9 2023-2024 UL HPC Team hpc-sysadmins@uni.lu Author: Georgios Kafanas</p> <p>Users of a programming language like Python, or any other software tool like R, quite often have to install various packages and tools to perform various operations. The various Linux/GNU distributions offer many packages, however, many package versions are either old or missing completely as administrators try to reduce the size of distributions and the associated maintenance.</p> <p>Software distribution systems that specialize on various categories of application software have been developed. Systems such as Conda distribute generic types of software, where as systems such as PyPI (Python), Packrat (R), and Pkg (Julia) specialize in a single kind of software. All software distribution system however offer a uniform functionality that includes</p> <ul> <li>the ability to create and reproduce software environments,</li> <li>isolation between environments, and between an environment and the system, and</li> <li>easy sourcing of packages from a variety of package sources.</li> </ul> <p>The objective of this course is to cover the basics of package management with Conda. In particular, after this course the users will be able to</p> <ul> <li>create Conda environment and use them to manage the software and package dependencies,</li> <li>document, and reproduce any Conda environment in variety of systems,</li> <li>install packages from multiple sources, and</li> <li>decide when Conda is the most appropriate tool to manage a system environment.</li> </ul>"},{"location":"environment/#pre-requisites","title":"Pre-requisites","text":"<p>This course focuses on generic aspects of package management. It is assumed that you have some basic knowledge of how to use packages in R or Python. The main package management framework used is Conda, although there will be mentions to some native tools. You can use the techniques covered here both in your personal machine and on HPC clusters.</p>"},{"location":"environment/#a-brief-introduction-to-conda","title":"A brief introduction to Conda","text":"<p>You must be familiar with a few concepts to start working with Conda. In brief, these concepts are package managers which are the programs used to create and manage environments, channels which are the repositories that contain the packages from which environments are composed, and distributions which are systems for shipping package managers.</p>"},{"location":"environment/#package-managers","title":"Package managers","text":"<p>Package managers are the programs that install and manage the Conda environments. There are multiple package managers, such as <code>conda</code>, <code>mamba</code>, and <code>micromamba</code>.</p>"},{"location":"environment/#channels","title":"Channels","text":"<p>Conda channels are the locations where packages are stored and from where they can be downloaded and installed. There are also multiple Conda channels, with some important channels being:</p> <ul> <li><code>defaults</code>, the default channel,</li> <li><code>anaconda</code>, a mirror of the default channel,</li> <li><code>bioconda</code>, a distribution of bioinformatics software, and</li> <li><code>conda-forge</code>, a community-led collection of recipes, build infrastructure, and distributions for the conda package manager.</li> </ul> <p>The most useful channel that comes pre-installed in all distributions, is Conda-Forge. Channels are usually hosted in the official Anaconda page, but in some rare occasions custom channels may be used. For instance the default channel is hosted independently from the official Anaconda page. Many channels also maintain web pages with documentation both for their usage and for packages they distribute:</p> <ul> <li>Default Conda channel</li> <li>Bioconda</li> <li>Conda-Forge</li> </ul>"},{"location":"environment/#distributions","title":"Distributions","text":"<p>Quite often, the package manager is not distributed on its own, but with a set of packages that are required for the package manager to work, or even with some additional packages that required for most applications. For instance, the <code>conda</code> package manager is distributed with the Miniconda and Anaconda distributions. Miniconda contains the bare minimum packages for the <code>conda</code> package manager to work, and Anaconda contains multiple commonly used packages and a graphical user interface. The relation between these distributions and the package manager is depicted in the following diagram.</p> <p></p> <p>The situation is similar for Mamba distributions. Mamba distributions are supported by Conda-Forge, and their default installation options set-up <code>conda-forge</code> as the default and only channel during installation. The <code>defaults</code> or its mirror <code>anaconda</code> must be explicitly added if required. The distribution using the Mamba package manager was originally distributed as Mambaforge and was recently renamed to Miniforge. Miniforge comes with a minimal set of python packages required by the Mamba package manager. The distribution using the Micromamba package manager ships no accompanying packages, as Micromamba is a standalone executable with no dependencies. Micromamba is using <code>libmamba</code>, a C++ library implementing the Conda API.</p>"},{"location":"environment/#the-micromamba-package-manager","title":"The Micromamba package manager","text":"<p>The Micromaba package manager is a minimal yet fairly complete implementation of the Conda interface in C++, which is shipped as a standalone executable. The package manager operates strictly on the user-space and thus it requires no special permissions to install packages. It maintains all its files in a couple of places, so uninstalling the package manager itself is also easy. Finally, the package manager is also lightweight and fast.</p>"},{"location":"environment/#installation","title":"Installation","text":"<p>A complete guide regarding Micromamba installation can be found in the official documentation. To install micromamaba in the HPC clusters, log in to Aion or Iris. Working on a login node, run the installation script, <pre><code>\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\n</code></pre> which will install the executable and setup the environment. There are 4 options to select during the installation of Micromamba:</p> <ul> <li>The directory for the installation of the binary file:   <pre><code>Micromamba binary folder? [~/.local/bin]\n</code></pre>   Leave empty and press enter to select the default displayed within brackets. Your <code>.bashrc</code> script should include <code>~/.local/bin</code> in the <code>$PATH</code> by default.</li> <li>The option to add to the environment autocomplete options for <code>micromamba</code>:   <pre><code>Init shell (bash)? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will append a clearly marked section in the <code>.bashrc</code> shell. Do not forget to remove this section when uninstalling Micromamba.</li> <li>The option to configure the channels by adding conda-forge:   <pre><code>Configure conda-forge? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will setup the <code>~/.condarc</code> file with <code>conda-forge</code> as the default channel. Note that Mamba and Micromamba will not use the <code>defaults</code> channel if it is not present in <code>~/.condarc</code> like <code>conda</code>.</li> <li>The option to select the directory where environment information and packages will be stored:   <pre><code>Prefix location? [~/micromamba]\n</code></pre>   Press enter to select the default option displayed within brackets.</li> </ul> <p>To activate the new environment log-out and log-in again. You now can use <code>micromamba</code> in the login and compute nodes, including the auto-completion feature.</p>"},{"location":"environment/#managing-environments","title":"Managing environments","text":"<p>The user interface of Conda is based around the concept of the environment. Environments are effectively system configurations that can be activated and deactivated by the Conda manager, and provide access to a set of packages that are installed in the environment. In the following examples we use the Micromamba package manager, but any other Conda package manager operates with the same commands, as package managers for Conda have the same interface.</p> <p>Environments are created with the command <pre><code>$ micromamba create --name &lt;environment name&gt;\n</code></pre> The environment is then activated with the command <pre><code>$ micromamba activate &lt;environment name&gt;\n</code></pre> anywhere in the file system. To install packages, first ensure that the target environment is active, and then install any required package with the command: <pre><code>$ micromamba install &lt;package name&gt;\n</code></pre> You can specify multiple packages at ones. Quite often, the channels where Conda should look for the package must also be specified. Using the syntax <pre><code>$ micromamba install --chanell &lt;chanell 1&gt; --channels &lt;chanell 2&gt; &lt;package name&gt;\n</code></pre> channels are listed in a series of <code>--channel &lt;channel name&gt;</code> entries and the channels are searched in the order they appear. Using the syntax <pre><code>$ micromamba install &lt;chanell&gt;::&lt;package name&gt;\n</code></pre> packages are searched in the specified channel only. Available packages can be found by searching the Anaconda search index or channel specific search indices, such as conda-forge.</p> <p>Specifying package channels</p> <p>The Anaconda index provides instructions for the installation of each package. Quite often the channel is specified in the installation instructions, with options such as <code>conda-forge::&lt;package name&gt;</code> or even <code>-c conda-forge</code> or <code>--channel conda-forge</code>. While the Micromamba installer sets-up <code>conda-forge</code> as the default channel, latter modification in <code>~/.condarc</code> may change the channel priority. It is thus a good practice to explicitly specify the source channel when installing a package.</p> <p>After work in an environment is complete, deactivate the environment, <pre><code>$ micromamba deactivate\n</code></pre> to ensure that it does not interfere with any other operations. In contrast to modules, Conda is designed to operate with a single environment active at a time. Create one environment for each project, and Conda will ensure that any package that is shared between multiple environments is installed once.</p> <p>Micromamba supports almost all the subcommands of Conda. For more details see the official documentation.</p> Creating an environment for R <p>Assume for instance that we want to install R. Create an environment call <code>R-project</code> and activate it: <pre><code>micromamba create --name R-project\nmicromamba activate R-project\n</code></pre> The basic functionality of the R software environment is contained in the <code>r-base</code> package. Calling <pre><code>micromamba install --channel conda-forge r-base\n</code></pre> or <pre><code>micromamba install conda-forge::r-base\n</code></pre> will install all the components required to run standalone R scripts. More involved scripts need functionality defined in various R packages. The R packages are prepended with a prefix 'r-' in the conda-forge channel. Thus, <code>plm</code> becomes <code>r-plm</code> and so on. After all the required packages have been installed, the environment is ready for use.</p>"},{"location":"environment/#using-environments-in-submission-scripts","title":"Using environments in submission scripts","text":"<p>In HPC clusters, all computationally heavy operations must be performed in compute nodes. Thus Conda environments are also used in jobs submitted to the queuing system. You can activate and deactivate environments in various sections of your script.</p> <p>Environment activations in Conda are stacked, and unlike modules, only one environment is active at a time with the rest being pushed down the stack. Assume that we are working with 2 environments, <code>R-project</code> and <code>python-project</code>, and consider the following script layout. <pre><code># Initialization code\n\nmicromabma activate python-project\n\n# Code to run a simulation and generate output with Python\n\nmicromabma activate R-project\n\n# Code to perform statistical analysis and ploting with R\n\nmicromamba deactivate\n\n# Code to save data with Python\n</code></pre> Such a script creates the following environment stack. <pre><code>(base)\n|\n| # No software is available here\n|\n+-(python-project) # micromabma activate python-project\n| |\n| | # Only Python is available here\n| |\n| +-(R-project) # micromabma activate R-project\n| | |\n| | | # Only R is available here\n| | |\n| +-+ # micromamba deactivate\n| |\n| | # Only Python is available here\n| |\n</code></pre></p> <p>We can see that the Python environment (<code>python-project</code>) is pushed down the stack when the R environment (<code>R-project</code>) is activated, and will be brought forth as soon as the R environment is deactivated.</p> Example SLURM submission script <p>Consider for instance a script running a single core job for R. The R script for the job is run inside an environment named <code>R-project</code>. A typical submission script is the following: <pre><code>#SBATCH --job-name R-job\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=0-02:00:00\n#SBATCH --partition batch\n#SBATCH --qos normal\n\nmicromamba activate R-project\n\necho \"Launched at $(date)\"\necho \"Job ID: ${SLURM_JOBID}\"\necho \"Node list: ${SLURM_NODELIST}\"\necho \"Submit dir.: ${SLURM_SUBMIT_DIR}\"\necho \"Numb. of cores: ${SLURM_CPUS_PER_TASK}\"\n\nexport SRUN_CPUS_PER_TASK=\"${SLURM_CPUS_PER_TASK}\"\nexport OMP_NUM_THREADS=1\nsrun Rscript --no-save --no-restore script.R\n\nmicromamba deactivate\n</code></pre></p> <p>The <code>micromamba deactivate</code> command at the end of the script is optional, but it functions as a reminder that a Conda environment is active if you expand the script at a later date.</p> <p>Useful scripting resources</p> <ul> <li>Formatting submission scripts for R (and other systems)</li> </ul>"},{"location":"environment/#exporting-and-importing-environment-specifications","title":"Exporting and importing environment specifications","text":"<p>An important feature of Conda is that it allows you to export and version control you environment specifications, and recreate the environment on demand.</p> <ul> <li>A description of the software installed in the Conda environment can be exported on demand to a text file.</li> <li>The specification file can then be used to populate a new environment, in effect recreating the environment.</li> </ul> <p>The environment reproducibility is particularly important when you want to have reproducible results, like for instance in a scientific simulation. You can setup and test your application in your local machine, save the environment, and later load the environment in an HPC system, and be sure that the application will behave identically. Conda in the background will ensure that identical packages will be installed.</p> <p>In Micromaba, you can export the specifications of an environment using the command: <pre><code>$ micromaba env export --name &lt;environment name&gt;\n</code></pre> By default the command prints to the standard output, but you can redirect the output to a file: <pre><code>$ micromaba env export --name &lt;environment name&gt; &gt; &lt;environment name&gt;.yml\n</code></pre> To recreate an environment from a specification file, pass the file as argument to the create command with the <code>--file</code> flag: <pre><code>$ micromamba create --name &lt;environment name&gt; --file &lt;environment name&gt;.yml\n</code></pre> This workflow demonstrates the use of simple YAML text files to store specifications, but Micormamba supports various specification file types. All specification files are text files and can be version controlled with a tool such as Git.</p> <p>Sources</p> <ul> <li>Micromamba User Guide: Specification files</li> </ul>"},{"location":"environment/#example-installing-jupyter-and-managing-the-dependencies-of-a-notebook-with-micromamba","title":"Example: Installing Jupyter and managing the dependencies of a notebook with Micromamba","text":"<p>In this example we will create an environment, install Jupyter, and install all the dependencies for our notebooks with Micromamba. Start by creating an environment: <pre><code>micromamba env create --name jupyter\n</code></pre> Next, install Jupyter in the environment. Have a look at the page for <code>jupyterlab</code> in the conda-forge channel. To install it in your environment call: <pre><code>micromamba install --name jupyter conda-forge::jupyterlab\n</code></pre> Now activate the environment, create a working directory for your notebooks, and launch Jypyter: <pre><code>micromamba activate jupyter\nmkdir ~/Documents/notebooks &amp;&amp; cd ~/Documents/notebooks\njupyter lab\n</code></pre> If a webpage appears with the Jupyter lab, the installation worked succeeded!</p> <p>You may need some Python package in your Jupyter notebook. You can make packages available in your notebook by installing the appropriate package in the Conda environment. For instance, assume that you need <code>pandas</code> and <code>numpy</code>. Searching the conda-forge channel, we can find the package name and installation instruction. With the <code>jupyter</code> environment active, run the command: <pre><code>micromamba install conda-forge::numpy conda-forge::pandas\n</code></pre> You should now be able to import <code>numpy</code> and <code>pandas</code> in your notebook!</p> <p>After completing your work, close down the notebook with the command <code>C-c</code>, and deactivate the <code>jupyter</code> Conda environment: <pre><code>micromamba deactive\n</code></pre> You should now be in your normal operating system environment.</p>"},{"location":"environment/#self-management-of-work-environments-in-hpc-systems-with-conda","title":"Self management of work environments in HPC systems with Conda","text":"<p>Conda is one of the systems for providing software in HPC systems, along with modules and containers. When starting a new project it is important to select the appropriate system. Before installing any software yourself in user space you should check if the HPC system provides the software though any method. System wide installations do not consume any of your storage quota, and are often configured and tested to provide optimal efficiency.</p>"},{"location":"environment/#when-a-conda-environment-is-useful","title":"When a Conda environment is useful","text":"<p>There are three aspects of environment management that you should consider when selecting the method with which you will manage your software.</p> <ul> <li> <p>Ease of use: Many software systems whose performance is not critical and are used by relatively few users are not provided though the standard distribution channels of modules or containers. In such cases the easiest installation option is a user side installation with Conda or some similar package management system.</p> </li> <li> <p>Reproducibility: Conda and containers can both create reproducible environments, with descriptions of the environment exported and version controlled in text files. However, containers require significant amount of manual configuration to create a reproducible environment and to perform well in a wide range of systems. If your aim is an easily reproducible environment Conda is the superior choice.</p> </li> <li> <p>Performance: Conda provides precompiled executables. Even thought multiple configurations are supported, you will not always find an executable tailored to your target system. Modules and containers provided by UPC systems are optimized to ensure performance and stability, so prefer them.</p> </li> </ul>"},{"location":"environment/#storage-limitations-in-hpc-systems","title":"Storage limitations in HPC systems","text":"<p>Regardless of installation method, when you install software in user space you are using up your storage quota. Conda environment managers download and store a sizable amount of data and a large number of files to provided packages to the various environments. Even though the package data are shared between the various environments, they still consume space in your or your project's account. There are limits in the storage space and number of files that are available to projects and users in a cluster. Since Conda packages are self managed, you need to clean unused data yourself.</p>"},{"location":"environment/#updating-the-package-manager-and-the-environment","title":"Updating the package manager and the environment","text":"<p>The Micromamba package manager is under active development, so updates provide the latest features and any bug fixes. To update the micromamba package manager itself, simply issue the command: <pre><code>micromamba self-update\n</code></pre> If a new version is available this commands will download the new executable and replace the old one.</p> <p>Package versions can be pinned with Conda during installation to provide an immutable environment. However, in many cases, like during development, it make sense to experiment and update package versions. To update a specific package inside an environment, use the command: <pre><code>micromamba update --name &lt;environment name&gt; &lt;package name&gt;\n</code></pre> You can specify more that one packages. To update all the packages in an environment at once, use the command: <pre><code>micromamba update --name &lt;environment name&gt; --all\n</code></pre></p>"},{"location":"environment/#cleaning-up-package-data","title":"Cleaning up package data","text":"<p>There are two main sources of unused data, compressed archives of packages that Conda stores in its cache when downloading a new package, and data of packages no longer used in any environment. All unused data in Micromoamba can be removed with the command <pre><code>micromamba clean --all --yes\n</code></pre> where the flag <code>--yes</code> suppresses an interactive dialogue with details about the operations performed. In general you can use the default options with <code>--yes</code>, unless you have manually edited any files in you package data directory (default location <code>~/micromamba</code>) and you would like to preserve your changes.</p> <p>Updating environments to remove old package versions</p> <p>As we create new environments, environments often install the latest version of each package. However, if the environments are not updated regularly, we may end up with different versions of the same package across multiple environments. If we have the same version of a package installed in all environments, we can save space by removing unused older versions.</p> <p>To update a package across all environments, use the command <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --yes --name $e &lt;package name&gt;; done\n</code></pre> and to update all packages across all environments <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --yes --name $e --all; done\n</code></pre> where <code>FNR&gt;2</code> removes the headers in the output of <code>micromamba env list</code>, and is thus sensitive to changes in the user interface of Micromamba.</p> <p>After updating packages, the <code>clean</code> command can be called to removed the data of unused older package versions.</p> <p>Sources</p> <ul> <li>Oficial Conda <code>clean</code> documentation</li> <li>Understanding Conda <code>clean</code></li> </ul>"},{"location":"environment/#a-note-about-internal-workings-of-conda","title":"A note about internal workings of Conda","text":"<p>In general, Conda packages are stored in a central directory, and hard links are created in the library directories of any environment that requires the package. Since hard links do not consume space and inodes, Conda is very efficient in its usage of storage space.</p> <p>Consider for instance the MPFR package used in some environment <code>gaussian_regression</code>. Looking into the Conda installation managed by Micromamba, these are the installed library files: <pre><code>gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi pkgs/mpfr-4.2.1-h9458935_0/lib/\ntotal 1.3M\n5286432 drwxr-xr-x 1 gkaf gkaf   94 Oct 25 13:59 ./\n5286426 drwxr-xr-x 1 gkaf gkaf   38 Oct 25 13:59 ../\n5286436 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so -&gt; libmpfr.so.6.2.1*\n5286441 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so.6 -&gt; libmpfr.so.6.2.1*\n5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 libmpfr.so.6.2.1*\n5286442 drwxr-xr-x 1 gkaf gkaf   14 Oct 25 13:59 pkgconfig/\n</code></pre> Looking into the libraries of the <code>gaussian_regression</code> environment, there is a hard link to the MPFR library: <pre><code>gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi envs/gaussian_regression/lib/libmpfr.so.6.2.1 \n5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 envs/gaussian_regression/lib/libmpfr.so.6.2.1*\n</code></pre> You can use the <code>-i</code> flag in <code>ls</code> to print the inode number of a file. Hard links have the same inode number, meaning that they are essentially the same file.</p> <p>Conda will not automatically check if the files in the <code>pkgs</code> directories must be removed. For instance, when you uninstall a package from an environment, when you delete an environment, or when a package is updated in an environment, only the hard link in the environment directory will change. The files in <code>pkgs</code> will remain even if they are no longer used in any environment. The relevant <code>clean</code> routines check which packages are actually used and remove the unused files.</p>"},{"location":"environment/best-practises/","title":"Best practises","text":""},{"location":"environment/best-practises/#an-introduction-to-conda-and-environment-management","title":"An introduction to Conda and environment management","text":"<p>Copyright \u00a9 2023 UL HPC Team hpc-sysadmins@uni.lu Author: Georgios Kafanas</p> <p>The objective of this tutorial is to cover the basics of package management with Conda. Conda environments can simultaneously install dependencies from multiple software distributions, such as Python and R. Package management systems native to distributions of Python, R, and Julia will also be covered to demonstrate how Conda can interface with such systems.</p> <p>In this tutorial the users will learn to:</p> <ul> <li>use Conda environments to manage the software and package dependencies of projects,</li> <li>document and exchange Conda environment setups for reproducibility,</li> <li>determine which is the best environment management tool given the requirements of a project, and</li> <li>instal packages using the facilities available in R, Python, and Julia when these packages are not available in Conda.</li> </ul>"},{"location":"environment/best-practises/#pre-requisites","title":"Pre-requisites","text":"<p>This tutorial focuses on generic aspects of package management. It is assumed that you have some basic knowledge of how to use packages in R or Python. The main package management framework used is Conda, although there will be mentions to tools native to R, Python, and Julia. You can use the techniques covered here both in your personal machine and on the UL HPC clusters. If you would like to setup environments in the UL HPC clusters, please ensure that you are able to connect first.</p>"},{"location":"environment/best-practises/#a-brief-introduction-to-conda","title":"A brief introduction to Conda","text":"<p>You must be familiar with a few concepts to start working with Conda. In brief, these concepts are package managers which are the programs used to create and manage environments, channels which are the repositories that contain the packages from which environments are composed, and distributions which are systems for shipping package managers.</p>"},{"location":"environment/best-practises/#package-managers","title":"Package managers","text":"<p>Package managers are the programs that install and manage the Conda environments. There are multiple package managers, such as <code>conda</code>, <code>mamba</code>, and <code>micromamba</code>.</p> <p>The UL HPC centre supports the use of <code>micromamba</code> for the creation and management of personal Conda environments.</p>"},{"location":"environment/best-practises/#channels","title":"Channels","text":"<p>Conda channels are the locations where packages are stored. There are also multiple channels, with some important channels being:</p> <ul> <li><code>defaults</code>, the default channel,</li> <li><code>anaconda</code>, a mirror of the default channel,</li> <li><code>bioconda</code>, a distribution of bioinformatics software, and</li> <li><code>conda-forge</code>, a community-led collection of recipes, build infrastructure, and distributions for the conda package manager.</li> </ul> <p>The most useful channel that comes pre-installed in all distributions, is Conda-Forge. Channels are usually hosted in the official Anaconda page, but in some rare occasions custom channels may be used. For instance the default channel is hosted independently from the official Anaconda page. Many channels also maintain web pages with documentation both for their usage and for packages they distribute:</p> <ul> <li>Default Conda channel</li> <li>Bioconda</li> <li>Conda-Forge</li> </ul>"},{"location":"environment/best-practises/#distributions","title":"Distributions","text":"<p>Quite often, the package manager is not distributed on its own, but with a set of packages that are required for the package manager to work, or even with some additional packages that required for most applications. For instance, the <code>conda</code> package manager is distributed with the Miniconda and Anaconda distributions. Miniconda contains the bare minimum packages for the <code>conda</code> package manager to work, and Anaconda contains multiple commonly used packages and a graphical user interface. The relation between these distributions and the package manager is depicted in the following diagram.</p> <p></p> <p>The situation is similar for Mamba distributions. Mamba distributions are supported by Conda-Forge, and their default installation options set-up <code>conda-forge</code> as the default and only channel during installation. The <code>defaults</code> or its mirror <code>anaconda</code> must be explicitly added if required. The distribution using the Mamba package manager was originally distributed as Mambaforge and was recently renamed to Miniforge. Miniforge comes with a minimal set of python packages required by the Mamba package manager. The distribution using the Micromamba package manager ships no accompanying packages, as Micromamba is a standalone executable with no dependencies. Micromamba is using <code>libmamba</code>, a C++ library implementing the Conda API.</p>"},{"location":"environment/best-practises/#the-micromamba-package-manager","title":"The Micromamba package manager","text":"<p>The Micromaba package manager is a minimal yet fairly complete implementation of the Conda interface in C++, which is shipped as a standalone executable. The package manager operates strictly on the user-space and thus it requires no special permissions to install packages. It maintains all its files in a couple of places, so uninstalling the package manager itself is also easy. Finally, the package manager is also lightweight and fast.</p> <p>UL HPC provides support only for the Micromamba package manager.</p>"},{"location":"environment/best-practises/#installation","title":"Installation","text":"<p>A complete guide regarding Micromamba installation can be found in the official documentation. To install micromamaba in the HPC clusters, log in to Aion or Iris. Working on a login node, run the installation script, <pre><code>\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\n</code></pre> which will install the executable and setup the environment. There are 4 options to select during the installation of Micromamba:</p> <ul> <li>The directory for the installation of the binary file:   <pre><code>Micromamba binary folder? [~/.local/bin]\n</code></pre>   Leave empty and press enter to select the default displayed within brackets. Your <code>.bashrc</code> script should include <code>~/.local/bin</code> in the <code>$PATH</code> by default.</li> <li>The option to add to the environment autocomplete options for <code>micromamba</code>:   <pre><code>Init shell (bash)? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will append a clearly marked section in the <code>.bashrc</code> shell. Do not forget to remove this section when uninstalling Micromamba.</li> <li>The option to configure the channels by adding conda-forge:   <pre><code>Configure conda-forge? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will setup the <code>~/.condarc</code> file with <code>conda-forge</code> as the default channel. Note that Mamba and Micromamba will not use the <code>defaults</code> channel if it is not present in <code>~/.condarc</code> like <code>conda</code>.</li> <li>The option to select the directory where environment information and packages will be stored:   <pre><code>Prefix location? [~/micromamba]\n</code></pre>   Press enter to select the default option displayed within brackets.</li> </ul> <p>To activate the new environment log-out and log-in again. You now can use <code>micromamba</code> in the login and compute nodes, including the auto-completion feature.</p>"},{"location":"environment/best-practises/#managing-environments","title":"Managing environments","text":"<p>As an example we consider the creation and use of an environment for R jobs. The command, <pre><code>$ micromamba create --name R-project\n</code></pre> creates an environment named <code>R-project</code>. The environment is activated with the command <pre><code>$ micromamba activate R-project\n</code></pre> anywhere in the file system.</p> <p>Next, install the base R environment package that contains the R program, and any R packages required by the project. To install packages, first ensure that the <code>R-project</code> environment is active, and then install any package with the command <pre><code>$ micromamba install &lt;package_name&gt;\n</code></pre> all the required packages. Quite often, the channel where Conda should first look for the package must also be specified. Using the syntax <pre><code>$ micromamba install --chanell &lt;chanell_1&gt; --channels &lt;chanell_2&gt; &lt;package_name&gt;\n</code></pre> channels are listed in a series of <code>--channel &lt;channel_name&gt;</code> entries and the channels are searched in the order they appear. Using the syntax <pre><code>$ micromamba install &lt;chanell&gt;::&lt;package_name&gt;\n</code></pre> packages are searched in the specified channel only. Available packages can be found by searching the conda-forge channel.</p> <p>For instance, the basic functionality of the R software environment is contained in the <code>r-base</code> package. Calling <pre><code>micromamba install --channel conda-forge r-base\n</code></pre> or <pre><code>micromamba install conda-forge::r-base\n</code></pre> will install all the components required to run standalone R scripts. More involved scripts use functionality defined in various packages. The R packages are prepended with a prefix 'r-'. Thus, <code>plm</code> becomes <code>r-plm</code> and so on. After all the required packages have been installed, the environment is ready for use.</p> <p>Packages in the conda-forge channel come with instructions for their installation. Quite often the channel is specified in the installation instructions, <code>conda-forge::&lt;package name&gt;</code> or even <code>-c conda-forge</code> or <code>--channel conda-forge</code>. While the Micromamba installer sets-up <code>conda-forge</code> as the default channel, latter modification in <code>~/.condarc</code> may change the channel priority. Thus it is a good practice to explicitly specify the source channel when installing a package.</p> <p>After work in an environment is complete, deactivate the environment, <pre><code>$ micromamba deactivate\n</code></pre> to ensure that it does not interfere with any other operations. In contrast to modules, Conda is designed to operate with a single environment active at a time. Create one environment for each project, and Conda will ensure that any package that is shared between multiple environments is installed once.</p> <p>Micromamba supports almost all the subcommands of Conda. For more details see the official documentation.</p>"},{"location":"environment/best-practises/#using-environments-in-submission-scripts","title":"Using environments in submission scripts","text":"<p>Since all computationally heavy operations must be performed in compute nodes, Conda environments are also used in jobs submitted to the queuing system. You can activate and deactivate environment in various sections of your script.</p> <p>Returning to the R example, a submission script running a single core R job can use the <code>R-project_name</code> environment as follows: <pre><code>#SBATCH --job-name R-test-job\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=0-02:00:00\n#SBATCH --partition batch\n#SBATCH --qos normal\n\nmicromamba activate R-project\n\necho \"Launched at $(date)\"\necho \"Job ID: ${SLURM_JOBID}\"\necho \"Node list: ${SLURM_NODELIST}\"\necho \"Submit dir.: ${SLURM_SUBMIT_DIR}\"\necho \"Numb. of cores: ${SLURM_CPUS_PER_TASK}\"\n\nexport SRUN_CPUS_PER_TASK=\"${SLURM_CPUS_PER_TASK}\"\nexport OMP_NUM_THREADS=1\nsrun Rscript --no-save --no-restore script.R\n\nmicromamba deactivate\n</code></pre></p> <p>Environment activations in Conda are stacked, and unlike modules, only one environment is active at a time with the rest being pushed down the stack. Consider the following script excerpt. <pre><code># Initialization code\n\nmicromabma activate python-project\n\n# Code to run a simulation and generate output with Python\n\nmicromabma activate R-project\n\n# Code to perform statistical analysis and ploting with R\n\nmicromamba deactivate\n\n# Code to save data with Python\n</code></pre></p> <p>These script creates the following environment stack.</p> <pre><code>(base)\n|\n| # No software is available here\n|\n+-(python-project) # micromabma activate python-project\n| |\n| | # Only Python is available here\n| |\n| +-(R-project) # micromabma activate R-project\n| | |\n| | | # Only R is available here\n| | |\n| +-+ # micromamba deactivate\n| |\n| | # Only Python is available here\n| |\n</code></pre> <p>We can see that the Python environment (<code>python-project</code>) remains in the stack while the R environment (<code>R-project</code>) is active, and will be broght forth as soon as the R environment is deactivated.</p> <p>Useful scripting resources</p> <ul> <li>Formatting submission scripts for R (and other systems)</li> </ul>"},{"location":"environment/best-practises/#exporting-and-importing-environment-specifications","title":"Exporting and importing environment specifications","text":"<p>An important feature of Conda is that it allows you to export and version control you environment specifications, and recreate the environment on demand.</p> <ul> <li>A description of the software installed in the Conda environment can be exported on demand to a text file.</li> <li>In turn, a specification file can be used to populate a new environment, in effect recreating the environment.</li> </ul> <p>The environment reproducibility is particularly important when you want to have reproducible results, like for instance in a scientific simulation. You can setup and test your application in your local machine, save the environment, and later load the environment in an HPC system, and be sure that the application will behave identically. Conda in the background will ensure that identical packages will be installed.</p> <p>In Micromaba, you can export the specifications of an environment using the command: <pre><code>$ micromaba env export --name &lt;environment name&gt;\n</code></pre> By default the command prints to the standard output, but you can redirect the output to a file: <pre><code>$ micromaba env export --name &lt;environment name&gt; &gt; &lt;environment name&gt;.yaml\n</code></pre> To recreate an environment from a specification file, pass the file as argument to the create command with the <code>--file</code> flag: <pre><code>$ micromamba env create --name &lt;environment name&gt; --file &lt;environment name&gt;.yaml\n</code></pre> This workflow demonstrates the use of simple text files to store specifications, but Micormamba supports various specification file types. All specification files are text files and can be version controlled with a tool such as Git.</p> <p>Sources</p> <ul> <li>Micromamba User Guide: Specification files</li> </ul>"},{"location":"environment/best-practises/#example-installing-jupyter-and-managing-the-dependencies-of-a-notebook-with-micromamba","title":"Example: Installing Jupyter and managing the dependencies of a notebook with Micromamba","text":"<p>In this example we will create an environment, install Jupyter, and install all the dependencies for our notebooks with Micromamba. Start by creating an environment: <pre><code>micromamba env create --name jupyter\n</code></pre> Next, install Jupyter in the environment. Have a look at the page for <code>jupyterlab</code> in the conda-forge channel. To install it in your environment call: <pre><code>micromamba install --name jupyter conda-forge::jupyterlab\n</code></pre> Now activate the environment, create a working directory for your notebooks, and launch Jypyter: <pre><code>micromamba activate jupyter\nmkdir ~/Documents/notebooks &amp;&amp; cd ~/Documents/notebooks\njupyter lab\n</code></pre> If a webpage appears with the Jupyter lab, the installation worked succeeded!</p> <p>You may need some Python package in your Jupyter notebook. You can make packages available in your notebook by installing the appropriate package in the Conda environment. For instance, assume that you need <code>pandas</code> and <code>numpy</code>. Searching the conda-forge channel, we can find the package name and installation instruction. With the <code>jupyter</code> environment active, run the command: <pre><code>micromamba install conda-forge::numpy conda-forge::pandas\n</code></pre> You should now be able to import <code>numpy</code> and <code>pandas</code> in your notebook!</p> <p>After completing your work, close down the notebook with the command <code>C-c</code>, and deactivate the <code>jupyter</code> Conda environment: <pre><code>micromamba deactive\n</code></pre> You should now be in your normal operating system environment.</p>"},{"location":"environment/best-practises/#self-management-of-work-environments-in-ul-hpc-with-conda","title":"Self management of work environments in UL HPC with Conda","text":"<p>Conda is one of the systems for providing software in UL HPC systems, along with modules and containers. When starting a new project it is important to select the appropriate system.</p> <p>Before installing any software yourself in user space you should contact the ULHPC High Level Support Team in the service portal [Home &gt; Research &gt; HPC &gt; Software environment &gt; Request expertise] to check if we can install the software in our system. A system wide installation will not consume any of your storage quota, and it will be configured and tested to provide optimal efficiency.</p>"},{"location":"environment/best-practises/#when-a-conda-environment-is-useful","title":"When a Conda environment is useful","text":"<p>There are three aspects of environment management that you should consider when selecting the method with which you will manage your software.</p> <ul> <li> <p>Ease of use: Many software systems whose performance is not critical and are used by relatively few users are not provided though the standard distribution channels of modules or containers. In such cases the easiest installation option is a user side installation with Conda or some similar package management system.</p> </li> <li> <p>Reproducibility: Conda and containers can both create reproducible environments, with descriptions of the environment exported and version controlled in text files. However, containers require significant amount of manual configuration to create a reproducible environment and to perform well in a wide range of systems. If your aim is an easily reproducible environment Conda is the superior choice.</p> </li> <li> <p>Performance: Conda provides precompiled executables. Even thought multiple configurations are supported, you will not always find an executable tailored to your target system. Modules and containers provided by UL UPC are optimized to ensure performance and stability in our systems, so prefer them.</p> </li> </ul>"},{"location":"environment/best-practises/#storage-limitations-in-ul-hpc","title":"Storage limitations in UL HPC","text":"<p>Regardless of installation method, when you install software in user space you are using up your storage quota. Conda environment managers download and store a sizable amount of data to provided packages to the various environments. Even though the package data are shared between the various environments, they still consume space in your or your project's account. There are limits in the storage space and number of files that are available to projects and users in the cluster. Since Conda packages are self managed, you need to clean unused data yourself.</p>"},{"location":"environment/best-practises/#cleaning-up-package-data","title":"Cleaning up package data","text":"<p>There are two main sources of unused data, compressed archives of packages that Conda stores in its cache when downloading a new package, and data of packages no longer used in any environment. All unused data in Micromoamba can be removed with the command <pre><code>micromamba clean --all --yes\n</code></pre> where the flag <code>--yes</code> suppresses an interactive dialogue with details about the operations performed. In general you can use the default options with <code>--yes</code>, unless you have manually edited any files in you package data directory (default location <code>~/micromamba</code>) and you would like to preserve your changes.</p> <p>Updating environments to remove old package versions</p> <p>As we create new environments, we often install the latest version of each package. However, if the environments are not updated regularly, we may end up with different versions of the same package across multiple environments. If we have the same version of a package installed in all environments, we can save space by removing unused older versions.</p> <p>To update a package across all environments, use the command <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --yes --name $e &lt;package name&gt;; done\n</code></pre> and to update all packages across all environments <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --yes --name $e --all; done\n</code></pre> where <code>FNR&gt;2</code> removes the headers in the output of <code>micromamba env list</code>, and is thus sensitive to changes in the user interface of Micromamba.</p> <p>After updating packages, the <code>clean</code> command can be called to removed the data of unused older package versions.</p> <p>Sources</p> <ul> <li>Oficial Conda <code>clean</code> documentation</li> <li>Understanding Conda <code>clean</code></li> </ul>"},{"location":"environment/best-practises/#a-note-about-internal-workings-of-conda","title":"A note about internal workings of Conda","text":"<p>In general, Conda packages are stored in a central directory, and hard links are created in the library directories of any environment that requires the package. Since hard links do not consume space and inodes, Conda is very efficient in its usage of storage space.</p> <p>Consider for instance the MPFR package used in some environment <code>gaussian_regression</code>. Looking into the Conda installation managed by Micromamba, these are the installed library files: <pre><code>gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi pkgs/mpfr-4.2.1-h9458935_0/lib/\ntotal 1.3M\n5286432 drwxr-xr-x 1 gkaf gkaf   94 Oct 25 13:59 ./\n5286426 drwxr-xr-x 1 gkaf gkaf   38 Oct 25 13:59 ../\n5286436 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so -&gt; libmpfr.so.6.2.1*\n5286441 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so.6 -&gt; libmpfr.so.6.2.1*\n5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 libmpfr.so.6.2.1*\n5286442 drwxr-xr-x 1 gkaf gkaf   14 Oct 25 13:59 pkgconfig/\n</code></pre> Looking into the libraries of the <code>gaussian_regression</code> environment, there is a hard link to the MPFR library: <pre><code>gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi envs/gaussian_regression/lib/libmpfr.so.6.2.1 \n5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 envs/gaussian_regression/lib/libmpfr.so.6.2.1*\n</code></pre> You can use the <code>-i</code> flag in <code>ls</code> to print the inode number of a file. Hard links have the same inode number, meaning that they are essentially the same file.</p> <p>Conda will not automatically check if the files in the <code>pkgs</code> directories must be removed. For instance, when you uninstall a package from an environment, when you delete an environment, or when a package is updated in an environment, only the hard link in the environment directory will change. The files in <code>pkgs</code> will remain even if they are no longer used in any environment. The relevant <code>clean</code> routines check which packages are actually used and remove the unused files.</p>"},{"location":"environment/best-practises/#environment-management-best-practices","title":"Environment management best practices","text":"<p>Environment management systems are diverse but support a simple common set of features, which are:</p> <ul> <li>the ability to create and reproduce software environments,</li> <li>isolation between environments and between each environment and the system, and</li> <li>easy access to multiple sources of software packages.</li> </ul> <p>The environment of a software system can be categorized in 2 components,</p> <ul> <li>the system comprising of installed software components and environment settings, and</li> <li>the packages added to various software components of the environment.</li> </ul> <p>The system is a superset of the packages in the environment, however, it makes sense to consider them separately as many software distributions provide native managers for the packages they ship.</p> <p></p> <p>Environment management systems usually focus on the management of the system, installing software, setting environment variables and so on. Package management systems on the other hand usually focus on managing the packages installed for some specific software component. The distinction albeit useful is not always clear, and usually environment managers that can also manage the packages of some software systems.</p> <p>Furthermore, both environment and package management systems can be further subdivided according to the extend of the target environment. The management systems can target</p> <ul> <li>the whole system (system environment management tools), or</li> <li>the environment within a single project directory (project environment management tools).</li> </ul> <p>All project environment management tools and most system environment management tools provide methods to</p> <ul> <li>store the environment setup in a text file that can be version controlled, and</li> <li>to recreate the environment from the description in the text file.</li> </ul> <p>The project environment management tools in particular often automate the function of updating the text file describing the environment in order to automatically initialize the environment when initializing a new instance of the project. Thus project environments are very useful in storing a distributing research projects, since they automate the reproducibility of the project setup.</p> <p>The overarching theory for environment management tools such as Conda is simple. However, there are implementation details which affect how environment management tools are used. In the following section we present some case studies about practical issues you may encounter with few environment management tools. </p>"},{"location":"environment/best-practises/#using-pip-for-python-package-management","title":"Using <code>pip</code> for Python package management","text":"<p>The official package installer for Python is <code>pip</code>. You can use pip to install packages from the Python Package Index (PyPI) and other indexes. With <code>pip</code> you can install packages in 3 modes,</p> <ul> <li>system-wide installation, where a package is available to all users,</li> <li>user-wide installation, where a package are installed in a special directory in the user home directory and are available to the user only, and</li> <li>environment installation where a package is only available inside the environment where it was installed.</li> </ul> <p>Python is now part of many Linux distributions, such as Debian. This means now that users cannot (or should not) install packages system-wide using <code>pip</code>. User-wide and <code>venv</code> environment installations are still possible though.</p> <p>To install a package user-wide use the command: <pre><code>$ pip install --user &lt;package_name&gt;\n</code></pre> User-wide installations install packages in a directory located at the home directory of the user. Python searches for packages in the user installation path first. This packages are meant to be installed with the system Python and run without root privileges. Thus user-wide installation is appropriate for packages that extend the system functionality for a single user.</p> <p>User-wide installation is not recommended for all but the system Python environment. User-wide installations rely on system packages, so if you install any package using a Conda environment the package will depend on the Conda environment. Thus, packages installed user-wide with a Conda environment will may be incompatible with the system environment or with any other Conda environment.</p>"},{"location":"environment/best-practises/#python-venv-virtual-environments","title":"Python <code>venv</code> virtual environments","text":"<p>For most applications, functionality should be installed in a virtual environment. To use a <code>venv</code> environment, first initialize the environment in some directory. The official distribution of Python comes packaged with the <code>venv</code> module that can create a virtual environment with the command: <pre><code>$ python -m venv &lt;path_to_directory&gt;\n</code></pre> You activate the environment with <pre><code>$ source &lt;path_to_directory&gt;/bin/activate\n</code></pre> and deactivate with: <pre><code>$ deactivate\n</code></pre> When the environment is active, you can install packages with the command: <pre><code>$ pip install &lt;package_name&gt;\n</code></pre> This is the same command installing packages system-wide, but because the environment is active the package is installed in the environment directory.</p> <p>The <code>pip</code> package manager also provides the functionality required to export environment setups in text files and recreate environments from a text file description of the environment setup. To export the environment, activate the environment and use the <code>freeze</code> command: <pre><code>$ source &lt;path_to_directory&gt;/bin/activate\n(&lt;environment name&gt;) $ pip freeze &gt; &lt;environment name&gt;.yml\n</code></pre> To recreate the environment, create an empty environment, <pre><code>python -m venv ~/environments/&lt;environment name&gt;\n</code></pre> activate the environment, <pre><code>source ~/environments/&lt;environment name&gt;/bin/activate\n</code></pre> and install all the required packages from the requirement file: <pre><code>(&lt;environment name&gt;) $ pip install --requirement /path/to/&lt;environment name&gt;.yml\n</code></pre></p> <p>When managing an environment, it is often required to upgrade a package to access new functionality or fix a bug. To upgrade a package with <code>pip</code>, simply install/reinstall the package with the <code>--upgrade</code> flag: <pre><code>(&lt;environment name&gt;) $ pip install --upgrade &lt;package name&gt;\n</code></pre> Note that in Python environments managed with <code>pip</code>, you have to update each package individually. Package management can thus be quote laborious, but there are some methods to speed up the process.</p> <p>Finally, to delete a virtual environment installation simply deactivate the environment and remove its directory. Thus, assuming that an environment was installed in <code>~/environments/&lt;environment name&gt;</code>, remove it with: <pre><code>$ rm -r ~/environments/&lt;environment name&gt;\n</code></pre></p> <p>With all the features that <code>pip</code> provides, it is a very attractive method for creating and managing environments. However, is often necessary even when you are managing your environments with <code>pip</code>!</p>"},{"location":"environment/best-practises/#creating-a-venv-environment-with-python-installed-in-a-conda-environment","title":"Creating a <code>venv</code> environment with Python installed in a Conda environment","text":"<p>In most distributions <code>venv</code> comes packages together with python, however, in some distributions <code>venv</code> is provided as an independent package. In Debian for instance, the system Python package, <code>python3</code>,  does not contain the <code>venv</code> module. The module is distributed through the <code>python3-venv</code> package. You may want to avoid using the system package for a variety of reasons:</p> <ul> <li>The PyPI package you want to install may require a version of Python that is not provided by your system.</li> <li>You may be in a constrained system where installing a package such as <code>python3-venv</code> is not possible, either because you don't have the rights, or because you are working with some high performance computer where installing a package required installing the package in hundreds of machines.</li> <li>You may not want to pollute your system installation. In accordance to the UNIX philosophy use the system ensures that your machine operates correctly, and use the Conda to run the latest application!</li> </ul> <p>The trick is to install a version of Python in a Conda environment, and use the Python installed in the Conda environment to create <code>venv</code> environments. Start by creating a python environment with the required version for python. For instance: <pre><code>$ micromamba env create --name &lt;environment name&gt;-python\n$ micromamba install --name &lt;environment name&gt;-python conda-forge::python\n</code></pre> The Python distribution from conda-forge comes with <code>venv</code> installed. Use <code>venv</code> to create your virtual environment: <pre><code>$ micromamba run --name &lt;environment name&gt;-python python -m venv ~/environments/&lt;environment name&gt;\n</code></pre> It is a good idea to use some standard location to install global environments; a good choice is some directory in your home directory such as: <pre><code>~/environments\n</code></pre> Now an environment has been installed with the required version of python.</p> <p>Any other tool requires the activation of the Conda environment before you are able to activate an environment created by an internal tool, such as <code>venv</code>. In fact, you can activate your <code>venv</code> as follows: <pre><code>$ micromaba activate &lt;environment name&gt;-python\n(&lt;environment name&gt;-python) $ source ~/environments/&lt;environment name&gt;/bin/activate\n(&lt;environment name&gt;) (&lt;environment name&gt;-python) $\n</code></pre> However, activating the Conda environment is redundant with a <code>venv</code> environment. Yow you can activate the environment simply with: <pre><code>source ~/environments/&lt;environment name&gt;/bin/active\n(&lt;environment name&gt;) $\n</code></pre></p> <p>After creating a <code>venv</code> environment you no longer need to interact with the Conda environment, <code>&lt;environment name&gt;-python</code>, except for updating Python itself. This is because <code>pip</code> environments are completely isolated from the system, except for the Python executable. If you browse the environment definition file, <pre><code>$ cat ~/environments/&lt;environment name&gt;/pyvenv.cfg\nhome = /home/gkaf/micromamba/envs/&lt;environment name&gt;-python/bin\ninclude-system-site-packages = false\nversion = 3.8.18\n</code></pre> you can see that system package usage is disabled by default, with the option <code>include-system-site-packages = false</code>. Also, note that the binary directory of the Conda environment that provides the Python executable is noted in the <code>home</code> entry. This is important, as the only package that <code>pip</code> cannot install is python itself. The python executable is selected the environment is created with the command: <pre><code>$ python -m venv ~/environments/&lt;environment name&gt;\n</code></pre> In fact, listing the contents of <code>~/environments/&lt;environment name&gt;/bin</code>, there are the symbolic links <pre><code>python -&gt; /home/gkaf/micromamba/envs/&lt;environmen name&gt;-python/bin/python\npython3 -&gt; python\n</code></pre> that point to the Python installed in our Conda environment. Every other executable and library package installed in our environment, is installed locally, and as the options <code>include-system-site-packages = false</code> suggests, it will shadow any other package in the Conda environment.</p>"},{"location":"environment/best-practises/#example-installing-jupyter-and-managing-the-dependencies-of-a-notebook-with-pip","title":"Example: Installing Jupyter and managing the dependencies of a notebook with <code>pip</code>","text":"<p>In this example we create an environment, install Jupyter, and install all the dependencies for our notebooks with <code>pip</code>. Start by creating a Conda environment for the Python used in the <code>venv</code> environment: <pre><code>micromamba env create --name jupyter-python\n</code></pre> Next, install Python in the environment. Have a look at the page for Python in the conda-forge channel. To install it in your environment call: <pre><code>micromamba install --name jupyter-python conda-forge::python\n</code></pre> Now use the python of the <code>jupyter-python</code> environment to create your <code>venv</code> environment. We use the <code>run</code> command of Conda to execute a single shot command: <pre><code>micromamba run --name jupyter-python python -m venv ~/environments/jupyter\n</code></pre> We are now ready to activate our <code>venv</code> environment and install our main dependencies: <pre><code>$ source ~/environments/jupyter/bin/activate\n(jupyter) $ pip install jupyterlab\n</code></pre> Now create a working directory for your notebooks, and launch Jupyter: <pre><code>(jupyter) $ mkdir ~/Documents/notebooks &amp;&amp; cd ~/Documents/notebooks\n(jupyter) $ jupyter lab\n</code></pre> If a webpage appears with the Jupyter lab, the installation worked succeeded!</p> <p>You may need some Python package in your Jupyter notebook. You can install new packages at your <code>venv</code> environment. For instance, assume that you need <code>pandas</code> and <code>numpy</code>. Activate the environment, and install the dependencies with the command: <pre><code>(jupyter) $ pip install numpy pandas\n</code></pre> You should now be able to import <code>numpy</code> and <code>pandas</code> in your notebook!</p> <p>After completing your work, close down the notebook with the command <code>C-c</code>, and deactivate the <code>jupyter</code> environment: <pre><code>(jupyter) $ deactivate\n</code></pre> You should now be in your normal operating system environment.</p>"},{"location":"environment/best-practises/#combining-conda-with-other-package-management-tools","title":"Combining Conda with other package management tools","text":"<p>In some cases Conda is used to manage the environment and other tools are used to install pacakges. There are a few reasons why you may want to manage packages with different tools.</p> <ul> <li> <p>You may want to use a project environment management tool. For instance, you may install Python with Conda and use project environments managed with Virtualenv, Pipenv and Poetry. In the case of R you may install R with Conda and manage project environments with Packrat. </p> </li> <li> <p>In some cases packages are not available through Conda, but they may be available through other source code or binary distributions. A typical example is Julia where packages are only available trough the Pkg package manager. Similarly, many less popular packages are available through PyPI and can be installed with <code>pip</code>, but they are not available through a Conda channel.</p> </li> </ul> <p>A list of case studies follows. In these case studies you can find how Conda and other package managers can be combined to install software.</p>"},{"location":"environment/best-practises/#installing-packages-with-pip-in-a-conda-environment","title":"Installing packages with <code>pip</code> in a Conda environment","text":"<p>In this example Conda and <code>pip</code> are used to create an environment for working with with MkDocs. We assume that we want to work with a custom python distribution installed in a Conda environment and we want to install 2 packages,</p> <ul> <li><code>mkdocs</code>, and</li> <li><code>mkdocs-minify-plugin</code>.</li> </ul> <p>While <code>mkdocs</code> is available though the conda-forge Conda channel, at the time of witting <code>mkdocs-minify-plugin</code> is only available through PyPI. At this point we have 3 options,</p> <ul> <li>install <code>mkdocs</code> and <code>mkdocs-minify-plugin</code> in a <code>venv</code> environment,</li> <li>install <code>mkdocs</code> and <code>mkdocs-minify-plugin</code> in the Conda environment from PyPI with <code>pip</code>, or</li> <li>install <code>mkdocs</code> in a Conda and <code>mkdocs-minify-plugin</code> with <code>pip</code>, by installing <code>mkdocs-minify-plugin</code> either<ul> <li>directly in the Conda environment, or</li> <li>in a <code>venv</code> environment end enabling <code>venv</code> to include system packages to access <code>mkdocs</code>.</li> </ul> </li> </ul> <p>To begin with, create the base Conda environment which will provide the Python executable and related tools such as <code>pip</code> and <code>venv</code>. <pre><code>micromamba env create --name mkdocs-python\n</code></pre> Exporting the environment, <pre><code>$ micromamba env export --name mkdocs-python\nname: mkdocs-python\nchannels:\ndependencies:\n</code></pre> we can see that the environment is empty. Start by installing Python: <pre><code>micromamba install --name mkdocs-python conda-forge::python\n</code></pre> Exporting the environment specifications, <pre><code>$ micromamba env export --name mkdocs-python\nname: mkdocs-python\nchannels:\n- conda-forge\ndependencies:\n- _libgcc_mutex=0.1=conda_forge\n- _openmp_mutex=4.5=2_gnu\n- bzip2=1.0.8=hd590300_5\n- ca-certificates=2024.2.2=hbcca054_0\n- ld_impl_linux-64=2.40=h41732ed_0\n- libexpat=2.6.2=h59595ed_0\n- libffi=3.4.2=h7f98852_5\n- libgcc-ng=13.2.0=h807b86a_5\n- libgomp=13.2.0=h807b86a_5\n- libnsl=2.0.1=hd590300_0\n- libsqlite=3.45.2=h2797004_0\n- libuuid=2.38.1=h0b41bf4_0\n- libxcrypt=4.4.36=hd590300_1\n- libzlib=1.2.13=hd590300_5\n- ncurses=6.4.20240210=h59595ed_0\n- openssl=3.2.1=hd590300_1\n- pip=24.0=pyhd8ed1ab_0\n- python=3.12.2=hab00c5b_0_cpython\n- readline=8.2=h8228510_1\n- setuptools=69.2.0=pyhd8ed1ab_0\n- tk=8.6.13=noxft_h4845f30_101\n- tzdata=2024a=h0c530f3_0\n- wheel=0.43.0=pyhd8ed1ab_0\n- xz=5.2.6=h166bdaf_0\n</code></pre> we can see that some version of <code>pip</code> (<code>pip=24.0=pyhd8ed1ab_0</code>) is installed, along with other packages.</p> <p>In all cases we will use <code>pip</code>, the PyPI package manager, that comes packages with Python. Make sure that in each case you start with an empty environment with only Python installed.</p>"},{"location":"environment/best-practises/#installing-all-dependencies-on-a-venv-environment","title":"Installing all dependencies on a <code>venv</code> environment","text":"<p>The simplest option is to install all the required packages in a <code>venv</code> virtual environment. Start by creating an environment with the <code>venv</code> module of the Python installed in the <code>mkdocs-python</code> environment: <pre><code>micromamba run --name mkdocs-python python -m venv ~/environments/mkdocs\n</code></pre> Activate the environment, <pre><code>$ source ~/environment/mkdocs/bin/active\n</code></pre> and export the environment specification, <pre><code>(mkdocs) $ pip freeze\n</code></pre> which should result in no output since we have not installed ant packages yet. Some infrastructure packages are installed in the environment by default, all installed packages can be printed with the command <code>pip list</code>, <pre><code>(mkdocs) $ pip list --format=freeze\npip==24.0\n</code></pre> where the <code>--format</code> option selected ensures that the output if produced in a YAML file format. We can see that in the environment there is the <code>pip</code> package installed. Infrastructure packages are not version tracked as they are required only for <code>pip</code>, and not for packages provided by the environment.</p> <p>With the <code>venv</code> environment active, install the packages: <pre><code>(mkdocs) $ pip install --upgrade mkdocs mkdocs-minify-plugin\n</code></pre> Now the <code>pip freeze</code> command should print all the installed packages: <pre><code>(mkdocs) $ pip freeze\nclick==8.1.7\ncsscompressor==0.9.5\nghp-import==2.1.0\nhtmlmin2==0.1.13\nJinja2==3.1.3\njsmin==3.0.1\nMarkdown==3.6\nMarkupSafe==2.1.5\nmergedeep==1.3.4\nmkdocs==1.5.3\nmkdocs-minify-plugin==0.8.0\npackaging==24.0\npathspec==0.12.1\nplatformdirs==4.2.0\npython-dateutil==2.9.0.post0\nPyYAML==6.0.1\npyyaml_env_tag==0.1\nsix==1.16.0\nwatchdog==4.0.0\n</code></pre> Again, the only difference with the <code>pip list</code> command is that the package for <code>pip</code> is self is not listed.</p> <p>Overall, installing the  required packages in a separate <code>venv</code> virtual Python environment is the simplest solution. There is a distinction between the packages managed by the Conda package manager and <code>pip</code>, with Conda managing only the Python distribution, and <code>pip</code> managing the environment packages and the <code>pip</code> installation. However, this distinction comes at the cost of replicating package installations, as unlike Conda, package files are not share in <code>venv</code> virtual Python environments.</p>"},{"location":"environment/best-practises/#installing-all-dependencies-from-pypi-with-pip-on-the-conda-environment","title":"Installing all dependencies from PyPI with <code>pip</code> on the Conda environment","text":"<p>The <code>pip</code> package manager is in fact able to install packages directly on the Conda environment! When the Conda environment is active, the system environment for <code>pip</code> is the Conda environment. Activate the Conda environment, and list the packages with <code>pip</code>: <pre><code>$ micromamba activate mkdocs-coda\n(mkdocs-conda) $ pip list --format=freeze\npip==24.0 \nsetuptools==69.2.0\nwheel==0.43.0\n</code></pre> Note that we use the <code>list</code> command instead of the <code>freeze</code> command to export all packages. We observe that the packages</p> <ul> <li><code>pip</code>,</li> <li><code>setuptools</code>, and</li> <li><code>wheel</code>,</li> </ul> <p>are installed with the Conda package manager, and are also visible in the output of <code>micromamba env export</code>.</p> <p>Do not mix package management tools! Packages installed with the Conda package manager should be managed with Conda, and package installed with <code>pip</code> should be managed with <code>pip</code>. All the packages listed so far are installed with the Conda package manager and should not be updated or otherwise altered with <code>pip</code>.</p> <p>Now install the <code>mkdocs</code> and <code>mkdocs-minify-plugin</code> with <code>pip</code> while the <code>mkdocs-conda</code> environment is active: <pre><code>(mkdocs-conda) $ pip install --upgrade mkdocs mkdocs-minify-plugin\n</code></pre> Now all the dependencies are installed in the Conda environment, and can be listed with: <pre><code>(mkdocs-conda) $ pip list --format=freeze\n$ pip list --format=freeze\nclick==8.1.7\ncsscompressor==0.9.5\nghp-import==2.1.0\nhtmlmin2==0.1.13\nJinja2==3.1.3\njsmin==3.0.1\nMarkdown==3.6\nMarkupSafe==2.1.5\nmergedeep==1.3.4\nmkdocs==1.5.3\nmkdocs-minify-plugin==0.8.0\npackaging==24.0\npathspec==0.12.1\npip==24.0\nplatformdirs==4.2.0\npython-dateutil==2.9.0.post0\nPyYAML==6.0.1\npyyaml_env_tag==0.1\nsetuptools==69.2.0\nsix==1.16.0\nwatchdog==4.0.0\nwheel==0.43.0\n</code></pre></p> <p>Note that Conda is aware of which packages is meant to manage, listing the Conda packages <pre><code>(mkdics-conda) $ micromamba env export --name mkdocs-conda\nname: mkdocs-conda\nchannels:\n- conda-forge\ndependencies:\n- _libgcc_mutex=0.1=conda_forge\n- _openmp_mutex=4.5=2_gnu\n- bzip2=1.0.8=hd590300_5\n- ca-certificates=2024.2.2=hbcca054_0\n- ld_impl_linux-64=2.40=h41732ed_0\n- libexpat=2.6.2=h59595ed_0\n- libffi=3.4.2=h7f98852_5\n- libgcc-ng=13.2.0=h807b86a_5\n- libgomp=13.2.0=h807b86a_5\n- libnsl=2.0.1=hd590300_0\n- libsqlite=3.45.2=h2797004_0\n- libuuid=2.38.1=h0b41bf4_0\n- libxcrypt=4.4.36=hd590300_1\n- libzlib=1.2.13=hd590300_5\n- ncurses=6.4.20240210=h59595ed_0\n- openssl=3.2.1=hd590300_1\n- pip=24.0=pyhd8ed1ab_0\n- python=3.12.2=hab00c5b_0_cpython\n- readline=8.2=h8228510_1\n- setuptools=69.2.0=pyhd8ed1ab_0\n- tk=8.6.13=noxft_h4845f30_101\n- tzdata=2024a=h0c530f3_0\n- wheel=0.43.0=pyhd8ed1ab_0\n- xz=5.2.6=h166bdaf_0\n</code></pre> we can see that the packages managed by Conda have not changed!</p> <p>Updating such a combined environment can be tricky. Start by updating all the Conda packages with <pre><code>micromamba update --name mkdocs-conda --all\n</code></pre> and then active the environment and update all the packages managed by <code>pip</code> one by one. Note that with the Conda environment active</p> <ul> <li>packages that appear in the output of <code>micromamba env export</code> and <code>pip list --format=freeze</code> are managed by Conda, and</li> <li>packages that appear in the output of <code>pip list --format=freeze</code> are managed by <code>pip</code>.</li> </ul> <p>Some Conda package managers integrate some function of the <code>pip</code> interface. For instance, recreating the same environment with official Conda package manager, <code>conda</code>, the packages exported are: <pre><code>$ conda env export --name mkdocs-python\nname: mkdocs-python\nchannels:\n  - conda-forge\n  - nodefaults\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=2_gnu\n  - bzip2=1.0.8=hd590300_5\n  - ca-certificates=2024.2.2=hbcca054_0\n  - ld_impl_linux-64=2.40=h41732ed_0\n  - libexpat=2.6.2=h59595ed_0\n  - libffi=3.4.2=h7f98852_5\n  - libgcc-ng=13.2.0=h807b86a_5\n  - libgomp=13.2.0=h807b86a_5\n  - libnsl=2.0.1=hd590300_0\n  - libsqlite=3.45.2=h2797004_0\n  - libuuid=2.38.1=h0b41bf4_0\n  - libxcrypt=4.4.36=hd590300_1\n  - libzlib=1.2.13=hd590300_5\n  - ncurses=6.4.20240210=h59595ed_0\n  - openssl=3.2.1=hd590300_1\n  - pip=24.0=pyhd8ed1ab_0\n  - python=3.12.2=hab00c5b_0_cpython\n  - readline=8.2=h8228510_1\n  - setuptools=69.2.0=pyhd8ed1ab_0\n  - tk=8.6.13=noxft_h4845f30_101\n  - tzdata=2024a=h0c530f3_0\n  - wheel=0.43.0=pyhd8ed1ab_0\n  - xz=5.2.6=h166bdaf_0\n  - pip:\n      - click==8.1.7\n      - csscompressor==0.9.5\n      - ghp-import==2.1.0\n      - htmlmin2==0.1.13\n      - jinja2==3.1.3\n      - jsmin==3.0.1\n      - markdown==3.6\n      - markupsafe==2.1.5\n      - mergedeep==1.3.4\n      - mkdocs==1.5.3\n      - mkdocs-minify-plugin==0.8.0\n      - packaging==24.0\n      - pathspec==0.12.1\n      - platformdirs==4.2.0\n      - python-dateutil==2.9.0.post0\n      - pyyaml==6.0.1\n      - pyyaml-env-tag==0.1\n      - six==1.16.0\n      - watchdog==4.0.0\nprefix: /home/gkaf/micromamba/envs/conda/envs/mkdocs-python\n</code></pre> Note that in this output, <code>conda</code> package manager indicates correctly that the packages <code>pip</code>, <code>wheel</code>, and <code>setuptools</code> are managed by <code>conda</code>. Creating a clean environment with the resulting <code>YAML</code> file using the <code>--file</code> option will install the <code>pip</code> packages as well. Even though Micromamba does not support exporting the <code>pip</code> installed dependencies, it supports importing files with <code>pip</code> dependencies for compatibility.</p> <p>Despite any integration that <code>conda</code> offers for exporting <code>pip</code> installed packages, it is still the responsibility of the user to ensure that packages are managed with the correct package manager.</p>"},{"location":"environment/best-practises/#mixing-packages-from-conda-channels-and-pypi","title":"Mixing packages from Conda channels and PyPI","text":"<p>The <code>mkdocs</code> package is quite popular and as a result it is available through the conda-forge Conda channel. At the time of writing, the package <code>mkdocs-minify-plugin</code> is only available though PyPI. Given that the version on <code>mkdocs</code> in conda-forge is compatible with the version of <code>mkdocs-minify-plugin</code> in PyPI, you may consider installing the <code>mkdocs</code> from conda-forge, and <code>mkdocs-minify-plugin</code> from PyPI.</p> <p>Start by creating an empty Conda environment for Python, and install Python: <pre><code>$ micromamba create --name mkdocs-conda\n$ micromamba install --name mkdocs-conda conda-forge::python\n</code></pre> Then, install <code>mkdocs</code> from the conda-forge channel in the <code>mkdocs-conda</code> environment: <pre><code>$ micromamba install --name mkdocs-conda conda-forge::mkdocs\n</code></pre></p> <p>You have now installed <code>mkdocs</code> through the conda-forge channel in the Conda environment. There are now 2 options for installing <code>mkdocs-minify-plugin</code> from PyPI, you can install it</p> <ul> <li>in the Conda environment from PyPI with <code>pip</code>, or</li> <li>in an isolated <code>venv</code> virtual Python environment.</li> </ul> <p>The option of installing packages from PyPI in a Conda environment has already been discussed in the previous section. The most interesting option is installing <code>mkdocs-minify-plugin</code> in a <code>venv</code> that has access to the Conda environment package <code>mkdocs</code>.</p> <p>Create a <code>vanv</code> that has access to system packages with the command: <pre><code>$ micromamba run --name mkdocs-conda python -m venv --system-site-packages ~/environments/mkdocs\n</code></pre> The resulting configuration configuration file <pre><code>$ cat ~/environments/mkdocs/pyvenv.cfg \nhome = /home/gkaf/micromamba/envs/mkdocs-conda/bin\ninclude-system-site-packages = true\nversion = 3.12.2\nexecutable = /home/gkaf/micromamba/envs/mkdocs-conda/bin/python3.12\ncommand = /home/gkaf/micromamba/envs/mkdocs-conda/bin/python -m venv --system-site-packages /home/gkaf/environments/mkdocs\n</code></pre> has the <code>include-system-site-packages</code> option enabled. Activate the environment and export the environment setup: <pre><code>$ source ~/environments/mkdocs/bin/activate\n(mkdocs) $ pip freeze\nclick @ file:///home/conda/feedstock_root/build_artifacts/click_1692311806742/work\ncolorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\nghp-import @ file:///home/conda/feedstock_root/build_artifacts/ghp-import_1651585738538/work\nimportlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1710971335535/work\nJinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1704966972576/work\nMarkdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1710435156458/work\nMarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1706899920239/work\nmergedeep @ file:///home/conda/feedstock_root/build_artifacts/mergedeep_1612711302171/work\nmkdocs @ file:///home/conda/feedstock_root/build_artifacts/mkdocs_1695086541719/work\npackaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1710075952259/work\npathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1702249949303/work\nplatformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1706713388748/work\npython-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work\nPyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1695373450623/work\npyyaml_env_tag @ file:///home/conda/feedstock_root/build_artifacts/pyyaml-env-tag_1624388951658/work\nsetuptools==69.2.0\nsix @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\ntyping_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1708904622550/work\nwatchdog @ file:///home/conda/feedstock_root/build_artifacts/watchdog_1707295114593/work\nwheel==0.43.0\nzipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work\n</code></pre> The <code>freeze</code> command now list the packages that are provided by the environment and the packages that are provided by the system which in our case is our Conda environment.</p> <p>To recreate the environment from a YAML file with the environment specifications,</p> <ul> <li>enable the <code>include-system-site-packages</code> option during the creation of the new environment, and</li> <li>when installing the packages from the YAML file, ensure that all the dependences are located in the path mentioned in the specification file.</li> </ul> <p>Despite the face that the package manager exports the dependencies correctly, the user is still responsible for using the correct tool to manage the packages. Listing all the packages in the environment, <pre><code>(mkdocs) $ pip list --format=freeze\nclick==8.1.7\ncolorama==0.4.6\nghp-import==2.1.0\nimportlib_metadata==7.1.0\nJinja2==3.1.3\nMarkdown==3.6\nMarkupSafe==2.1.5\nmergedeep==1.3.4\nmkdocs==1.5.3\npackaging==24.0\npathspec==0.12.1\npip==24.0\nplatformdirs==4.2.0\npython-dateutil==2.9.0\nPyYAML==6.0.1\npyyaml_env_tag==0.1\nsetuptools==69.2.0\nsix==1.16.0\ntyping_extensions==4.10.0\nwatchdog==4.0.0\nwheel==0.43.0\nzipp==3.17.0\n</code></pre> we can see that <code>pip</code> lists all the packages irrespective of whether they are installed in the environment of the system. The advantage of using a <code>venv</code> however, is that any change made with <code>pip</code> will be local to the <code>venv</code> and will simply override without altering the Conda installation.</p>"},{"location":"environment/best-practises/#combining-conda-with-pip-to-install-a-python-source-package","title":"Combining Conda with <code>pip</code> to install a Python source package","text":"<p>Let's consider the installation of the PySPQR module that wraps the SuiteSparseQR decomposition function for use with SciPy. Installing this software is challenging because</p> <ul> <li>the PySPQR package is not available through Conda channels, and</li> <li>installing PySPQR (<code>sparseqr</code>) directly in a <code>venv</code> fails. </li> </ul> <p>The PySPQR package is a source code package and its installation in a <code>venv</code> requires linking with the SuiteSparse library. We can install SparseSuite in our system (e.g. <code>apt-get install libsuitesparse-dev</code> in Debian), but we will avoid modifying system libraries and use a Conda environment instead.</p> <p>A Conda environment is a directory with packages where you can install anything you need with a compatible package manager like Micromamba. Create an environment with all the required dependencies: <pre><code>$ micromamba env create --name python-suitesparse\n$ micromamba install conda-forge::python conda-forge::suitesparse --name python-suitesparse\n</code></pre></p> <p>The package <code>conda-forge::suitesparse</code> is effectively what <code>apt-get install libsuitesparse-dev</code> installs globally in a Deabian based system, but now it is only available in the Conda environment <code>python-suitesparse</code>.</p> <p>You are not going to use <code>python-suitesparse</code> directly though! You will use the Python installed in <code>python-suitesparse</code> to create a venv where you will install <code>sparseqr</code>.</p> <p>Create a venv with the Python of <code>python-suitesparse</code> like this: <pre><code>$ micromamba run --name python-suitesparse python -m venv ~/environments/PySPQR\n</code></pre> This will create a venv in <code>~/environments/PySPQR</code>, and since it is created with the python of the <code>python-suitesparse</code> Conda environment, SuiteSparse is now available inside the environment.</p> <p>Activate your new environment, and install <code>sparseqr</code> with any method you prefer, I have chosen to use the PyPI repo: <pre><code>$ source ~/environments/PySPQR/bin/activate\n(PySPQR) $ pip install --upgrade setuptools sparseqr\n(PySPQR) $ python\n&gt;&gt;&gt; import sparseqr\n</code></pre> Note that <code>setuptools</code> is also required and must be installed manually in a venv since PySPQR is a source package.</p> <p>N.B. When installing source packages from PyPI ensure that <code>setuptools</code> is available in your environment. The <code>setuptools</code> library is used to package Python projects, and it is required by source packages to build the source code.</p> <p>After the first Python <code>import</code> command, your package should compile successfully. After that, the package should be available for import without the need to compile again.</p> <p>For future use activate the <code>venv</code> environment with <pre><code>$ source ~/environments/PySPQR/bin/activate\n</code></pre> and deactivate with <pre><code>(PySPQR) $ deactivate\n</code></pre></p>"},{"location":"environment/best-practises/#managing-packages-in-r","title":"Managing packages in R","text":"<p>The R program has a built-in package manager. Assuming that you have access to an installation of R, the R system contains 2 utilities that allow the installation of packages in 3 different modes. First, there is the built-in package manager that can instal packages</p> <ul> <li>in system wide accessible locations for packages that should be available to all users (requires elevated privileges), or</li> <li>in user specific location for packages that are accessible to the current user only.</li> </ul> <p>There are default locations where the built-in package manager searches for packages. The user specific locations take precedence over system locations. The package manager search path can be extended by the user to include bespoke locations. There is also the Packrat package manager which installs packages</p> <ul> <li>in project directories, with the packages being available in an environment isolated within the project directory.</li> </ul> <p>The Packrat package manager is available as an R package. When creating an environment within a project directory, the environment is activated automatically when starting R in the project directory (but not in its subdirectories due to the implementation of Packrat).</p> <p>In your local system you can install packages in any mode. In the HPC systems, you can only install packages in the user accessible location, so you are limited to user and project wide installations. Nevertheless, the HPC installation of R includes a number of commonly used packages, such as <code>dbplyr</code> and <code>tidyverse</code>. You should check if the package you require is installed and that the installed version provides the functionality you need before installing any packages locally. Remember, local package installations consume space and inodes against personal or project quota.</p>"},{"location":"environment/best-practises/#installing-r-packages-locally-and-globally","title":"Installing R packages locally and globally","text":"<p>Be default R installs packages system wide. When R detects that it does not have write access to the system directories it suggests installing packages for the current user only.</p> <p>Start and interactive session and then load the R module and start R: <pre><code>$ module load lang/R\n$ R\n</code></pre> You can list the directories where R is installing and looking for new packages using the function <code>.libPaths()</code> <pre><code>&gt; .libPaths()\n[1] \"/mnt/irisgpfs/apps/resif/aion/2020b/epyc/software/R/4.0.5-foss-2020b/lib64/R/library\"\n</code></pre> If you haven't installed any libraries, only the system path appears in the path where R is looking for libraries. Now, try installing for instance the Packrat package globally with the <code>install.packages</code> command. <pre><code>&gt; install.packages(c(\"packrat\"))\nWarning in install.packages(c(\"packrat\")) :\n  'lib = \"/mnt/irisgpfs/apps/resif/aion/2020b/epyc/software/R/4.0.5-foss-2020b/lib64/R/library\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel) yes\nWould you like to create a personal library\n\u2018~/R/x86_64-pc-linux-gnu-library/4.0\u2019\nto install packages into? (yes/No/cancel) yes\n--- Please select a CRAN mirror for use in this session ---\nSecure CRAN mirrors\n</code></pre> Select any mirror apart from <code>1: 0-Cloud [https]</code>; usually mirrors closer to your physical location will provide better bandwidth. After selecting a mirror the download and installation of the package proceeds automatically.</p> <p>Note that after failing to install the package in the system directory, R creates an installation directory for the user in their home directory <code>~/R/x86_64-pc-linux-gnu-library/4.0</code> and installs the package for the user only. After the installation, you can check the path where R is looking for packages again. <pre><code>&gt; .libPaths()\n[1] \"/mnt/irisgpfs/users/&lt;user name&gt;/R/x86_64-pc-linux-gnu-library/4.0\"\n[2] \"/mnt/irisgpfs/apps/resif/aion/2020b/epyc/software/R/4.0.5-foss-2020b/lib64/R/library\"\n</code></pre> Now R will look for packages in the user directory first (<code>/mnt/irisgpfs</code> is another path for <code>/home</code> that appears in the <code>${HOME}</code> variable). Note by the naming convention that R uses when it creates the directory for installing user packages, you can have multiple minor versions of R installed, and their packages will not interfere with each other. For instance,</p> <ul> <li>R version 4.0.5 installs packages in <code>~/R/x86_64-pc-linux-gnu-library/4.0</code>, and</li> <li>R version 4.3.2 installs packages in <code>~/R/x86_64-pc-linux-gnu-library/4.3</code>.</li> </ul> <p>Some useful commands for managing packages are,</p> <ul> <li><code>installed.packages()</code> to list installed packages and various information regarding each package installation,</li> <li><code>old.packages()</code> to list outdated packages,</li> <li><code>update.packages()</code> to update installed packages, and</li> <li><code>remove.packages(c(\"packrat\"))</code> to remove packages.</li> </ul> <p>To list the loaded packages, use the command <pre><code>search()\n</code></pre> and to get a detailed description of the environment, use the command <pre><code>sessionInfo()\n</code></pre> which provides information about the version of R, the OS, and loaded packages.</p> <p>To load a library that has been installed use the command <code>library</code>. For instance, <pre><code>library(packrat)\n</code></pre> where you cam notice that the use of quotes is optional and only a single can be loaded at a time. The <code>library</code> function causes an error when the loading of a package fails, so R provides the function <code>require</code> which returns the status of the package loading operation in a return variable, and is design for use inside R functions.</p> <p>Useful resources</p> <ul> <li>R Packages: A Beginner's Tutorial</li> <li>Efficient R programming: Efficient set-up</li> </ul>"},{"location":"environment/best-practises/#configuring-installation-paths-in-r","title":"Configuring installation paths in R","text":"<p>So far we have only used the default installation paths of R. However, in a local installation where the user has rights to install in the system directories (e.g. in a Conda environment with R) the user installation directory is not created automatically. Open an R session in an interactive session in the HPC cluster or in your personal machine. To get the location where user packages are installed call <pre><code>&gt; Sys.getenv(\"R_LIBS_USER\")\n[1] \"/home/&lt;user name&gt;/R/x86_64-conda-linux-gnu-library/4.3\"\n</code></pre> which will print an environment variable, <code>R_LIBS_USER</code>, which is set by R and stores the default location for storing user packages. If you create the directory with <pre><code>$ mkdir -p /home/&lt;user name&gt;/R/x86_64-conda-linux-gnu-library/4.3\n</code></pre> then you can print the locations where R is searching for packages (after reloading R), and the default location should appear first in the list. For instance for o Conda installation of R using the Micromamba package manager, the paths printed are <pre><code>&gt; .libPaths()\n[1] \"/home/&lt;user name&gt;/R/x86_64-conda-linux-gnu-library/4.3\"\n[2] \"/home/&lt;user name&gt;/micromamba/envs/R/lib/R/library\"\n</code></pre> where R is installed in a Conda environment named <code>R</code> in the second entry of the search path.</p> <p>There are now multiple locations where packages are stored. The location used by default is the first in the list. Thus, after creating the default location for user installed packages, packages are installed by default in user wide mode. For instance, installing the Packrat package, <pre><code>&gt; install.packages(c(\"packrat\"))\n</code></pre> listing the user installation directory <pre><code>$ ls \"/home/&lt;user name&gt;/R/x86_64-conda-linux-gnu-library/4.3\"\npackrat\n</code></pre> will show the directory with the installed Packrat package files. To install the package in a system wide installation, use the <code>lib</code>flag <pre><code>&gt; install.packages(c(\"packrat\"), lib=\"/home/&lt;user name&gt;/micromamba/envs/R/lib/R/library\")\n</code></pre> to specify the installation location. During loading, all directories in the path are searched consecutively until the package is located.</p> <p>The package installation paths can also be used to maintain multiple independent environments in R. For instance, you can maintain a personal environment and project environment for your research group. Lets consider the case where you want the create an environment in a project directory. First, create a directory for the R environment <pre><code>$ mkdir -p \"${PROJECTHOME}&lt;project name&gt;/R-environment\"\n</code></pre> where the variable <code>PROJECTHOME</code> is defined in the UL HPC system environment to point to the home of the project directories (and includes a trailing slash '/'). To install a package in the project environment, call the installation function with the appropriate <code>lib</code> argument <pre><code>&gt; install.packages( c(\"packrat\"), lib=paste0( Sys.getenv(\"PROJECTHOME\"), \"&lt;project name&gt;/\", \"R-environment\" ) )\n</code></pre> and follow the typical instructions. To load the package, you now must also specify the location of the library, <pre><code>&gt; library( packrat, lib.loc=paste0( Sys.getenv(\"PROJECTHOME\"), \"&lt;project name&gt;/\", \"R-environment\" ) )\n</code></pre> similar to the installation. Environment options can be used to extent the library paths and avoid having to specify the library path in each command.</p> <p>A startup file mechanism is provided by R to set up user and project wide environment options. There are 2 kinds of file,</p> <ul> <li><code>.Renviron</code> files used to set-up environment variables for R, and</li> <li><code>.Rprofile</code> files used to run any R code during initialization.</li> </ul> <p>Note that <code>.Renviron</code> files are simply a list of <pre><code>key=value\n</code></pre> assignment pairs which are read by R, not proper bash code (adding an <code>export</code> modifier is a syntax error). There are 2 locations where startup files appear,</p> <ul> <li>the home directory, <code>~/.Renviron</code> and <code>~/.Rprofile</code>, for user wide settings, and</li> <li>project directories for project wide settings.</li> </ul> <p>The definitions in project <code>.Rprofile</code> files override the user wide definitions in <code>~/.Rprofile</code>. The definitions in <code>.Renviron</code> files supersede the definitions in <code>~/.Renviron</code>, that is if the project has an environment file, the user wide definitions are ignored. Note that R is designed to source setup files at the directory where R starts, and any setup files in parent or descendent directories are ignored.</p> <p>Both the profile and environment startup files can setup a user wide environment. For instance, to use an environment setup in the project directories of the UL HPC systems add in the user wide environment setup file, <code>~/.Renviron</code>, the entry <pre><code>R_LIBS=${PROJECTHOME}&lt;project name&gt;/R-environment\n</code></pre> and then reload R. The new library path is <pre><code>&gt; .libPaths()\n[1] \"/mnt/irisgpfs/projects/&lt;project name&gt;/R-environment\"\n[2] \"/mnt/irisgpfs/users/&lt;user name&gt;/R/x86_64-pc-linux-gnu-library/4.0\"\n[3] \"/mnt/irisgpfs/apps/resif/iris-rhel8/2020b/broadwell/software/R/4.0.5-foss-2020b/lib64/R/library\"\n</code></pre> assuming that all directories appearing in the path exist. Note that the setup file options precede any default options.</p> <p>We can also use startup files to setup project wide libraries. For instance, assume that we are working on a project in a directory named <code>project</code> and the R packages are stored in a subdirectory <code>R-environment</code>. We use a project profile, to still be able to use any library paths defined in the user wide environment file. Add in a file <code>project/.Rprofile</code> the following definitions, <pre><code>project_path &lt;- paste0( getwd(), \"/R-environment\" )\nnewpaths &lt;- c( project_path, .libPaths() )\n.libPaths( newpaths )\n</code></pre> and then start R in the <code>project</code> directory. The new library path is <pre><code>&gt; .libPaths()\n[1] \"/mnt/irisgpfs/users/&lt;user name&gt;/Documents/project/R-environment\"\n[2] \"/mnt/irisgpfs/projects/&lt;project name&gt;/R-environment\"\n[3] \"/mnt/irisgpfs/users/&lt;user name&gt;/R/x86_64-pc-linux-gnu-library/4.0\"\n[4] \"/mnt/irisgpfs/apps/resif/iris-rhel8/2020b/broadwell/software/R/4.0.5-foss-2020b/lib64/R/library\"\n</code></pre> were the local project settings override the user and system wide settings. This is effectively a local project environment.</p>"},{"location":"environment/best-practises/#installing-packages-in-r-project-directories-with-packrat","title":"Installing packages in R project directories with Packrat","text":"<p>The Packrat library is used to automate the creation and management of project based environments. Packrat also automates operations such as tracking the version of the packages installed in the environment with snapshots, and saving the snapshot information in a text file that can be version controlled. The R distribution available through the UL HPC modules has a fairly old version of Packrat, which nevertheless supports all the basic features. Packrat is a light package, so you can install a more modern version in a user wide mode or in some environment accessible to all the users of a UL HPC project.</p> <p>To initialize the project, for instance in the directory <code>~/Documents/project</code>, use the commands: <pre><code>library(packrat)\npackrat::init(\"~/Document/project\")\n</code></pre> The initialization command creates, - a directory <code>~/Document/project/packrat</code> to store the packages, and - a setup script <code>~/Document/project/.Rprofile</code> to initialize the project. Therefore, start R within the project directory <code>~/Document/packrat</code>, to activate the project environment. After initializing the project or whenever you start R in the project directory, the <code>packrat</code> directory and its subdirectories will be the only ones appearing in the library paths: <pre><code>&gt; .libPaths()\n[1] \"/mnt/irisgpfs/users/&lt;user name&gt;/Documents/project/packrat/lib/x86_64-pc-linux-gnu/4.0.5\"    \n[2] \"/mnt/irisgpfs/users/&lt;user name&gt;/Documents/project/packrat/lib-ext/x86_64-pc-linux-gnu/4.0.5\"\n[3] \"/mnt/irisgpfs/users/&lt;user name&gt;/Documents/project/packrat/lib-R/x86_64-pc-linux-gnu/4.0.5\"\n</code></pre> Execute all package operations as usual. For instance, to install the <code>plyr</code> package, use the command: <pre><code>&gt; install.packages(c(\"plyr\"))\n</code></pre> All packages are stored in the <code>packrat</code> subdirectory of the project.</p> <p>Packrat stores the status of the project in the file <code>packrat/packrat.lock</code>. This file stores the precise package versions that were used to satisfy dependencies, including dependencies of dependencies, and should not be edited by hand. After any change in the installed packages run the command <pre><code>packrat::snapshot()\n</code></pre> to update the file. You can use the command <pre><code>packrat::status()\n</code></pre> to analyze the code in the project directory and get a report regarding the status of extraneous or missing packages. After running the <code>status</code> command, you can run <pre><code>packrat::clean()\n</code></pre> to remove any unused packages. Finally, after restoring the <code>packrat/packrat.lock</code> file from a version control system, or if <code>status</code> detects a missing package, use the command <pre><code>packrat::restore()\n</code></pre> to install any missing packages.</p> <p>Useful resources</p> <ul> <li>Official Packrat tutorial</li> </ul>"},{"location":"environment/best-practises/#issues-with-managing-packages-with-the-native-r-package-managers","title":"Issues with managing packages with the native R package managers","text":"<p>The native package manager of R is quite potent, and there are packages such as Packrat that further extend its capabilities. However, there are some drawbacks in installing packages with the native tools. Consider for instance installing the <code>hdf5r</code> package, a package used to read and write binary files, that is quite popular in HPC engineering applications. The installation mode is not important for our demonstration purposes, but assume that you are performing a user wide installation. <pre><code>&gt; install.packages(c(\"hdf5r\"))\n</code></pre></p> <p>During the installation, you can see that R is compiling the package components. This can be advantageous is the compilation process is tailored to optimize the build for the underlying system configuration. If you use the module available in the UL HPC systems, it is configured to use the main components of the FOSS tool chain (you can see that by calling <code>module list</code> after loading R), so the compiled packages are well optimized.</p> <p>N.B. If you encounter any issues with missing packages load the whole FOSS tool chain module with the command, <pre><code>module load toolchain/foss\n</code></pre> as there are a few popular packages missing in the dependencies of R.</p> <p>However, if you want to avoid compiling packages from source, which can be quite time consuming, you can use binary distributions of R. These include the distributions provided though native package managers in various Linux distributions, like APT and YUM, as well as Conda package managers like Mamba.</p>"},{"location":"environment/conda/","title":"An introduction to Conda and environment management","text":"<p>Copyright \u00a9 2023-2024 UL HPC Team hpc-sysadmins@uni.lu Author: Georgios Kafanas</p> <p>Users of a programming language like Python, or any other software tool like R, quite often have to install various packages and tools to perform various operations. The various Linux/GNU distributions offer many packages, however, many package versions are either old or missing completely as administrators try to reduce the size of distributions and the associated maintenance.</p> <p>Software distribution systems that specialize on various categories of application software have been developed. Systems such as Conda distribute generic types of software, where as systems such as PyPI (Python), Packrat (R), and Pkg (Julia) specialize in a single kind of software. All software distribution system however offer a uniform functionality that includes</p> <ul> <li>the ability to create and reproduce software environments,</li> <li>isolation between environments, and between an environment and the system, and</li> <li>easy sourcing of packages from a variety of package sources.</li> </ul> <p>The objective of this course is to cover the basics of package management with Conda. In particular, after this course the users will be able to</p> <ul> <li>create Conda environment and use them to manage the software and package dependencies,</li> <li>document, and reproduce any Conda environment in variety of systems,</li> <li>install packages from multiple sources, and</li> <li>decide when Conda is the most appropriate tool to manage a system environment.</li> </ul>"},{"location":"environment/conda/#pre-requisites","title":"Pre-requisites","text":"<p>This course focuses on generic aspects of package management. It is assumed that you have some basic knowledge of how to use packages in R or Python. The main package management framework used is Conda, although there will be mentions to some native tools. You can use the techniques covered here both in your personal machine and on HPC clusters.</p>"},{"location":"environment/conda/#a-brief-introduction-to-conda","title":"A brief introduction to Conda","text":"<p>You must be familiar with a few concepts to start working with Conda. In brief, these concepts are package managers which are the programs used to create and manage environments, channels which are the repositories that contain the packages from which environments are composed, and distributions which are systems for shipping package managers.</p>"},{"location":"environment/conda/#package-managers","title":"Package managers","text":"<p>Package managers are the programs that install and manage the Conda environments. There are multiple package managers, such as <code>conda</code>, <code>mamba</code>, and <code>micromamba</code>.</p>"},{"location":"environment/conda/#channels","title":"Channels","text":"<p>Conda channels are the locations where packages are stored and from where they can be downloaded and installed. There are also multiple Conda channels, with some important channels being:</p> <ul> <li><code>defaults</code>, the default channel,</li> <li><code>anaconda</code>, a mirror of the default channel,</li> <li><code>bioconda</code>, a distribution of bioinformatics software, and</li> <li><code>conda-forge</code>, a community-led collection of recipes, build infrastructure, and distributions for the conda package manager.</li> </ul> <p>The most useful channel that comes pre-installed in all distributions, is Conda-Forge. Channels are usually hosted in the official Anaconda page, but in some rare occasions custom channels may be used. For instance the default channel is hosted independently from the official Anaconda page. Many channels also maintain web pages with documentation both for their usage and for packages they distribute:</p> <ul> <li>Default Conda channel</li> <li>Bioconda</li> <li>Conda-Forge</li> </ul>"},{"location":"environment/conda/#distributions","title":"Distributions","text":"<p>Quite often, the package manager is not distributed on its own, but with a set of packages that are required for the package manager to work, or even with some additional packages that required for most applications. For instance, the <code>conda</code> package manager is distributed with the Miniconda and Anaconda distributions. Miniconda contains the bare minimum packages for the <code>conda</code> package manager to work, and Anaconda contains multiple commonly used packages and a graphical user interface. The relation between these distributions and the package manager is depicted in the following diagram.</p> <p></p> <p>The situation is similar for Mamba distributions. Mamba distributions are supported by Conda-Forge, and their default installation options set-up <code>conda-forge</code> as the default and only channel during installation. The <code>defaults</code> or its mirror <code>anaconda</code> must be explicitly added if required. The distribution using the Mamba package manager was originally distributed as Mambaforge and was recently renamed to Miniforge. Miniforge comes with a minimal set of python packages required by the Mamba package manager. The distribution using the Micromamba package manager ships no accompanying packages, as Micromamba is a standalone executable with no dependencies. Micromamba is using <code>libmamba</code>, a C++ library implementing the Conda API.</p>"},{"location":"environment/conda/#the-micromamba-package-manager","title":"The Micromamba package manager","text":"<p>The Micromaba package manager is a minimal yet fairly complete implementation of the Conda interface in C++, which is shipped as a standalone executable. The package manager operates strictly on the user-space and thus it requires no special permissions to install packages. It maintains all its files in a couple of places, so uninstalling the package manager itself is also easy. Finally, the package manager is also lightweight and fast.</p>"},{"location":"environment/conda/#installation","title":"Installation","text":"<p>A complete guide regarding Micromamba installation can be found in the official documentation. To install micromamaba in the HPC clusters, log in to Aion or Iris. Working on a login node, run the installation script, <pre><code>\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\n</code></pre> which will install the executable and setup the environment. There are 4 options to select during the installation of Micromamba:</p> <ul> <li>The directory for the installation of the binary file:   <pre><code>Micromamba binary folder? [~/.local/bin]\n</code></pre>   Leave empty and press enter to select the default displayed within brackets. Your <code>.bashrc</code> script should include <code>~/.local/bin</code> in the <code>$PATH</code> by default.</li> <li>The option to add to the environment autocomplete options for <code>micromamba</code>:   <pre><code>Init shell (bash)? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will append a clearly marked section in the <code>.bashrc</code> shell. Do not forget to remove this section when uninstalling Micromamba.</li> <li>The option to configure the channels by adding conda-forge:   <pre><code>Configure conda-forge? [Y/n]\n</code></pre>   Press enter to select the default option <code>Y</code>. This will setup the <code>~/.condarc</code> file with <code>conda-forge</code> as the default channel. Note that Mamba and Micromamba will not use the <code>defaults</code> channel if it is not present in <code>~/.condarc</code> like <code>conda</code>.</li> <li>The option to select the directory where environment information and packages will be stored:   <pre><code>Prefix location? [~/micromamba]\n</code></pre>   Press enter to select the default option displayed within brackets.</li> </ul> <p>To activate the new environment log-out and log-in again. You now can use <code>micromamba</code> in the login and compute nodes, including the auto-completion feature.</p>"},{"location":"environment/conda/#managing-environments","title":"Managing environments","text":"<p>The user interface of Conda is based around the concept of the environment. Environments are effectively system configurations that can be activated and deactivated by the Conda manager, and provide access to a set of packages that are installed in the environment. In the following examples we use the Micromamba package manager, but any other Conda package manager operates with the same commands, as package managers for Conda have the same interface.</p> <p>Environments are created with the command <pre><code>$ micromamba create --name &lt;environment name&gt;\n</code></pre> The environment is then activated with the command <pre><code>$ micromamba activate &lt;environment name&gt;\n</code></pre> anywhere in the file system. To install packages, first ensure that the target environment is active, and then install any required package with the command: <pre><code>$ micromamba install &lt;package name&gt;\n</code></pre> You can specify multiple packages at ones. Quite often, the channels where Conda should look for the package must also be specified. Using the syntax <pre><code>$ micromamba install --chanell &lt;chanell 1&gt; --channels &lt;chanell 2&gt; &lt;package name&gt;\n</code></pre> channels are listed in a series of <code>--channel &lt;channel name&gt;</code> entries and the channels are searched in the order they appear. Using the syntax <pre><code>$ micromamba install &lt;chanell&gt;::&lt;package name&gt;\n</code></pre> packages are searched in the specified channel only. Available packages can be found by searching the Anaconda search index or channel specific search indices, such as conda-forge.</p> <p>Specifying package channels</p> <p>The Anaconda index provides instructions for the installation of each package. Quite often the channel is specified in the installation instructions, with options such as <code>conda-forge::&lt;package name&gt;</code> or even <code>-c conda-forge</code> or <code>--channel conda-forge</code>. While the Micromamba installer sets-up <code>conda-forge</code> as the default channel, latter modification in <code>~/.condarc</code> may change the channel priority. It is thus a good practice to explicitly specify the source channel when installing a package.</p> <p>After work in an environment is complete, deactivate the environment, <pre><code>$ micromamba deactivate\n</code></pre> to ensure that it does not interfere with any other operations. In contrast to modules, Conda is designed to operate with a single environment active at a time. Create one environment for each project, and Conda will ensure that any package that is shared between multiple environments is installed once.</p> <p>Micromamba supports almost all the subcommands of Conda. For more details see the official documentation.</p> Creating an environment for R <p>Assume for instance that we want to install R. Create an environment call <code>R-project</code> and activate it: <pre><code>micromamba create --name R-project\nmicromamba activate R-project\n</code></pre> The basic functionality of the R software environment is contained in the <code>r-base</code> package. Calling <pre><code>micromamba install --channel conda-forge r-base\n</code></pre> or <pre><code>micromamba install conda-forge::r-base\n</code></pre> will install all the components required to run standalone R scripts. More involved scripts need functionality defined in various R packages. The R packages are prepended with a prefix 'r-' in the conda-forge channel. Thus, <code>plm</code> becomes <code>r-plm</code> and so on. After all the required packages have been installed, the environment is ready for use.</p>"},{"location":"environment/conda/#using-environments-in-submission-scripts","title":"Using environments in submission scripts","text":"<p>In HPC clusters, all computationally heavy operations must be performed in compute nodes. Thus Conda environments are also used in jobs submitted to the queuing system. You can activate and deactivate environments in various sections of your script.</p> <p>Environment activations in Conda are stacked, and unlike modules, only one environment is active at a time with the rest being pushed down the stack. Assume that we are working with 2 environments, <code>R-project</code> and <code>python-project</code>, and consider the following script layout. <pre><code># Initialization code\n\nmicromabma activate python-project\n\n# Code to run a simulation and generate output with Python\n\nmicromabma activate R-project\n\n# Code to perform statistical analysis and ploting with R\n\nmicromamba deactivate\n\n# Code to save data with Python\n</code></pre> Such a script creates the following environment stack. <pre><code>(base)\n|\n| # No software is available here\n|\n+-(python-project) # micromabma activate python-project\n| |\n| | # Only Python is available here\n| |\n| +-(R-project) # micromabma activate R-project\n| | |\n| | | # Only R is available here\n| | |\n| +-+ # micromamba deactivate\n| |\n| | # Only Python is available here\n| |\n</code></pre></p> <p>We can see that the Python environment (<code>python-project</code>) is pushed down the stack when the R environment (<code>R-project</code>) is activated, and will be brought forth as soon as the R environment is deactivated.</p> Example SLURM submission script <p>Consider for instance a script running a single core job for R. The R script for the job is run inside an environment named <code>R-project</code>. A typical submission script is the following: <pre><code>#SBATCH --job-name R-job\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --cpus-per-task 1\n#SBATCH --time=0-02:00:00\n#SBATCH --partition batch\n#SBATCH --qos normal\n\nmicromamba activate R-project\n\necho \"Launched at $(date)\"\necho \"Job ID: ${SLURM_JOBID}\"\necho \"Node list: ${SLURM_NODELIST}\"\necho \"Submit dir.: ${SLURM_SUBMIT_DIR}\"\necho \"Numb. of cores: ${SLURM_CPUS_PER_TASK}\"\n\nexport SRUN_CPUS_PER_TASK=\"${SLURM_CPUS_PER_TASK}\"\nexport OMP_NUM_THREADS=1\nsrun Rscript --no-save --no-restore script.R\n\nmicromamba deactivate\n</code></pre></p> <p>The <code>micromamba deactivate</code> command at the end of the script is optional, but it functions as a reminder that a Conda environment is active if you expand the script at a later date.</p> <p>Useful scripting resources</p> <ul> <li>Formatting submission scripts for R (and other systems)</li> </ul>"},{"location":"environment/conda/#exporting-and-importing-environment-specifications","title":"Exporting and importing environment specifications","text":"<p>An important feature of Conda is that it allows you to export and version control you environment specifications, and recreate the environment on demand.</p> <ul> <li>A description of the software installed in the Conda environment can be exported on demand to a text file.</li> <li>The specification file can then be used to populate a new environment, in effect recreating the environment.</li> </ul> <p>The environment reproducibility is particularly important when you want to have reproducible results, like for instance in a scientific simulation. You can setup and test your application in your local machine, save the environment, and later load the environment in an HPC system, and be sure that the application will behave identically. Conda in the background will ensure that identical packages will be installed.</p> <p>In Micromaba, you can export the specifications of an environment using the command: <pre><code>$ micromaba env export --name &lt;environment name&gt;\n</code></pre> By default the command prints to the standard output, but you can redirect the output to a file: <pre><code>$ micromaba env export --name &lt;environment name&gt; &gt; &lt;environment name&gt;.yml\n</code></pre> To recreate an environment from a specification file, pass the file as argument to the create command with the <code>--file</code> flag: <pre><code>$ micromamba create --name &lt;environment name&gt; --file &lt;environment name&gt;.yml\n</code></pre> This workflow demonstrates the use of simple YAML text files to store specifications, but Micormamba supports various specification file types. All specification files are text files and can be version controlled with a tool such as Git.</p> <p>Sources</p> <ul> <li>Micromamba User Guide: Specification files</li> </ul>"},{"location":"environment/conda/#example-installing-jupyter-and-managing-the-dependencies-of-a-notebook-with-micromamba","title":"Example: Installing Jupyter and managing the dependencies of a notebook with Micromamba","text":"<p>In this example we will create an environment, install Jupyter, and install all the dependencies for our notebooks with Micromamba. Start by creating an environment: <pre><code>micromamba env create --name jupyter\n</code></pre> Next, install Jupyter in the environment. Have a look at the page for <code>jupyterlab</code> in the conda-forge channel. To install it in your environment call: <pre><code>micromamba install --name jupyter conda-forge::jupyterlab\n</code></pre> Now activate the environment, create a working directory for your notebooks, and launch Jypyter: <pre><code>micromamba activate jupyter\nmkdir ~/Documents/notebooks &amp;&amp; cd ~/Documents/notebooks\njupyter lab\n</code></pre> If a webpage appears with the Jupyter lab, the installation worked succeeded!</p> <p>You may need some Python package in your Jupyter notebook. You can make packages available in your notebook by installing the appropriate package in the Conda environment. For instance, assume that you need <code>pandas</code> and <code>numpy</code>. Searching the conda-forge channel, we can find the package name and installation instruction. With the <code>jupyter</code> environment active, run the command: <pre><code>micromamba install conda-forge::numpy conda-forge::pandas\n</code></pre> You should now be able to import <code>numpy</code> and <code>pandas</code> in your notebook!</p> <p>After completing your work, close down the notebook with the command <code>C-c</code>, and deactivate the <code>jupyter</code> Conda environment: <pre><code>micromamba deactive\n</code></pre> You should now be in your normal operating system environment.</p>"},{"location":"environment/conda/#self-management-of-work-environments-in-hpc-systems-with-conda","title":"Self management of work environments in HPC systems with Conda","text":"<p>Conda is one of the systems for providing software in HPC systems, along with modules and containers. When starting a new project it is important to select the appropriate system. Before installing any software yourself in user space you should check if the HPC system provides the software though any method. System wide installations do not consume any of your storage quota, and are often configured and tested to provide optimal efficiency.</p>"},{"location":"environment/conda/#when-a-conda-environment-is-useful","title":"When a Conda environment is useful","text":"<p>There are three aspects of environment management that you should consider when selecting the method with which you will manage your software.</p> <ul> <li> <p>Ease of use: Many software systems whose performance is not critical and are used by relatively few users are not provided though the standard distribution channels of modules or containers. In such cases the easiest installation option is a user side installation with Conda or some similar package management system.</p> </li> <li> <p>Reproducibility: Conda and containers can both create reproducible environments, with descriptions of the environment exported and version controlled in text files. However, containers require significant amount of manual configuration to create a reproducible environment and to perform well in a wide range of systems. If your aim is an easily reproducible environment Conda is the superior choice.</p> </li> <li> <p>Performance: Conda provides precompiled executables. Even thought multiple configurations are supported, you will not always find an executable tailored to your target system. Modules and containers provided by UPC systems are optimized to ensure performance and stability, so prefer them.</p> </li> </ul>"},{"location":"environment/conda/#storage-limitations-in-hpc-systems","title":"Storage limitations in HPC systems","text":"<p>Regardless of installation method, when you install software in user space you are using up your storage quota. Conda environment managers download and store a sizable amount of data and a large number of files to provided packages to the various environments. Even though the package data are shared between the various environments, they still consume space in your or your project's account. There are limits in the storage space and number of files that are available to projects and users in a cluster. Since Conda packages are self managed, you need to clean unused data yourself.</p>"},{"location":"environment/conda/#updating-the-package-manager-and-the-environment","title":"Updating the package manager and the environment","text":"<p>The Micromamba package manager is under active development, so updates provide the latest features and any bug fixes. To update the micromamba package manager itself, simply issue the command: <pre><code>micromamba self-update\n</code></pre> If a new version is available this commands will download the new executable and replace the old one.</p> <p>Package versions can be pinned with Conda during installation to provide an immutable environment. However, in many cases, like during development, it make sense to experiment and update package versions. To update a specific package inside an environment, use the command: <pre><code>micromamba update --name &lt;environment name&gt; &lt;package name&gt;\n</code></pre> You can specify more that one packages. To update all the packages in an environment at once, use the command: <pre><code>micromamba update --name &lt;environment name&gt; --all\n</code></pre></p>"},{"location":"environment/conda/#cleaning-up-package-data","title":"Cleaning up package data","text":"<p>There are two main sources of unused data, compressed archives of packages that Conda stores in its cache when downloading a new package, and data of packages no longer used in any environment. All unused data in Micromoamba can be removed with the command <pre><code>micromamba clean --all --yes\n</code></pre> where the flag <code>--yes</code> suppresses an interactive dialogue with details about the operations performed. In general you can use the default options with <code>--yes</code>, unless you have manually edited any files in you package data directory (default location <code>~/micromamba</code>) and you would like to preserve your changes.</p> <p>Updating environments to remove old package versions</p> <p>As we create new environments, environments often install the latest version of each package. However, if the environments are not updated regularly, we may end up with different versions of the same package across multiple environments. If we have the same version of a package installed in all environments, we can save space by removing unused older versions.</p> <p>To update a package across all environments, use the command <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --yes --name $e &lt;package name&gt;; done\n</code></pre> and to update all packages across all environments <pre><code>for e in $(micromamba env list | awk 'FNR&gt;2 {print $1}'); do micromamba update --yes --name $e --all; done\n</code></pre> where <code>FNR&gt;2</code> removes the headers in the output of <code>micromamba env list</code>, and is thus sensitive to changes in the user interface of Micromamba.</p> <p>After updating packages, the <code>clean</code> command can be called to removed the data of unused older package versions.</p> <p>Sources</p> <ul> <li>Oficial Conda <code>clean</code> documentation</li> <li>Understanding Conda <code>clean</code></li> </ul>"},{"location":"environment/conda/#a-note-about-internal-workings-of-conda","title":"A note about internal workings of Conda","text":"<p>In general, Conda packages are stored in a central directory, and hard links are created in the library directories of any environment that requires the package. Since hard links do not consume space and inodes, Conda is very efficient in its usage of storage space.</p> <p>Consider for instance the MPFR package used in some environment <code>gaussian_regression</code>. Looking into the Conda installation managed by Micromamba, these are the installed library files: <pre><code>gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi pkgs/mpfr-4.2.1-h9458935_0/lib/\ntotal 1.3M\n5286432 drwxr-xr-x 1 gkaf gkaf   94 Oct 25 13:59 ./\n5286426 drwxr-xr-x 1 gkaf gkaf   38 Oct 25 13:59 ../\n5286436 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so -&gt; libmpfr.so.6.2.1*\n5286441 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so.6 -&gt; libmpfr.so.6.2.1*\n5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 libmpfr.so.6.2.1*\n5286442 drwxr-xr-x 1 gkaf gkaf   14 Oct 25 13:59 pkgconfig/\n</code></pre> Looking into the libraries of the <code>gaussian_regression</code> environment, there is a hard link to the MPFR library: <pre><code>gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi envs/gaussian_regression/lib/libmpfr.so.6.2.1 \n5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 envs/gaussian_regression/lib/libmpfr.so.6.2.1*\n</code></pre> You can use the <code>-i</code> flag in <code>ls</code> to print the inode number of a file. Hard links have the same inode number, meaning that they are essentially the same file.</p> <p>Conda will not automatically check if the files in the <code>pkgs</code> directories must be removed. For instance, when you uninstall a package from an environment, when you delete an environment, or when a package is updated in an environment, only the hard link in the environment directory will change. The files in <code>pkgs</code> will remain even if they are no longer used in any environment. The relevant <code>clean</code> routines check which packages are actually used and remove the unused files.</p>"},{"location":"julia/","title":"High-performance scientific computing using Julia","text":"<p>Have you already heard about Julia, the high-level, high-performance scientific computing language of the future?</p> <p>This course will teach you the basics of Julia, and how to get started writing your code parallel-ready. You will get a glimpse on how to scale your code in a high-performance computing (HPC) environment.</p>"},{"location":"julia/#learning-outcomes","title":"Learning outcomes","text":"<p>This is an introductory course into Julia programming. Attendees will learn, among others:</p> <ul> <li>use Julia for a wide range of computation-heavy scientific and software engineering tasks</li> <li>basic skills to process data using Julia language</li> <li>basic practices of program efficiency required for accelerating analyses</li> <li>tips on scaling up computational analyses.</li> </ul> <p>The course will give an overview of the main scientific programming libraries in the Julia ecosystem, giving the attendees the ability to quickly utilize the available software for solving their problems, with additional focus on parallelism and HPC utilization.</p>"},{"location":"julia/#agenda-tentative","title":"Agenda (tentative)","text":""},{"location":"julia/#session-i-50-mins-getting-started","title":"Session I (~50 mins): Getting started","text":"<ul> <li>Welcome</li> <li>General introduction to Julia</li> <li>Motivation \u2013 what problems are best solved with Julia</li> <li>Installation of Julia, REPL, managing packages</li> <li>Programming language basics (variables and types, loops, arrays, functions, \u2026)</li> </ul> <p>(10 mins break)</p>"},{"location":"julia/#session-ii-50-mins-data-processing","title":"Session II (~50 mins): Data processing","text":"<ul> <li>I/O and data manipulation</li> <li>Read, write, different data formats</li> <li>Plotting (data visualization)</li> </ul> <p>(10 mins break)</p>"},{"location":"julia/#session-iii-50-mins-high-performance-and-distributed-processing","title":"Session III (~50 mins): High-performance and distributed processing","text":"<ul> <li>Overview of the usual performance bottlenecks</li> <li>Parallelization model of Julia, threads</li> <li>Distributed programming and helper packages</li> <li>GPU usage</li> </ul>"},{"location":"julia/#session-iv-50-mins-reproducibility","title":"Session IV (~50 mins): Reproducibility","text":"<ul> <li>Reprodubcibility with Julia</li> <li>Dependency management</li> <li>Notes on containerization</li> </ul>"},{"location":"julia/#prerequisites","title":"Prerequisites","text":"<p>Detailed instructions on what to prepare will be sent out to confirmed participants.</p> <p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"mpi-python/","title":"Accelerating Python with Multiple Nodes with MPI for Python","text":"<p>The Luxembourg SuperComputing Competence Center will host a half-day online course on accelerating Python using multiple nodes with MPI for Python. This course will introduce you to mpi4py and the core concepts of parallel computing across multiple nodes. You will gain hands-on experience in leveraging High-Performance Computing (HPC) to optimize Python programs, enabling them to scale efficiently across multiple processors and nodes.</p>"},{"location":"mpi-python/#for-whom","title":"For whom?","text":"<p>This workshop is ideal for those looking to enhance their Python-based HPC skills. Participants should have intermediate Python proficiency and a basic understanding of parallel programming (MPI). The session will provide a comprehensive introduction to utilizing HPC to optimize Python code for performance.</p>"},{"location":"mpi-python/#what-will-you-learn-and-how","title":"What will you learn and how?","text":"<p>In this workshop, you will learn how to harness the power of HPC clusters with mpi4py to optimize and accelerate your computational workflows. The course will guide you through configuring a SLURM environment, understanding the basics of mpi4py, implementing efficient data distribution techniques, and exploring practical applications in real-world scenarios.</p>"},{"location":"mpi-python/#learning-outcomes","title":"Learning Outcomes","text":""},{"location":"mpi-python/#after-completing-this-course-participants-will-be-able-to","title":"After completing this course, participants will be able to:","text":"<ul> <li>Configure a SLURM environment for running MPI-based applications.</li> <li>Implement efficient data distribution using mpi4py.</li> <li>Accelerate Python workflows by integrating MPI.</li> </ul>"},{"location":"mpi-python/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of SLURM.</li> <li>A fundamental understanding of Python and NumPy.</li> <li>Intermediate-level knowledge of parallel programming.</li> </ul>"},{"location":"mpi-python/#compute-resources","title":"Compute Resources","text":"<p>Participants attending the training will receive access to the MeluXina supercomputer during the session. For more information about MeluXina, please refer to the MeluXina Overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"mpi-python/#agenda","title":"Agenda","text":"<p>This half-day will be hosted online in Central European Time (CET) on May 7<sup>th</sup>, 2025 (9:00 AM \u2013 12:30 PM CET). </p>"},{"location":"mpi-python/#schedule","title":"Schedule:","text":"<ul> <li>Introduction to mpi4py</li> <li>Configuring scripts to run with multiple MPI tasks</li> <li>Practical use cases and applications</li> </ul>"},{"location":"mpi-python/#important-limited-spots-available-25-participants-max","title":"Important: Limited spots available (25 participants max)!","text":"<p>Contact people for more info: Oscar CASTRO-LOPEZ, oscar.castro@uni.lu </p>"},{"location":"numerical-libraries/","title":"Introduction to numerical libraries","text":"<p>The Luxembourg SuperComputing Competence Center will host a half-day online course on numerical libraries. The course will be divided into two parts: the first will cover theoretical concepts, while the second will focus on practical, hands-on challenges using the MeluXina supercomputer.</p>"},{"location":"numerical-libraries/#who-should-attend","title":"Who should attend","text":"<p>This course is ideal for current and prospective users of large hybrid CPU/GPU clusters and supercomputers who develop application that rely on numerical algorithms such as the vector and matrix operations implemented in BLAS and the basic linear algebra operations implemented in LAPACK.</p>"},{"location":"numerical-libraries/#what-will-you-learn-and-how","title":"What will you learn and how","text":"<p>Participants in this course will learn how to use numerical libraries in C/C++ programs. Calling library function and linking with library object files will be covered first, followed by a detailed investigation of how the effects of caching affect the performance of libraries highly optimized to exploit the caching.</p> <p>A presentation will cover the data structures and the algorithms used in BLAS and LAPACK, and how they are designed to exploit the cache. Accelerated implementations such as cuBLAS and MAGMA will also be presented, including an analysis of data movement to and from the accelerator that tends to determine the performance of accelerated libraries. Then, the participants will have the opportunity to play with demonstrative examples using BLAS and LAPACK in the practical session. Examples with accelerated libraries will also be available utilizing the GPU partition of MeluXina.</p>"},{"location":"numerical-libraries/#learning-outcomes","title":"Learning Outcomes","text":""},{"location":"numerical-libraries/#by-the-end-of-the-course-participants-will-be-able-to","title":"By the end of the course, participants will be able to:","text":"<ul> <li>Understand the design of numerical libraries, including:<ul> <li>Matrix data structures for dense and sparse matrices (CSR, ELL, COO)</li> <li>Algorithmic implementation of linear algebra operations and how it takes advantage of the cache</li> </ul> </li> <li>Compile efficient implementations of numerical libraries, including:<ul> <li>Enabling implementation specific optimizations</li> <li>Inspecting object files to determine which components of the library interface they support</li> <li>Using libraries in compilations with build automation systems such as CMake</li> </ul> </li> <li>Efficiently exploit cache to leverage the best performance, with methods such as:<ul> <li>Cache aware programming</li> <li>Cache alignment</li> </ul> </li> <li>Efficient use libraries optimized for accelerators, including:<ul> <li>Understanding the difference between cache and accelerator-optimized libraries</li> <li>Efficiently manage data movement to and from the accelerator</li> </ul> </li> </ul>"},{"location":"numerical-libraries/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to participants with solid experience in C/C++ and/or FORTRAN. Some knowledge about the steps involved in compilation and linking would be helpful but not necessary.</p>"},{"location":"numerical-libraries/#computing-resources","title":"Computing Resources","text":"<p>Participants will have access to the MeluXina supercomputer CPU and GPU partitions during the training. For more information about MeluXina, please refer to the MeluXina Overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"numerical-libraries/#agenda","title":"Agenda","text":"<p>This half-day course will be conducted online in Central European Time (CET) on February 14<sup>th</sup>, 2025 (9:00 AM \u2013 13:00 PM CET).</p>"},{"location":"numerical-libraries/#schedule","title":"Schedule:","text":"<ul> <li>9:00 AM \u2013 9:50 AM: Lecture Part 1: The data structure and algorithms of BLAS and LAPACK</li> <li>9:50 AM \u2013 10:00 AM: Break</li> <li>10:00 AM \u2013 10:50 AM: Lecture Part 2: Practical aspects of working with libraries</li> <li>10:50 AM \u2013 11:00 AM: Break</li> <li>11:00 AM \u2013 11:50 AM: Lecture Part 3: Effects of cache and accelerated implementations</li> <li>11:50 AM \u2013 12:00 PM: Break</li> <li>12:00 PM \u2013 12:50 PM: Practical session</li> <li>12:50 PM \u2013 13:00 PM: Q&amp;A</li> </ul>"},{"location":"numerical-libraries/#important-limited-spots-are-available","title":"Important: Limited spots are available!","text":"<p>Contact people for more info:  Georgios KAFANAS, georgios.kafanas@uni.lu </p>"},{"location":"openacc/","title":"Introduction to OpenACC for Heterogeneous Computing","text":"<p>Participants from this course will learn GPU programming using the OpenACC programming model, such as compute constructs, loop constructs and data clauses. Furthermore, understanding the GPU architecture and how parallel threads blocks are created and used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the OpenACC programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the OpenACC programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openacc/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openacc/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture </li> <li>Threads blocks </li> </ul> </li> <li>Implement the OpenACC programming model  <ul> <li>Compute constructs  </li> <li>Loop constructs </li> <li>Data clauses</li> </ul> </li> <li>Efficient handling of memory management  <ul> <li>Host to Device </li> <li>Unified memory </li> </ul> </li> <li>Apply the OpenACC programming knowledge to accelerate examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openacc/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No GPU programming knowledge is required; however, knowing the OpenMP programming model is advantageous. </p>"},{"location":"openacc/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openacc/#course-organization-and-registration","title":"Course Organization and Registration","text":"<p>Format - Online  Previous events:  19<sup>th</sup> September, 2023  16<sup>th</sup> June, November, 2024  Net event:  25<sup>th</sup> March 2025 </p> <p>Registration: 25<sup>th</sup>, March, 2025</p>"},{"location":"openacc/exercise-1/","title":"Compute Constructs and Paralleize Loops","text":""},{"location":"openacc/exercise-1/#compute-constructs","title":"Compute Constructs","text":"<p>In this initial exercise, we will explore the process of offloading computational tasks to a device, specifically the GPU (Graphics Processing Unit). The primary objective of OpenACC is to streamline this offloading using its dedicated APIs.</p> <p>OpenACC offers two primary constructs for offloading computations to the GPU, which we will discuss in detail:</p> <ol> <li> <p>Parallel Construct (<code>parallel</code>): This construct allows for the parallelization of computations across multiple processing units. It is a suitable choice for programmers who possess a strong understanding of their code's parallel behavior.</p> </li> <li> <p>Kernels Construct (<code>kernels</code>): This alternative also enables parallelization but provides greater control over the parallel region. It is generally recommended for programmers who may not be as familiar with the intricacies of parallel execution, as the compiler will manage the complexities of safe parallelization within this construct.</p> </li> </ol> <p>Both constructs serve similar purposes in facilitating computation on the GPU; however, the choice between them depends on the programmer's comfort level with parallel programming. If you have significant experience and knowledge of the computations being executed in parallel, you may opt for the <code>parallel</code> construct. In contrast, for those who prefer a more managed approach to ensure safety and correctness in parallel execution, using <code>kernels</code> is advisable.</p> <p>To effectively utilize OpenACC constructs, clauses, and environment variables, it is essential to include the OpenACC library in your code. This inclusion enables access to the full range of OpenACC features and functionalities.</p> <p>OpenACC library</p> C/C++FORTRAN <pre><code>#include&lt;openacc.h&gt;\n</code></pre> <pre><code>use openacc\n</code></pre> <p></p> <p>To create a parallel region in OpenACC, we utilize the following compute constructs:</p> <p>Parallel Constructs</p> C/C++FORTRAN <pre><code>#pragma acc parallel [clause-list] new-line\n   structured block\n</code></pre> <pre><code>!$acc parallel [ clause-list ]\n    structured block\n!$acc end parallel\n</code></pre> Available clauses for parallel C/C++ and FORTRAN <pre><code>    async [ ( int-expr ) ]\n    wait [ ( int-expr-list ) ]\n    num_gangs( int-expr )\n    num_workers( int-expr )\n    vector_length( int-expr )\n    device_type( device-type-list )\n    if( condition )\n    self [ ( condition ) ]\n    reduction( operator : var-list )\n    copy( var-list )\n    copyin( [ readonly: ] var-list )\n    copyout( [ zero: ] var-list )\n    create( [ zero: ] var-list )\n    no_create( var-list )\n    present( var-list )\n    deviceptr( var-list )\n    attach( var-list )\n    private( var-list )\n    firstprivate( var-list )\n    default( none | present )\n</code></pre> <p>Kernels Constructs</p> C/C++FORTRAN <pre><code>#pragma acc kernels [ clause-list ] new-line\n   structured block\n</code></pre> <pre><code>!$acc kernels [ clause-list ]\n   structured block\n!$acc end kernels\n</code></pre> Available clauses for kernels C/C++ and FORTRAN <pre><code>async [ ( int-expr ) ]\nwait [ ( int-expr-list ) ]\nnum_gangs( int-expr )\nnum_workers( int-expr )\nvector_length( int-expr )\ndevice_type( device-type-list )\nif( condition )\nself [ ( condition ) ]\ncopy( var-list )\ncopyin( [ readonly: ] var-list )\ncopyout( [ zero: ] var-list )\ncreate( [ zero: ] var-list )\nno_create( var-list )\npresent( var-list )\ndeviceptr( var-list )\nattach( var-list )\ndefault( none | present )\n</code></pre>"},{"location":"openacc/exercise-1/#compilers-supporting-openacc-programming-model","title":"Compilers Supporting OpenACC Programming Model","text":"<p>The following compilers provide support for the OpenACC programming model, which facilitates the development of parallel applications across various architectures:</p> <ul> <li>GNU Compiler Collection (GCC): This is an open-source compiler that supports both Nvidia and AMD CPUs, making it a versatile choice for developers looking to implement OpenACC.</li> <li>Nvidia HPC SDK: Developed by Nvidia, this compiler is specifically optimized for Nvidia GPUs. It offers robust support for the OpenACC programming model, enabling efficient utilization of GPU resources.</li> <li>HPE Compiler: Currently, this compiler supports FORTRAN but does not have support for C/C++. It is designed for high-performance computing applications and works well with the OpenACC model.</li> </ul> <p>Examples (GNU, Nvidia HPC SDK and HPE): Compilation</p> Nvidia HPC SDK <pre><code>$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel test.c \n</code></pre>"},{"location":"openacc/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-versionOpenACC-version-parallelOpenACC-version-kernels <pre><code>//Hello-world-CPU.c \n#include&lt;stdio.h&gt;\nint main()\n{\n  printf(\"Hello World from CPU!\\n\");        \n  return 0;\n}\n</code></pre> <pre><code>//Hello-world-parallel.c    \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{ \n#pragma acc parallel                                                             \n  printf(\"Hello World from GPU!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>//Hello-world-kernels.c \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{\n#pragma acc kernels                            \n  printf(\"Hello World from GPU!\\n\");\n  return 0;\n}\n</code></pre> Compilation and Output Serial-versionOpenACC-version-parallelOpenACC-version-kernels <pre><code>// compilation\n$ gcc Hello-world-CPU.c -o Hello-World-CPU\n\n// execution \n$ ./Hello-World-CPU\n\n// output\n$ Hello World from CPU!\n</code></pre> <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-parallel.c -o Hello-World-GPU\nmain:\n7, Generating NVIDIA GPU code\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n</code></pre> <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-kernels.c -o Hello-World-GPU\nmain:\n7, Accelerator serial kernel generated\n   Generating NVIDIA GPU code\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n</code></pre>"},{"location":"openacc/exercise-1/#loop","title":"Loop","text":"<p>In our second exercise, we will delve into the principles of loop parallelization, a crucial technique in high-performance computing that significantly enhances the efficiency of intensive computations. When dealing with computationally heavy operations within loops, it is often advantageous to parallelize these loops to leverage the full capabilities of multi-core processors or GPUs.</p> <p>To illustrate this concept, we will begin with a straightforward example: printing <code>Hello World from GPU</code> multiple times. This will serve as a basis for understanding how to implement loop parallelization effectively.</p> <p>It is important to note that simply adding directives such as <code>#pragma acc parallel</code> or <code>#pragma acc kernels</code> is insufficient for achieving parallel execution of computations. These directives are primarily designed to instruct the compiler to execute the computations on the device, but additional considerations and structure are required to fully exploit parallelization. Understanding how to appropriately organize and optimize loops for parallel execution is essential for maximizing performance in computational tasks. </p> <p></p> <p>Loop Constructs</p> C/C++FORTRAN <pre><code>#pragma acc loop [clause-list] new-line\n   for loop\n</code></pre> <pre><code>!$acc loop [clause-list]\n   do loop\n</code></pre> Available clauses for loop C/C++ and FORTRAN <pre><code>collapse( n )\ngang [( gang-arg-list )]\nworker [( [num:]int-expr )]\nvector [( [length:]int-expr )]\nseq\nindependent\nauto\ntile( size-expr-list )\ndevice_type( device-type-list )\nprivate( var-list )\nreduction( operator:var-list )\n</code></pre>"},{"location":"openacc/exercise-1/#questions-and-solutions_1","title":"Questions and Solutions","text":"Examples: Loop (Hello World) Serial-version-loopOpenACC-version-parallel-loopOpenACC-version-kernels-loop <pre><code>//Hello-world-CPU-loop.c    \n#include&lt;stdio.h&gt;\nint main()\n{\n  for(int i = 0; i &lt; 5; i++)\n    {         \n      printf(\"Hello World from CPU!\\n\");\n    }       \n  return 0;\n}\n</code></pre> <pre><code>//Hello-world-parallel-loop.c   \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{\n#pragma acc parallel loop\n  for(int i = 0; i &lt; 5; i++)\n    {                                \n      printf(\"Hello World from GPU!\\n\");\n    }\nreturn 0;\n}\n</code></pre> <pre><code>//Hello-world-kernels-loop.c    \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{\n#pragma acc kernels loop\n  for(int i = 0; i &lt; 5; i++)\n    {                                \n      printf(\"Hello World from GPU!\\n\");\n    }\nreturn 0;\n}\n</code></pre> Compilation and Output Serial-version-loopOpenACC-version-parallel-loopOpenACC-version-kernels-loop <pre><code>// compilation\n$ gcc Hello-world-CPU-loop.c -o Hello-World-CPU\n\n// execution \n$ ./Hello-World-CPU\n\n// output\n$ Hello World from CPU!\n$ Hello World from CPU!\n$ Hello World from CPU!\n$ Hello World from CPU!\n$ Hello World from CPU!                                \n</code></pre> <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-parallel-loop.c -o Hello-World-GPU\nmain:\n5, Generating NVIDIA GPU code\n  7, #pragma acc loop gang /* blockIdx.x */\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!                                \n</code></pre> <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-kernels-loop.c -o Hello-World-GPU\nmain:\n7, Loop is parallelizable\n   Generating NVIDIA GPU code\n    7, #pragma acc loop gang, vector(32) /* blockIdx.x threadIdx.x */\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!                                \n</code></pre>"},{"location":"openacc/exercise-2/","title":"Data Locality","text":""},{"location":"openacc/exercise-2/#data-clauses","title":"Data Clauses","text":"<p>Vector addition is a fundamental operation in linear algebra that involves summing two vectors element-wise. Each component of the resulting vector is the sum of the corresponding components from the two input vectors. This example of vector addition highlights two crucial constructs and clauses in OpenACC: compute constructs and data clauses. These include:</p> <ul> <li><code>#pragma acc parallel loop</code>: This directive is useful when the computation involves parallelizing a loop.</li> <li><code>#pragma acc kernels loop</code>: This directive also applies to loops and enables OpenACC to manage the kernels efficiently.</li> </ul> <p>Data clauses in OpenACC play a pivotal role in seamlessly transferring and managing data between the Central Processing Unit (CPU) and the Graphics Processing Unit (GPU). Below is a description of various data clauses and their usages:</p> <ul> <li><code>copy</code>: Allocates space for a variable on the device, transfers data to the device at the start of the region, copies the data back to the host after the region, and subsequently releases the memory allocated on the device.</li> <li><code>copyin</code>: Allocates device memory for a variable, transfers data to the device before the region begins, and does not return the data to the host after the region. The device memory is then released. </li> <li><code>copyout</code>: Allocates space for a variable on the device but does not copy data to the device before the region. It transfers data back to the host once the region is complete and releases the memory used on the device.</li> <li><code>create</code>: Allocates memory on the device without transferring any data from the host to the device or vice versa. </li> <li><code>present</code>: Indicates that the listed variables already exist on the device, thus requiring no additional action for the transfer. </li> <li><code>deviceptr</code>: Utilized for managing data outside of OpenACC, allowing for more controlled access to device pointers.</li> </ul> <p>This comprehensive approach to data management enhances the efficiency of GPU computing in high-performance applications.</p> <p></p> <p>Data Constructs</p> C/C++FORTRAN <pre><code>#pragma acc data [clause-list] new-line\n   structured block\n</code></pre> <pre><code>!$acc data [clause-list]\n   structured block\n!$acc end data\n</code></pre> Available clauses for data C/C++ and FORTRAN <pre><code>if( condition )\nasync [( int-expr )]\nwait [( wait-argument )]\ndevice_type( device-type-list )\ncopy( var-list )\ncopyin( [readonly:]var-list )\ncopyout( [zero:]var-list )\ncreate( [zero:]var-list )\nno_create( var-list )\npresent(a var-list )\ndeviceptr( var-list )\nattach( var-list )\ndefault( none | present )\n</code></pre> <p>To effectively implement the vector addition example using OpenACC, we need to focus on two specific data clauses. </p> <ol> <li> <p>Data Transfer for Input Vectors: The two initialized vectors must be transferred from the host to the device. To achieve this, we will utilize the <code>copyin</code> clause, which ensures that the data from the host is available on the device.</p> </li> <li> <p>Data Transfer for Output Vector: The product vector, which will store the results of the vector addition, does not require a transfer from the host to the device at the beginning of the computation. However, once the computation is complete, this vector must be transferred back from the device to the host. For this purpose, we will use the <code>copyout</code> clause.</p> </li> </ol> <p>By incorporating these data clauses, we can effectively manage the data flow between the host and the device during the execution of the vector addition example. </p> <p>In summary, follow these steps to set up the vector addition example with OpenACC: </p> <ol> <li>Use <code>copyin</code> to transfer the initialized input vectors to the device.</li> <li>Perform the vector addition on the device.</li> <li>Use <code>copyout</code> to transfer the resulting product vector back to the host. </li> </ol> <p>This approach ensures efficient data handling and optimizes the performance of the application.</p> <p>Here are the steps for learning the vector addition example:</p> <p></p> <ul> <li> <p>Allocating the CPU memory for <code>a</code>, <code>b</code>, and <code>c</code> vector <pre><code>// Initialize the memory on the host\nfloat *restrict a, *restrict b, *restrict c;\n\n// Allocate host memory\na = (float*)malloc(sizeof(float) * N);\nb = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Now, we need to fill in the values for the     arrays <code>a</code> and <code>b</code>.  <pre><code>// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n  {\n    a[i] = 1.0f;\n    b[i] = 2.0f;\n  }\n</code></pre></p> </li> <li> <p>Vector addition kernel function call definition</p> vector addition function call Serial-versionOpenACC-version <pre><code>// CPU function that adds two vector \nvoid Vector_Addition(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n}\n</code></pre> <pre><code>// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n#pragma acc kernels loop copyin(a[0:n], b[0:n]) copyout(c[0:n])\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n}       \n</code></pre> </li> </ul> <p> </p> <ul> <li>Deallocate the host memory <pre><code>// Deallocate host memory\nfree(a); \nfree(b); \nfree(c);\n</code></pre></li> </ul>"},{"location":"openacc/exercise-2/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial-versionOpenACC-templateOpenACC-version <pre><code>// Vector-addition.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Addition(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>// Vector-addition-template.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;    \n\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n\n// add here either parallel or kernel plus data map clauses\n#pragma acc \nfor(int i = 0; i &lt; n; i ++)\n   {\n     c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>// Vector-addition-openacc.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n// or #pragma acc kernels loop copyin(a[0:n], b[0:n]) copyout(c[0:n])\n#pragma acc kernels loop copyin(a[0:n], b[0:n]) copyout(c[0:n])\nfor(int i = 0; i &lt; n; i ++)\n   {\n    c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> Compilation and Output Serial-versionOpenACC-version <pre><code>// compilation\n$ gcc Vector-addition.c -o Vector-Addition-CPU\n\n// execution \n$ ./Vector-Addition-CPU\n\n// output\n$ ./Vector-addition-CPU \nPASSED\n</code></pre> <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Vector-addition-openacc.c -o Vector-Addition-GPU\nVector_Addition:\n12, Generating copyin(a[:n]) [if not already present]\n    Generating copyout(c[:n]) [if not already present]\n    Generating copyin(b[:n]) [if not already present]\n14, Loop is parallelizable\n    Generating NVIDIA GPU code\n    14, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n\n// execution\n$ ./Vector-Addition-GPU\n\n// output\n$ ./Vector-addition-GPU\nPASSED\n</code></pre> Question <ul> <li>Please try other data clauses for different applications and get familiarised with them.</li> </ul>"},{"location":"openacc/exercise-3/","title":"Optimize Loops","text":""},{"location":"openacc/exercise-3/#collapse-clause","title":"Collapse Clause","text":"<p>The collapse clause is an important feature for optimizing nested loops in parallel computing. When applied, it allows the entire section of the iteration to be divided among the available number of threads. Specifically, if the number of iterations in the outer loop matches the number of available threads, the outer loop can be effectively divided based on the number of threads. </p> <p>As illustrated in the figure below, without the use of the <code>collapse</code> clause, only the outer loop is parallelized. This means that each iteration of the outer loop will have a corresponding N number of inner loop executions, which is not the desired outcome for efficient parallel processing.</p> <p></p> Effect of not using the `collapse` clause. <p>To optimize the utilization of threads, especially on GPUs with a higher number of threads available, such as the Nvidia A100 GPU, we implement the <code>collapse</code> clause. This adjustment enables all available threads to participate in executing every single iteration, as demonstrated in the figure below.</p> <p></p> Effect of using the `collapse` clause. <p>Next, we will examine a basic example of matrix multiplication, a computation that inherently relies on nested loops. Understanding this example will provide valuable insights into handling nested loops effectively, which is a common scenario in parallel computing.</p> <p></p> <ul> <li> <p>Allocating the CPU memory for A, B, and C matrices.    Here, we notice that the matrix is stored in a    1D array because we want to consider the same function concept for CPU and GPU. <pre><code>// Initialize the memory on the host\nfloat *restrict a, *restrict b, *restrict c;\n\n// Allocate host memory\na  = (float*)malloc(sizeof(float) * (N*N));\nb  = (float*)malloc(sizeof(float) * (N*N));\nc  = (float*)malloc(sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Now, we need to fill in the values for the matrix A and B. <pre><code>// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n   {\n    a[i] = 2.0f;\n    b[i] = 2.0f;\n   }\n</code></pre></p> </li> <li> <p>Calling function <pre><code>// Function call\nMatrix_Multiplication(d_a, d_b, d_c, N);\n</code></pre></p> matrix multiplication function call SerialOpenACC <pre><code>void Matrix_Multiplication(float *a, float *b, float *c, int width)\n{\n  for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          float temp = 0;\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              temp += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = float;\n        } \n    }   \n}\n</code></pre> <pre><code>void Matrix_Multiplication(float *restrict a, float *restrict b, float *restrict c, int width)\n{\n  int length = width*width;\n  float sum = 0;\n#pragma acc parallel copyin(a[0:(length)], b[0:(length)]) copyout(c[0:(length)])\n#pragma acc loop collapse(2) reduction (+:sum)\n for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              sum += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = sum;\n          sum=0;\n        }\n    }\n}       \n</code></pre> </li> <li> <p>Deallocate the host memory <pre><code>// Deallocate host memory\nfree(a); \nfree(b); \nfree(c);\n</code></pre></p> </li> </ul>"},{"location":"openacc/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial-versionOpenACC-templateOpenACC-version <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n\nvoid Matrix_Multiplication(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float temp = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              temp += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = temp;                            \n        }                                                         \n    }   \n}\n\nint main()\n{\n\n  printf(\"Programme assumes that matrix size is N*N \\n\");\n  printf(\"Please enter the N size number \\n\");\n  int N =0;\n  scanf(\"%d\", &amp;N);\n\n  // Initialize the memory on the host\n  float *a, *b, *c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * (N*N));\n  b = (float*)malloc(sizeof(float) * (N*N));\n  c = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  Matrix_Multiplication(a, b, c, N);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      for(int j = 0; j &lt; N; j++)\n         {\n          printf(\"%f \", c[j]);\n         }\n      printf(\"%f \", c[j]);\n   }\n\n  // Deallocate host memory\n free(a); \n free(b); \n free(c);\n\n return 0;\n}\n</code></pre> <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;openacc.h&gt;\n#include&lt;stdbool.h&gt;\n\nvoid Matrix_Multiplication(float *restrict a, float *restrict b, float *restrict c, int width)\n{\n  int length = width*width;\n  float sum = 0;\n//#pragma acc ....\n//#pragma acc ....\n for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              sum += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = sum;\n          sum=0;\n        }\n    }\n}    \n\n\n// Host call (matrix multiplication)\nvoid CPU_Matrix_Multiplication(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n}\n\nint main()\n{\n\n  printf(\"Programme assumes that matrix size is N*N \\n\");\n  printf(\"Please enter the N size number \\n\");\n  int N =0;\n  scanf(\"%d\", &amp;N);\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  Matrix_Multiplication(a, b, c, N);\n\n\n  // CPU computation for verification \n  Matrix_Multiplication(a, b, host_check, N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)  \n      printf(\"Two matrices are not equal\\n\");\n  else\n      printf(\"Two matrices are equal\\n\");\n\n  // Deallocate host memory\n  free...\n\n  return 0;\n}\n</code></pre> <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;openacc.h&gt;\n#include&lt;stdbool.h&gt;\n\nvoid Matrix_Multiplication(float *restrict a, float *restrict b, float *restrict c, int width)\n{\n  int length = width*width;\n  float sum = 0;\n#pragma acc parallel copyin(a[0:(length)], b[0:(length)]) copyout(c[0:(length)])\n#pragma acc loop collapse(2) reduction (+:sum)\n for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              sum += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = sum;\n          sum=0;\n        }\n    }\n}       \n\n\n// Host call (matrix multiplication)\nvoid CPU_Matrix_Multiplication(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n}\n\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_c;\n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  b   = (float*)malloc(sizeof(float) * (N*N));\n  c   = (float*)malloc(sizeof(float) * (N*N));\n  host_check = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  Matrix_Multiplication(d_a, d_b, d_c, N);\n\n  // cpu computation for verification \n  CPU_Matrix_Multiplication(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n  return 0;\n}\n</code></pre> Compilation and Output Serial-versionOpenACC-version <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-CPU\n\n// execution \n$ ./Matrix-Multiplication-CPU\n\n// output\n$ g++ Matrix-multiplication.cc -o Matrix-multiplication\n$ ./Matrix-multiplication\nProgramme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n16 16 16 16 \n16 16 16 16  \n16 16 16 16  \n16 16 16 16 \n</code></pre> <pre><code>// compilation\n$ nvcc -arch=compute_70 Matrix-multiplication.cu -o Matrix-Multiplication-GPU\nMatrix_Multiplication:\n      9, Generating copyin(a[:length]) [if not already present]\n         Generating copyout(c[:length]) [if not already present]\n         Generating copyin(b[:length]) [if not already present]\n         Generating NVIDIA GPU code\n         12, #pragma acc loop gang collapse(2) /* blockIdx.x */\n             Generating reduction(+:sum)\n         14,   /* blockIdx.x collapsed */\n         16, #pragma acc loop vector(128) /* threadIdx.x */\n            Generating implicit reduction(+:sum)\n     16, Loop is parallelizable\n\n// execution\n$ ./Matrix-Multiplication-GPU\nThe programme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number\n$ 256\n\n// output\n$ Two matrices are equal\n</code></pre> Questions <pre><code>- Try to compute different matrix sizes instead of square matrices.\n</code></pre>"},{"location":"openacc/exercise-3/#thread-levels-of-parallelism","title":"Thread Levels of Parallelism","text":"<p>By default, the compiler selects the most effective configuration of thread blocks necessary for computation. However, programmers have the ability to control these thread blocks within their applications. OpenACC offers clear directives that enable the manipulation of threads and thread blocks effectively.</p> <p></p> OpenACC CUDA Parallelism num_gangs Grid Block coarse numn_workers Warps fine vector_length Threads SIMD or vector <p>This table illustrates the relationship between OpenACC and CUDA in terms of parallelism levels. Understanding these distinctions is crucial for optimizing performance in parallel computing.</p> Questions <ul> <li>What happens to performance when you modify the values in <code>num_gangs()</code>, <code>num_workers()</code>, and <code>vector_length()</code> compared to the default thread settings used by the compiler?</li> </ul>"},{"location":"openacc/exercise-4/","title":"Unified Memory","text":"<p>Unified memory simplifies the explicit data movement from the host to the device for programmers. The OpenACC API automatically manages data transfer between the CPU and the GPU. In this example, we will explore vector addition on the GPU utilizing the unified memory concept.</p> <p></p> Illustration of Unified Memory Concept <p>To enable unified memory in OpenACC, it is sufficient to use the compiler flag <code>-gpu=managed</code>.</p> <p>The following table summarizes the required steps for implementing the unified memory concept: </p> <p>Unified Memory</p> C/C++FORTRAN <pre><code>nvc -fast -acc=gpu -gpu=cc80 -gpu=managed -Minfo=accel test.c\n</code></pre> <pre><code>nvfortran -fast -acc=gpu -gpu=cc80 -gpu=managed -Minfo=accel test.c\n</code></pre> Without unified memory With unified memory Allocate the host memory Allocate the host memory Initialize the host value Initialize the host value Use data cluases, e.g,. copy, copyin Use data cluases, e.g,. copy, copyin Do the computation using the GPU kernel Do the computation using the GPU kernel Free host memory Free host memory"},{"location":"openacc/exercise-4/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition OpenACC-templateOpenACC-versionOpenACC-template (FORTRAN)OpenACC-version (FORTRAN) <pre><code>// Vector-addition-template.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;    \n\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n\n// add here either parallel or kernel and do need to add data map clauses\n#pragma acc \nfor(int i = 0; i &lt; n; i ++)\n   {\n     c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>// Vector-addition-openacc.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n#pragma acc kernels loop\nfor(int i = 0; i &lt; n; i ++)\n   {\n    c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>!! Vector-addition-openacc.f90\n\nmodule Vector_Addition_Mod\n  implicit none\ncontains\n subroutine Vector_Addition(a, b, c, n)\n    ! Input vectors\n    real(8), intent(in), dimension(:) :: a                        \n    real(8), intent(in), dimension(:) :: b\n    real(8), intent(out), dimension(:) :: c\n    integer :: i, n\n    // add here your acc directive\n    do i = 1, n\n       c(i) = a(i) + b(i)\n    end do\n    !$acc.....\n  end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\n  use openacc\n  use Vector_Addition_Mod\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n  real(8), dimension(:), allocatable :: b \n  ! Output vector\n  real(8), dimension(:), allocatable :: c\n\n  integer :: n, i             \n  print *, \"This program does the addition of two vectors \"\n  print *, \"Please specify the vector size = \" \n  read *, n  \n\n  ! Allocate memory for vector\n  allocate(a(n))\n  allocate(b(n))\n  allocate(c(n))\n\n  ! Initialize content of input vectors, \n  ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\n  do i = 1, n\n     a(i) = sin(i*1D0) * sin(i*1D0)\n     b(i) = cos(i*1D0) * cos(i*1D0) \n  enddo\n\n  ! Call the vector add subroutine \n  call Vector_Addition(a, b, c, n)\n\n  !!Verification\n  do i = 1, n\n     if (abs(c(i)-(a(i)+b(i))==0.00000)) then \n     else\n        print *, \"FAIL\"\n     endif\n  enddo\n  print *, \"PASS\"\n\n  ! Delete the memory\n  deallocate(a)\n  deallocate(b)\n  deallocate(c)\n\nend program main        \n</code></pre> <pre><code>!! Vector-addition-openacc.f90\n\nmodule Vector_Addition_Mod\n  implicit none\ncontains\n subroutine Vector_Addition(a, b, c, n)\n    ! Input vectors\n    real(8), intent(in), dimension(:) :: a                        \n    real(8), intent(in), dimension(:) :: b\n    real(8), intent(out), dimension(:) :: c\n    integer :: i, n\n    !$acc parallel loop \n    do i = 1, n\n       c(i) = a(i) + b(i)\n    end do\n    !$acc end parallel\n  end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\n  use openacc\n  use Vector_Addition_Mod\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n  real(8), dimension(:), allocatable :: b \n  ! Output vector\n  real(8), dimension(:), allocatable :: c\n\n  integer :: n, i             \n  print *, \"This program does the addition of two vectors \"\n  print *, \"Please specify the vector size = \" \n  read *, n  \n\n  ! Allocate memory for vector\n  allocate(a(n))\n  allocate(b(n))\n  allocate(c(n))\n\n  ! Initialize content of input vectors, \n  ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\n  do i = 1, n\n     a(i) = sin(i*1D0) * sin(i*1D0)\n     b(i) = cos(i*1D0) * cos(i*1D0) \n  enddo\n\n  ! Call the vector add subroutine \n  call Vector_Addition(a, b, c, n)\n\n  !!Verification\n  do i = 1, n\n     if (abs(c(i)-(a(i)+b(i))==0.00000)) then \n     else\n        print *, \"FAIL\"\n     endif\n  enddo\n  print *, \"PASS\"\n\n  ! Delete the memory\n  deallocate(a)\n  deallocate(b)\n  deallocate(c)\n\nend program main\n</code></pre> Compilation and Output OpenACC-versionOpenACC-version (FORTRAN) <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel -gpu=managed Vector-addition-openacc.c -o Vector-Addition-GPU\nVector_Addition:\n12, Generating copyin(a[:n]) [if not already present]\n    Generating copyout(c[:n]) [if not already present]\n    Generating copyin(b[:n]) [if not already present]\n14, Loop is parallelizable\n    Generating NVIDIA GPU code\n    14, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n\n// execution\n$ ./Vector-Addition-GPU\n\n// output\n$ ./Vector-addition-GPU\nPASSED\n</code></pre> <pre><code>// compilation\n$ nvfortran -fast -acc=gpu -gpu=cc80 -gpu=managed -Minfo=accel Vector-addition-openacc.f90 -o Vector-Addition-GPU\nvector_addition:\n     12, Generating NVIDIA GPU code\n         13, !$acc loop gang, vector(128) ! blockidx%x threadidx%x\n     12, Generating implicit copyin(a(:n)) [if not already present]\n         Generating implicit copyout(c(:n)) [if not already present]\n         Generating implicit copyin(b(:n)) [if not already present\n\n// execution         \n$ ./Vector-Addition-GPU\n\n// output\nThis program does the addition of two vectors \nPlease specify the vector size = \n1000000\nPASS\n</code></pre> Questions <ul> <li>Have you noticed any performance improvements when using unified memory?</li> </ul>"},{"location":"openacc/preparation/","title":"Preparation","text":""},{"location":"openacc/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"openacc/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example, the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"openacc/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200776\n[u100490@login02 p200776]$ pwd\n/project/home/p200776\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200776]$ mkdir $USER\n### or \n[u100490@login02 p200776]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"openacc/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200776]$cd u100490\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200776/OpenACC .\n[u100490@login03 u100490]$ cd OpenACC/\n[u100490@login03 OpenACC]$ pwd\n/project/home/p200776/u100490/OpenACC\n[u100490@login03 OpenACC]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200776   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200776 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200776 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#7-until-now-you-are-in-the-login-node-now-it-is-time-to-do-the-dry-run-test","title":"7. Until now, you are in the login node; now it is time to do the dry run test","text":"<ul> <li> <p>7.1 Reserve the interactive node for running/testing OpenACC applications    <pre><code>$ salloc -A p200776 --res gpu-openacc-morning --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></p> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200776 --res gpu-openacc-morning --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> <li> <p>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490    p200776  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></p> </li> </ul>"},{"location":"openacc/preparation/#8-now-we-need-to-check-the-simple-openacc-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check the simple OpenACC application if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 OpenACC]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-openacc-codes","title":"9. Finally, we need to load the compiler to test the GPU OpenACC codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing OpenACC code  <pre><code>$ module load OpenMPI/5.0.3-NVHPC-24.9-CUDA-12.6.0\n### or\n$ source module.sh\n</code></pre></p> check if the module is loaded properly <pre><code>[u100490@mel2129 OpenACC]$ module load OpenMPI/5.0.3-NVHPC-24.9-CUDA-12.6.0\n[u100490@mel2129 OpenACC]$ ml\n\nCurrently Loaded Modules:\n1) env/release/2024.1            (S)   7) CUDA/12.6.0                         13) OpenSSL/3                                   19) PMIx/5.0.2-GCCcore-13.3.0\n2) lxp-tools/myquota/0.3.3       (S)   8) NVHPC/24.9-CUDA-12.6.0              14) libevent/2.1.12-GCCcore-13.3.0              20) PRRTE/3.0.5-GCCcore-13.3.0\n3) GCCcore/13.3.0                      9) XZ/5.4.5-GCCcore-13.3.0             15) UCX/1.16.0-GCCcore-13.3.0                   21) UCC/1.3.0-GCCcore-13.3.0\n4) zlib/1.3.1-GCCcore-13.3.0          10) libxml2/2.12.7-GCCcore-13.3.0       16) GDRCopy/2.4.1-GCCcore-13.3.0                22) NCCL/2.22.3-GCCcore-13.3.0-CUDA-12.6.0\n5) binutils/2.42-GCCcore-13.3.0       11) libpciaccess/0.18.1-GCCcore-13.3.0  17) UCX-CUDA/1.16.0-GCCcore-13.3.0-CUDA-12.6.0  23) UCC-CUDA/1.3.0-GCCcore-13.3.0-CUDA-12.6.0\n6) numactl/2.0.18-GCCcore-13.3.0      12) hwloc/2.10.0-GCCcore-13.3.0         18) libfabric/1.21.0-GCCcore-13.3.0             24) OpenMPI/5.0.3-NVHPC-24.9-CUDA-12.6.0\nWhere:\nS:  Module is Sticky, requires --force to unload or purge\n</code></pre> </li> </ul>"},{"location":"openacc/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>10.1 For example, Dry-run-test  <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world.c -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly, for the hands-on session, we need to do the node reservation:","text":"<ul> <li> <p>10.1 For example, reservation  <pre><code>$ salloc -A p200776 --res gpu-openacc-afternoon --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre></p> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200776 --res gpu-openacc-afternoon --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> </ul>"},{"location":"openacc/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands-on exercise","text":"<ul> <li>12.1 For example, in the <code>Hello World</code> example, we do the following steps:  <pre><code>[u100490@mel2063 OpenACC]$ pwd\n/project/home/p200776/u100490/OpenACC\n[u100490@mel2063 OpenACC]$ ls\n[u100490@mel2063 OpenACC]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Vector-addition\n[u100490@mel2063 OpenACC]$ source module.sh\n[u100490@mel2063 OpenACC]$ cd Hello-world\n// compilation\n[u100490@mel2063 OpenACC]$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world.c -o Hello-World-GPU\n\n// execution\n[u100490@mel2063 OpenACC]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 OpenACC]$ Hello World from GPU\n</code></pre></li> </ul>"},{"location":"openacc/profiling/","title":"Profiling and Performance","text":"<p>Profiling is an essential procedure to ensure that you are utilising the given architecture with a given algorithm. Sometimes, we might think we are doing efficient computation. However, this will not always be the case unless we do the proper profiling and check if all the resources are utilized properly. </p> <p>Using Nvidia HPC SDK, we could profile our OpenACC code. We could do the profiling using the Command line or the GUI.</p>"},{"location":"openacc/profiling/#command-line","title":"Command Line","text":"<ul> <li><code>export NVCOMPILER_ACC_TIME=[]</code><ul> <li>[1]: kernel launches</li> <li>[2]: data transfers</li> <li>[4]: region entry/exit</li> <li>[8]: wait for operations or synchronizations</li> <li>[16]: device memory allocates and deallocates</li> </ul> </li> </ul> <p>Setting <code>export NVCOMPILER_ACC_NOTIFY=3</code> provides kernel executions and data transfer information.</p> <p>Profiling: Compilation</p> Nvidia HPC SDK <pre><code>// compilation \nVector_Addition:\n     12, Generating NVIDIA GPU code\n     14, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n     12, Generating implicit copyin(a[:n]) [if not already present]\n         Generating implicit copyout(c[:n]) [if not already present]\n         Generating implicit copyin(b[:n]) [if not already present]\n\n//execution        \n[u100@mel2041 Unified-memory]$ ./a.out \nThis program does the addition of two vectors \nupload CUDA data  file=/Vector-addition-openacc.c function=Vector_Addition line=12 device=0 threadid=1 variable=b bytes=400\nupload CUDA data  file=/Vector-addition-openacc.c function=Vector_Addition line=12 device=0 threadid=1 variable=a bytes=400\nlaunch CUDA kernel  file=/Vector-addition-openacc.c function=Vector_Addition line=12 device=0 threadid=1 num_gangs=1 num_workers=1 vector_length=128 grid=1 block=128\ndownload CUDA data  file=/Vector-addition-openacc.c function=Vector_Addition line=17 device=0 threadid=1 variable=c bytes=400\nPASSED  \n</code></pre>"},{"location":"openacc/profiling/#gui","title":"GUI","text":"<p>The Visual Profiler is organized into views. Together, the views allow you to analyze and visualize the performance of your application.  The Timeline View shows CPU and GPU activity that occurred while your application was being profiled.  Multiple timelines can be opened in the Visual Profiler simultaneously in different tabs. The following figure shows a Timeline View for an OpenACC application.</p> <p>In order to visualize the performance of your application, you should connect to the HPC machine via -X forward;  otherwise, you will not be able to see the GUI application. For example, on MeluXina, you should do the following.</p> <p>GUI login</p> <pre><code>$ ssh -X meluxina\n\n$ salloc -A p200117 --res p200117-openacc-2 --partition=gpu --qos default -N 1 -t 00:30:00 srun --forward-x --pty bash -l\n</code></pre> <p>We also need to add a few extra modules to open a GUI application.  On MeluXina, we need to add the following modules: </p> <p>Required modules</p> <pre><code>module load NVHPC/22.7\nmodule load CUDA/11.7.0\nmodule load Mesa/22.0.3-GCCcore-11.3.0      \nmodule load Qt5/5.15.5-GCCcore-11.3.0\n</code></pre> <p>Once the required modules are loaded, you can compile your application and visualize its performance.  Finally, we need to use the command line <code>nsys-ui</code> to open the GUI application and load <code>timeline.nsys-rep</code>. </p> <p>Compilation and GUI</p> <pre><code>```\n[u100@mel2073 Vector-addition]$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Vector-addition.c\nnvc-Warning-CUDA_HOME has been deprecated. Please use NVHPC_CUDA_HOME instead.\n[u100@mel2073 Vector-addition]$ nsys profile -o timeline ./a.out\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\nThis program does the addition of two vectors \nPlease specify the vector size = 10000\nPASSED\nGenerating '/tmp/nsys-report-6c02.qdstrm'\n[1/1] [========================100%] timeline.nsys-rep\n\n// Open the GUI application  and load timeline.nsys-rep\n$ nsys-ui &amp;\n```\n</code></pre> <p> </p>"},{"location":"openmp/","title":"Introduction to OpenMP Programming Model","text":"<p>Participants from this course will learn Multicore (shared memory) CPU programming using the OpenMP programming model, such as parallel region, environmental routines, and data sharing. Furthermore, understanding the multicore shared memory architecture and how parallel threads blocks are used to parallelise the computational task. Since we deal with multicores and parallel threads, proper parallel work sharing and the synchronisation of the parallel calls are to be studied in detail. Finally, participants will also learn to use the OpenMP programming model to accelerate linear algebra (routines) and iterative solvers on the Multicore CPU. Participants will learn theories first and implement the OpenMP programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openmp/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openmp/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the shared memory architecture <ul> <li>Unified Memory Access (UMA) and Non-Unified Memory Access (NUMA)  </li> <li>Hybrid distributed shared memory architecture  </li> </ul> </li> <li>Implement OpenMP programming model  <ul> <li>Parallel region  </li> <li>Environment routines  </li> <li>Data sharing  </li> </ul> </li> <li>Efficient handling of OpenMP constructs  <ul> <li>Work sharing  </li> <li>Synchronisation constructs  </li> <li>Single Instruction Multiple Data (SIMD) directive </li> </ul> </li> <li>Apply the OpenMP programming knowledge to parallelise examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openmp/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No prior parallel programming experience is needed.</p>"},{"location":"openmp/#cpu-compute-resource","title":"CPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openmp/#course-organization-and-registration","title":"Course Organization and Registration","text":"<p>Format - Online  Previous event: 31<sup>st</sup> May, 2023  Next Event: 29<sup>th</sup>, October, 2024</p> <p>Registration: 29<sup>th</sup>, October, 2024</p>"},{"location":"openmp/exercise-1/","title":"Parallel Region","text":""},{"location":"openmp/exercise-1/#parallel-construct","title":"Parallel Construct","text":"<p>In this exercise, we will create a parallel region and execute the computational content in parallel. First, however, this exercise is to create a parallel region and understand the threads' behaviour in parallel. In later exercises, we will study how to parallelise the computational task within the parallel region.</p> <p></p> <p>To create a parallel region, we use the following parallel constructs:</p> <p>Parallel Constructs</p> C/C++FORTRAN <pre><code>#pragma omp parallel\n</code></pre> <pre><code>!$omp parallel \n</code></pre> <p>The above figure illustrates the parallel region behaviour; as we notice, within the parallel region, we get parallel threads. This means parallel threads can be executed independently of each other, and there is no order of execution.</p> <p>At the same time, in order to enable OpenMP constructs, clauses, and environment variables. etc., we need to include the OpenMP library as follows:</p> <p>OpenMP library</p> C/C++FORTRAN <pre><code>#include&lt;omp.h&gt;\n</code></pre> <pre><code>use omp_lib\n</code></pre>"},{"location":"openmp/exercise-1/#compilers","title":"Compilers","text":"<p>The following compilers would support the OpenMP programming model.</p> <ul> <li>GNU - It is an open source and can be used for Intel and AMD CPUs</li> <li>Intel - It is from Intel and only optimized for Intel CPUs</li> <li>AOOC - Suitable for AMD CPUs, especially \u201cZen\u201d core architecture.</li> </ul> <p>Examples (GNU, Intel and AMD): Compilation</p> GNUIntelAOOC <pre><code>$ gcc test.c -fopenmp\n$ g++ test.cc -fopenmp\n$ gfortran test.f90 -fopenmp\n</code></pre> <pre><code>$ icc test.c -qopenmp\n$ icpc test.cc -qopenmp\n$ ifort test.f90 -qopenmp        \n</code></pre> <pre><code>$ clang test.c -fopenmp\n$ clang++ test.cc -fopenmp\n$ flang test.f90 -fopenmp        \n</code></pre>"},{"location":"openmp/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-version (C/C++)Serial-version (FORTRAN)OpenMP-version (C/C++)OpenMP-version (FORTRAN) <pre><code>#include&lt;iostream&gt;\nusing namespace std;\n\nint main()\n{\n  cout &lt;&lt; endl;\n  cout &lt;&lt; \"Hello world from the master thread\"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre> <pre><code>program Hello_world_Serial\n\nprint *, 'Hello world from master thread'\n\nend program\n</code></pre> <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from the master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        cout &lt;&lt; \"Hello world from the parallel region \"&lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from the master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\nprint *, 'Hello world from master thread'\n\n!$omp parallel\nprint *, 'Hello world from parallel region'\n!$omp end parallel\n\nprint *,'end of the programme from master thread'\n\nend program\n</code></pre> Compilation and Output Serial-version (C/C++)Serial-version (FORTRAN)OpenMP-version (C/C++)OpenMP-version (FORTRAN) <pre><code>// compilation\n$ g++ Hello-world-Serial.cc -o Hello-World-Serial\n\n// execution \n$ ./Hello-World-Serial\n\n// output\n$ Hello world from the master thread\n</code></pre> <pre><code>// compilation\n$ gfortran Hello-world-Serial.f90 -o Hello-World-Serial\n\n// execution \n$ ./Hello-World-Serial\n\n// output\n$ Hello world from the master thread\n</code></pre> <pre><code>// compilation\n$ g++ -fopenmp Hello-world-OpenMP.cc -o Hello-World-OpenMP\n\n// execution\n$ ./Hello-World-OpenMP\n\n// output\n$ Hello world from the parallel region\nHello world from the parallel region\n..\n..\nHello world from the parallel region\n\nend of the programme from the master thread\n</code></pre> <pre><code>// compilation\n$ gfortran -fopenmp Hello-world-OpenMP.f90 -o Hello-World-OpenMP\n\n// execution\n$ ./Hello-World-OpenMP\n\n// output\n$ Hello world from the master thread\nHello world from the parallel region\n..\n..\nHello world from the parallel region\nEnd of the programme from the master thread\n</code></pre> Questions <ul> <li>What do you notice from those examples? Can you control parallel region printout, that is, how many times it should be printed or executed?     </li> <li>What happens if you do not use the OpenMP library, <code>#include&lt;omp.h&gt; or use omp_lib</code>?</li> </ul> <p>Although creating a parallel region would allow us to do the parallel computation, however, at the same time, we should have control over the threads being created in the parallel region, for example, how many threads are needed for a particular computation, thread number, etc. For this, we need to know a few of the important environmental routines provided by OpenMP. The list below shows a few of the most important environment routines that the programmer should know about for optimised OpenMP coding.</p>"},{"location":"openmp/exercise-1/#environment-routines-important","title":"Environment Routines (important)","text":"<ul> <li> <p>Define the number of threads to be used within the parallel region</p> <pre><code>(C/C++): void omp_set_num_threads(int num_threads);\n(FORTRAN): subroutine omp_set_num_threads(num_threads) \ninteger num_threads\n</code></pre> </li> <li> <p>To get the number of threads in the current parallel region</p> <pre><code>(C/C++): int omp_get_num_threads(void);\n(FORTRAN): integer function omp_get_num_threads()\n</code></pre> </li> <li> <p>To get available maximum threads (system default)</p> <pre><code>(c/c++): int omp_get_max_threads(void);\n(FORTRAN): integer function omp_get_max_threads()\n</code></pre> </li> <li> <p>To get thread numbers (e.g., 1, 4, etc.)</p> <pre><code>(c/c+): int omp_get_thread_num(void);\n(FORTRAN): integer function omp_get_thread_num()\n</code></pre> </li> <li> <p>To know the number of processors available to the device</p> <pre><code>(c/c++): int omp_get_num_procs(void);\n(FROTRAN): integer function omp_get_num_procs()\n</code></pre> </li> </ul>"},{"location":"openmp/exercise-1/#questions-and-solutions_1","title":"Questions and Solutions","text":"Questions <ul> <li>How can you identify the thread numbers within the parallel region?</li> <li>What happens if you not set <code>omp_set_num_threads()</code>, for example, <code>omp_set_num_threads(5)|call omp_set_num_threads(5)</code>, what do you notice?   Alternatively, you can also set the number of threads to be used in the application during the compilation <code>export OMP_NUM_THREADS</code>; what do you see?</li> </ul> Question (C/C++)Question (FORTRAN)Answer (C/C++)Answer (FORTRAN)AnswerSolution Output (C/C++)Solution Output (FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from the master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        //cout &lt;&lt; \"Hello world from thread id \"\n        &lt;&lt; \" from the team size of \"\n        &lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from the master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\n!$omp parallel \n!! print *, \n!$omp end parallel\n\nend program\n</code></pre> <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from the master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        cout &lt;&lt; \"Hello world from thread id \"\n        &lt;&lt; omp_get_thread_num() &lt;&lt; \" from the team size of \"\n        &lt;&lt; omp_get_num_threads()\n        &lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from the master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\n!$omp parallel \nprint *, 'Hello world from thread id ', omp_get_thread_num(), 'from the team size of', omp_get_num_threads()\n!$omp end parallel\n\nend program\n</code></pre> <pre><code>$ export OMP_NUM_THREADS=10\n// or \n$ setenv OMP_NUM_THREADS 4\n// or\n$ OMP NUM THREADS=4 ./omp code.exe\n</code></pre> <pre><code>ead id Hello world from thread id Hello world from thread id 3 from the team size of 9 from the team size of 52 from the team size of  from the team size of 10\n0 from the team size of 10\n10\n10\n10\n7 from the team size of 10\n4 from the team size of 10\n8 from the team size of 10\n1 from the team size of 10\n6 from the team size of 10\n</code></pre> <pre><code>Hello world from thread id            0 from the team size of          10\nHello world from thread id            4 from the team size of          10\nHello world from thread id            5 from the team size of          10\nHello world from thread id            9 from the team size of          10\nHello world from thread id            2 from the team size of          10\nHello world from thread id            3 from the team size of          10\nHello world from thread id            7 from the team size of          10\nHello world from thread id            6 from the team size of          10\nHello world from thread id            8 from the team size of          10\nHello world from thread id            1 from the team size of          10\n</code></pre>"},{"location":"openmp/exercise-1/#utilities","title":"Utilities","text":"<p>The main aim is to do the parallel computation to speed up computation on a given parallel architecture. Therefore, measuring the timing and comparing the solution between serial and parallel code is very important. In order to measure the timing, OpenMP provides an environmental variable, <code>omp_get_wtime()</code>.</p> Time measuring C/C++FORTRAN <pre><code>double start; \ndouble end; \nstart = omp_get_wtime(); \n... work to be timed ... \nend = omp_get_wtime(); \nprintf(\"Work took %f seconds\\n\", end - start);\n</code></pre> <pre><code>DOUBLE PRECISION START, END \nSTART = omp_get_wtime() \n... work to be timed ... \nEND = omp_get_wtime() \nPRINT *, \"Work took\", END - START, \"seconds\"        \n</code></pre>"},{"location":"openmp/exercise-2/","title":"Data Sharing Attribute","text":""},{"location":"openmp/exercise-2/#shared-variable","title":"Shared variable","text":"<ul> <li>All the threads have access to the shared variable.</li> <li>By default, in the parallel region, all the variables are considered shared variables except the loop iteration counter variables.</li> </ul> <p>Note</p> <p>Shared variables should be handled carefully; otherwise, they cause race conditions in the program.</p> <p></p> Examples: Shared variable (C/C++)(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  // Array size\n  int N = 10;\n\n  // Initialize the variables\n  float *a;\n\n  // Allocate the memory\n  a  = (float*)malloc(sizeof(float) * N);\n\n  //#pragma omp parallel for\n  // or \n#pragma omp parallel for shared(a)\n  for (int i = 0; i &lt; N; i++)\n    {\n      a[i] = a[i] + i;  \n      cout &lt;&lt; \"value of a in the parallel region\" &lt;&lt; a[i] &lt;&lt; endl;\n    }\n\n  for (int i = 0; i &lt; N; i++)\n    cout &lt;&lt; \"value of a after the parallel region \" &lt;&lt; a[i] &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre> <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n\n  integer :: n, i\n  n=10\n\n  ! Allocate memory for vector\n  allocate(a(n))\n\n  !$omp parallel shared(a)\n  !$omp do\n  do i = 1, n\n      a(i) = a(i) + i\n      print*, 'value of a in the parallel region', a(i)\n  end do\n  !$omp end do\n  !$omp end parallel\n\n  do i = 1, n\n      a(i) = a(i) + i\n      print*,'value of an after the parallel region', a(i)\n  end do\n\n  ! Delete the memory\n  deallocate(a)\n\nend program main\n</code></pre> Questions <ul> <li>Does the value of vector <code>a</code> change after the parallel loop, if not why, think?</li> <li>Do we really need to mention <code>shared(a)</code>, is it neccessary? </li> </ul>"},{"location":"openmp/exercise-2/#private-variable","title":"Private variable","text":"<ul> <li>Each thread will have its own copy of the private variable.</li> <li>And the private variable is only accessible within the parallel region,  not outside of the parallel region.</li> <li>By default, the loop iteration counters are considered as a private.</li> <li>A change made by one thread is not visible to other threads.</li> </ul> Examples: Private variable (C/C++)(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  // Array size\n  int N = 10;\n\n  // Initialize the variables\n  float *a,b,c;\n  b = 1.0;\n  c = 2.0;\n\n  // Allocate the memory\n  a  = (float*)malloc(sizeof(float) * N);\n\n#pragma omp parallel for private(b,c)\n  for (int i = 0; i &lt; N; i++)\n    {\n      b = a[i] + i;\n      c = b + 10 * i;\n      cout &lt;&lt; \"value of c in the parallel region \" &lt;&lt; c &lt;&lt; endl;\n    }\n\n  cout &lt;&lt; \"value of c after the parallel region \" &lt;&lt; c &lt;&lt; endl; \n\n  return 0;\n}\n</code></pre> <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n\n  real(8) :: b, c\n  integer :: n, i  \n  n=10\n  b=1.0\n  c=2.0\n\n  ! Allocate memory for vector\n  allocate(a(n))\n\n  !$omp parallel private(b,c) shared(a)\n  !$omp do\n  do i = 1, n\n      b = a(i) + i\n      c = b + 10 * i\n      print*, 'value of c in the parallel region', c\n  end do\n  !$omp end do\n  !$omp end parallel\n  print*, 'value of c after the parallel region', c\n\n  ! Delete the memory\n  deallocate(a)\n\nend program main\n</code></pre> Questions <ul> <li>What is the value of the variable <code>c</code> in the parallel region and after the parallel region?</li> <li>After the parallel region, has variable <code>c</code> been updated or not? </li> </ul>"},{"location":"openmp/exercise-2/#lastprivate","title":"Lastprivate","text":"<ul> <li>lastprivate: is also similar to a private clause</li> <li>But each thread will have an uninitialized copy of the variables passed  as lastprivate</li> <li>At the end of the parallel loop or sections, the final variable value will  be the last thread accessed value in the section or in a parallel loop.</li> </ul> Examples: Lastprivate variable (C/C++)(FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int n = 10;\n  int var = 5;\n  omp_set_num_threads(10);\n#pragma omp parallel for lastprivate(var)\n  for(int i = 0; i &lt; n; i++)\n    {\n      var += omp_get_thread_num();\n      cout &lt;&lt; \" lastprivate in the parallel region \" &lt;&lt; var &lt;&lt; endl;\n    } /*-- End of parallel region --*/\n  cout &lt;&lt; \"lastprivate after the parallel region \" &lt;&lt; var &lt;&lt;endl;\n\n  return 0;\n}\n</code></pre> <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Initialise the variable\n  real(8) :: var\n  integer :: n, i  \n  n = 10\n  var = 5\n\n  call omp_set_num_threads(10)\n\n  !$omp parallel \n  !$omp do lastprivate(var)\n  do i = 1, n\n     var  =  var + omp_get_thread_num()\n     print*, 'lastprivate in the parallel region ', var\n  end do\n  !$omp end do\n  !$omp end parallel\n\n  print*, 'lastprivate after the parallel region ', var\n\nend program main\n</code></pre> Questions <ul> <li>What is the value of the variable <code>var</code> in the parallel region and after the parallel region?</li> <li>Do you think the initial value of variable <code>var</code> is considered within the parallel region? </li> </ul>"},{"location":"openmp/exercise-2/#firstprivate","title":"Firstprivate","text":"<ul> <li>firstprivate: is similar to a private clause</li> <li>But each thread will have an initialized copy of the variables passed  as firstprivate</li> <li>Available for parallel constructs, loops, sections and single  constructs</li> </ul> Examples: Firstprivate variable (C/C++)(FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int n = 10;\n  int var = 5;\n  omp_set_num_threads(10);\n#pragma omp parallel for firstprivate(var)\n  for(int i = 0; i &lt; n; i++)\n    {\n      var += omp_get_thread_num();\n      cout &lt;&lt; \" lastprivate in the parallel region \" &lt;&lt; var &lt;&lt; endl;\n    } /*-- End of parallel region --*/\n  cout &lt;&lt; \"lastprivate after the parallel region \" &lt;&lt; var &lt;&lt;endl;\n\n  return 0;\n}\n</code></pre> <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Initialise the variable\n  real(8) :: var\n  integer :: n, i  \n  n = 10\n  var = 5\n\n  call omp_set_num_threads(10)\n\n  !$omp parallel \n  !$omp do firstprivate(var)\n  do i = 1, n\n     var  =  var + omp_get_thread_num()\n     print*, 'lastprivate in the parallel region ', var\n  end do\n  !$omp end do\n  !$omp end parallel\n\n  print*, 'lastprivate after the parallel region ', var\n\nend program main\n</code></pre> Questions <ul> <li>What is the value of the variable <code>var</code> in the parallel region and after the parallel region?</li> <li>Is variable <code>var</code> has been updated after the parallel region, if not why, think?</li> </ul>"},{"location":"openmp/exercise-3/","title":"Work Sharing Constructs(loop)","text":""},{"location":"openmp/exercise-3/#serial-version-discussion","title":"Serial version discussion","text":"<p>To begin to understand the work-sharing constructs, we need to learn how to parallelise the <code>for - C/C++</code> or <code>do - FORTRAN</code> loop. For this, we will learn simple vector addition examples.</p> <p></p> <p>As we can see from the above figure, the two vectors should be added to get a single vector. This is done by iterating over the elements and adding them together. For this, we use <code>for - C/C++</code> or <code>do - FORTRAN</code>.  Since there are no data dependencies, the loop indexes do not have any data dependency on the other indexes. Therefore, it is easy to parallelise.</p> Examples: Loop Serial(C/C++)Serial(FORTRAN) <pre><code>for(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n</code></pre> <pre><code>do i = 1, n\n  c(i) = a(i) + b(i)\nend do\n</code></pre> <p>Note</p> <p>FORTRAN has a column-major order and C/C++ has a row-major order</p> <pre><code>Fortran array index starts from 1\nC/C++ array index starts from 0\n</code></pre>"},{"location":"openmp/exercise-3/#parallel-version-discussion","title":"Parallel version discussion","text":"<p>Now we will look into the how to parallelise the <code>for - C/C++</code> or <code>do - FORTRAN</code> loops. For this, we just need to add below syntax (OpenMP directives).</p> Functionality Syntax in C/C++ Syntax in FORTRAN Distribute iterations over the threads #pragma omp for !$omp do <p>With the help of the above syntax, the loops can be easily parallelised. The figure below shows an example of how the loops are parallelised. As we can notice here, we set the <code>omp_set_num_threads(5)</code> for the number of parallel threads that should be used within the loops. Furthermore, the loop index goes from <code>0</code> to <code>9</code>; in total, we need to iterate <code>10</code> elements. </p> <p>In this example, using <code>5</code> threads would divide <code>10</code> iterations by <code>two</code>. Therefore, each thread will handle <code>2</code> iterations. In total, <code>5</code> threads will do just <code>2</code> iterations in parallel for <code>10</code> elements.  </p> <p></p> Examples: Loops parallelisation Serial(C/C++)FORTRAN(C/C++) <pre><code>#pragma omp parallel for\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n\n//or\n\n#pragma omp parallel \n#pragma omp for\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n</code></pre> <pre><code>!$omp parallel do\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end parallel do\n\n//or\n\n!$omp parallel\n!$omp do\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end do\n!$omp end parallel\n</code></pre> <p>From understating loop parallelisation, we will continue with vector operations in parallel, that is, adding two vectors. It is very simple, and we just need to add the <code>#pragma omp parallel for</code> for C/C++, <code>!$omp parallel do</code> for FORTRAN. Could you please try this yourself? The serial code, templates, and compilation command have been provided below.</p>"},{"location":"openmp/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120 // 500000000\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing vector addition function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n! Call the vector addition subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n// ADD YOUR PARALLEL REGION FOR THE LOOP\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // ADD YOUR PARALLEL REGION HERE  \n  // Executing vector addition function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!! ADD YOUR PARALLEL DO LOOP\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!! ADD YOUR PARALLEL REGION \n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120 //500000000\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n)\n{\n// ADD YOUR PARALLEL\n#pragma omp parallel for\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  omp_set_num_threads(omp_get_max_threads());\n\n  // Start measuring time\n  double start = omp_get_wtime();\n\n  // Executing vector addition function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  double end = omp_get_wtime();\n\n  printf(\"Time measured: %.3f seconds.\\n\", end - start);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!$omp do\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n!$omp end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!$omp parallel \n! Call the vector addition subroutine \ncall Vector_Addition(a, b, c, n)\n!$omp end parallel\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> Compilation and Output Serial(C/C++)Serial(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>// compilation\n$ gcc Vector-addition-Serial.c -o Vector-addition-Serial\n\n// execution \n$ ./Vector-addition-Serial\n\n// output\n$ ./Vector-addition-Serial\n</code></pre> <pre><code>// compilation\n$ gfortran Vector-addition-Serial.f90 -o Vector-addition-Serial\n\n// execution\n$ ./Vector-addition-Serial\n\n// output\n$ ./Vector-addition-Serial\n</code></pre> <pre><code>// compilation\n$ gcc -fopennmp Vector-addition-OpenMP-solution.c -o Vector-addition-Solution\n\n// execution \n$ ./Vector-addition-Solution\n\n// output\n$ ./Vector-addition-Solution\n</code></pre> <pre><code>// compilation\n$ gfortran -fopenmp Vector-addition-OpenMP-solution.f90 -o Vector-addition-Solution\n\n// execution\n$ ./Vector-addition-Solution\n\n// output\n$ ./Vector-addition-Solution\n</code></pre> Questions <ul> <li>Can you measure the performance speedup for parallelising loop? Do you see any speedup?</li> <li>For example, can you create more threads to speed up the computation? If yer or not, why?</li> </ul>"},{"location":"openmp/exercise-4/","title":"Work Sharing Constructs(loop-scheduling)","text":""},{"location":"openmp/exercise-4/#loop-scheduling","title":"Loop scheduling","text":"<p>However, the above example is very simple.    Because, in most cases, we would end up doing a large list of arrays with complex computations within the loop.    Therefore, the workloading should be optimally distributed among the threads in those cases.    To handle those considerations, OpenMP has provided the following loop-sharing clauses. They are: <code>Static</code>, <code>Dynamic</code>, <code>Guided</code>, <code>Auto</code>, and <code>Runtime</code>.</p> <p></p> Example - Loop scheduling clauses C/C++FORTRAN <pre><code>#pragma omp parallel for schedule(static)\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n\n//or\n\n#pragma omp parallel \n#pragma omp for schedule(static)\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n</code></pre> <pre><code>!$omp parallel do schedule(static)\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end parallel do\n\n//or\n\n!$omp parallel\n!$omp do schedule(static)\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end do\n!$omp end parallel\n</code></pre>"},{"location":"openmp/exercise-4/#static","title":"Static","text":"<ul> <li>The number of iterations are divided by chunksize. </li> <li>If the chunksize is not provided, a number of iterations will be divided by the size of the team of threads.<ul> <li>e.g., n=100, numthreads=5; each thread will execute the 20 iterations in parallel.</li> </ul> </li> <li>This is useful when the computational cost is similar to each iteration.</li> </ul> Examples and Question: static OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(static)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(static)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre> <pre><code>Thread id           0\nThread id           0\nThread id           4\nThread id           4\nThread id           3\nThread id           3\nThread id           2\nThread id           2\nThread id           1\nThread id           1\n</code></pre> <ul> <li>What happens if you would set the chunksize, for example, <code>schedule(static,4)</code>? What do you notice?</li> </ul>"},{"location":"openmp/exercise-4/#dynamic","title":"Dynamic","text":"<ul> <li>The number of iterations are divided by chunksize.</li> <li>If the chunksize is not provided, the default value will be considered 1.</li> <li>This is useful when the computational cost is different in the iteration.</li> <li>This will quickly place the chunk of data in the queue.</li> </ul> Examples and Question: dynamic OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(dynamic)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(dynamic)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre> <pre><code>Thread id  Thread id 20 Thread id \n4 Thread id 2\nThread id 2\nThread id 2\nThread id 2\nThread id 2\nThread id\nThread id 1 \n3\n</code></pre> <ul> <li>What happens if you would set the chunksize, for example, schedule(dynamic,4)? What do you notice?</li> <li>Do you notice if the iterations are divided by the chunksize that we set?</li> </ul>"},{"location":"openmp/exercise-4/#guided","title":"Guided","text":"<ul> <li>Similar to dynamic scheduling, the number of iterations are divided by chunksize.</li> <li>But the chunk of the data size is decreasing, which is proportional to the number of unsigned iterations divided by the number of threads.</li> <li>If the chunksize is not provided, the default value will be considered 1.</li> <li>This is useful when there is poor load balancing at the end of the iteration.</li> </ul> Examples and Question: guided OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(guided)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(guided)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre> <pre><code>Thread id Thread id   Thread id0 41\nThread id 0\n\nThread id 4\nThread id 4\nThread id 2\nThread id 2\nThread id 3 Thread id\n</code></pre> <ul> <li>Are there any differences between <code>auto</code> and <code>guided</code> or <code>dynamic</code>?</li> </ul>"},{"location":"openmp/exercise-4/#auto","title":"Auto","text":"<ul> <li>Here the compiler chooses the best combination of the chunksize to be used. </li> </ul> Examples and Question: auto OpenMP(C/C++)OpenMP(FORTRAN)Output <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(auto)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(auto)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre> <pre><code>Thread id Thread id Thread id    Thread id0 34 Thread id \nThread id 0\n1\n Thread id 1\n\n Thread id 3\n2\n Thread id 2\n\n Thread id 4\n</code></pre> <ul> <li>What would you choose for your application, auto, dynamic, guided, or static? If you are going to choose either one of them, then have a valid reason. </li> </ul>"},{"location":"openmp/exercise-4/#runtime","title":"Runtime","text":"<ul> <li>During the compilation, we simply set the loop scheduling concept.</li> </ul> Example: Loop scheduling clauses - runtime Compilation <pre><code>setenv OMP_SCHEDULE=\"guided,4\" \nsetenv OMP_SCHEDULE=\"dynamic\" \nsetenv OMP_SCHEDULE=\"nonmonotonic:dynamic,4\"\n// or\nexport OMP_SCHEDULE=\"guided,4\" \nexport OMP_SCHEDULE=\"dynamic\" \nexport OMP_SCHEDULE=\"nonmonotonic:dynamic,4\"\n</code></pre> Examples and Question: runtime OpenMP(C/C++)OpenMP(FORTRAN)Compilation <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(runtime)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(runtime)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre> <pre><code>export OMP_SCHEDULE=\"dynamic,3\"\n// check if you have exported the environment value\n$ env | grep OMP_SCHEDULE\n$ OMP_SCHEDULE=dynamic,3 \n// if you want to unset\n$ unset OMP_SCHEDULE\n$ env | grep OMP_SCHEDULE\n// it(OMP_SCHEDULE=dynamic,3) will be removed\n</code></pre>"},{"location":"openmp/exercise-5/","title":"Worksharing Constructs(others)","text":"<p>Most of the time, we end up having more than one loop, a nested loop, where two or three loops will be next to each other. OpenMP provides a clause for handling this kind of situation with <code>collapse</code>. To understand this, we will now study Matrix multiplication, which involves a nested loop. Again, most of the time, we might do computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future.</p> <p></p>"},{"location":"openmp/exercise-5/#collapse","title":"Collapse","text":"<p>The collapse clause can be used for the nested loop; an entire part of the iteration will be divided by an available number of threads. If the outer loop is equal to the available threads, then the outer loop will be divided by the number of threads. The figure below shows an example of not using the <code>collapse</code> clause. Therefore, only the outer loop is parallelised; each outer loop index will have N number of inner loop iterations. </p> <p></p> <p>This is not what we want. Instead, with the available threads, we would like to parallelise the loops as efficiently as we could. Moreover, most of the time, we might have more threads available on a machine; for example, on MeluXina, we can have up to 256 threads. Therefore, when adding the <code>collapse</code> clause, we notice that the available threads execute every single iteration, as seen in the figure below.</p> <p></p> Collapse C/C++FORTRAN <pre><code>#pragma omp parallel\n#pragma omp for collapse(2)\n  for(int i = 0; i &lt; N; i++)\n     {\n      for(int j = 0; j &lt; N; j++)\n        {     \n         cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n        }\n    }\n\n// Or\n\n#pragma omp parallel for collapse(2)\n  for(int i = 0; i &lt; N; i++)\n    {\n      for(int j = 0; j &lt; N; j++)\n        { \n        cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n        }\n    }\n</code></pre> <pre><code>!$omp parallel\n!$omp do collapse(2) \ndo i = 1, n\n   do j = 1, n\n      print*, 'Thread id', omp_get_thread_num()\n   end do\nend do\n!$omp end do\n!$omp end parallel\n\n!! Or\n\n!$omp parallel do collapse(2)\ndo i = 1, n\n   do j = 1, n\n      print*, 'Thread id', omp_get_thread_num()\n   end do\nend do\n!$omp end parallel do\n</code></pre> Examples and Questions: Collapse OpenMP(C/C++)OpenMP(FORTRAN)Output(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int N=5;\n\n#pragma omp parallel\n#pragma omp for collapse(2)\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n        cout &lt;&lt; \"Outer loop id \" &lt;&lt; i &lt;&lt; \" Inner loop id \"&lt;&lt; j &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n       }\n    }\n\n  return 0;\n}\n</code></pre> <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i, j  \nn=5\n\n!$omp parallel\n!$omp do collapse(2) \ndo i = 1, n\n   do j = 1, n\n      print*, 'Outer loop id ', i , 'Inner loop id ', j , 'Thread id', omp_get_thread_num()\n   end do\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre> <pre><code>Outer loop id            4 Inner loop id            2 Thread id          16\nOuter loop id            1 Inner loop id            4 Thread id           3\nOuter loop id            5 Inner loop id            1 Thread id          20\nOuter loop id            4 Inner loop id            1 Thread id          15\nOuter loop id            2 Inner loop id            1 Thread id           5\nOuter loop id            3 Inner loop id            1 Thread id          10\nOuter loop id            3 Inner loop id            4 Thread id          13\nOuter loop id            4 Inner loop id            4 Thread id          18\nOuter loop id            4 Inner loop id            3 Thread id          17\nOuter loop id            3 Inner loop id            3 Thread id          12\nOuter loop id            1 Inner loop id            2 Thread id           1\nOuter loop id            2 Inner loop id            3 Thread id           7\nOuter loop id            1 Inner loop id            5 Thread id           4\nOuter loop id            2 Inner loop id            2 Thread id           6\nOuter loop id            3 Inner loop id            2 Thread id          11\nOuter loop id            2 Inner loop id            5 Thread id           9\nOuter loop id            3 Inner loop id            5 Thread id          14\nOuter loop id            5 Inner loop id            3 Thread id          22\nOuter loop id            5 Inner loop id            4 Thread id          23\nOuter loop id            5 Inner loop id            5 Thread id          24\nOuter loop id            2 Inner loop id            4 Thread id           8\nOuter loop id            1 Inner loop id            3 Thread id           2\nOuter loop id            4 Inner loop id            5 Thread id          19\nOuter loop id            1 Inner loop id            1 Thread id           0\nOuter loop id            5 Inner loop id            2 Thread id          21\n</code></pre> <ul> <li>Can you add any of the scheduling clauses here, such as static, dynamic, etc.?</li> <li>Is it really necessary to them when you use the <code>collapse</code>, or is it dependent on other factors, such as the nature of the    computation and available threads?</li> </ul>"},{"location":"openmp/exercise-5/#reduction","title":"Reduction","text":"<p>The reduction clauses are data-sharing attribute clauses that can be used to perform some forms of repetition calculations in the parallel region.</p> <ul> <li>it can be used for arithmetic reductions: +,*,-,max,min</li> <li>and also with logical operator reductions in C: &amp; &amp;&amp; | || \u02c6</li> </ul> Reduction C/C++FORTRAN <pre><code>#pragma omp parallel\n#pragma omp for reduction(+:sum)\n  for(int i = 0; i &lt; N; i++)\n     {\n      sum +=a[i];\n     }\n\n// Or\n\n#pragma omp parallel for reduction(+:sum)\n  for(int i = 0; i &lt; N; i++)\n    {\n     sum += a[i];\n    }\n</code></pre> <pre><code>!$omp parallel\n!$omp do reduction(+:sum)\ndo i = 1, n\n   sum = sum + a(i)\nend do\n!$omp end do\n!$omp end parallel\n\n!! Or\n\n!$omp parallel do reduction(+:sum)\ndo i = 1, n\n   sum = sum + a(i)\nend do\n!$omp end parallel do\n</code></pre> Examples and Question: Reduction OpenMP(C/C++)OpenMP(FORTRAN) <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int sum,N = 10;\n  float *a = (float*)malloc(sizeof(float) * N);\n\n#pragma omp parallel for reduction(+:sum)\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = i;\n      sum += a[i];\n    }\n  cout &lt;&lt; \"Sum is \"&lt;&lt; sum &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre> <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n\n  integer :: n, i, sum\n  n=10\n\n  ! Allocate memory for vector\n  allocate(a(n))\n\n  !$omp parallel do reduction(+:sum)\n  do i = 1, n\n      a(i) = i\n      sum = sum + a(i)\n  end do\n  !$omp end parallel do\n\n  print *, 'Sum is ', sum\n\nend program main\n</code></pre> <ul> <li>What happens if you do not use the reduction clause? Do we still get the correct answer?</li> </ul>"},{"location":"openmp/exercise-5/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>In this example, we consider a square matrix; <code>M=N</code> is equal for both <code>A</code> and <code>B</code> matrices. Even though we deal here with a 2D matrix, we create a 1D array to represent a 2D matrix. In this example,  we must use the <code>collapse</code> clause since matrix multiplication deals with 3 loops. The first 2 outer loops will take rows of the <code>A</code> matrix and columns of the <code>B</code> matrix. Therefore, these two loops can be easily parallelised. But then we need to sum the value of those two outer loops value finally; this is where we should use the <code>reduction</code> clause. </p> matrix multiplication function call C/C++FORTRAN <pre><code>for(int row = 0; row &lt; width ; ++row)                           \n   {                                                             \n     for(int col = 0; col &lt; width ; ++col)\n       {\n         sum=0;\n         for(int i = 0; i &lt; width ; ++i)                         \n           {                                                     \n             sum += a[row*width+i] * b[i*width+col];      \n           }                                                     \n         c[row*width+col] = sum;                           \n       }\n   } \n</code></pre> <pre><code>do row = 0, width-1\n   do col = 0, width-1\n      sum=0\n      do i = 0, width-1\n         sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n      enddo\n      c(row*width+col+1) = sum\n   enddo\nenddo\n</code></pre>"},{"location":"openmp/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\n\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   \n{ \n  float sum = 0;\n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)\n        {\n          sum=0;\n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              sum += a[row*width+i] * b[i*width+col];      \n            }                                                     \n          c[row*width+col] = sum;                           \n        }\n    }   \n}\n\nint main()\n {  \n   printf(\"Programme assumes that matrix size is N*N \\n\");\n   printf(\"Please enter the N size number \\n\");\n   int N =0;\n   scanf(\"%d\", &amp;N);\n\n   // Initialize the memory\n   float *a, *b, *c;       \n\n   // Allocate memory\n   a = (float*)malloc(sizeof(float) * (N*N));\n   b = (float*)malloc(sizeof(float) * (N*N));\n   c = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize arrays\n  for(int i = 0; i &lt; (N*N); i++)\n     {\n       a[i] = 1.0f;\n       b[i] = 2.0f;\n     }\n\n   // Fuction call \n   clock_t start = clock();\n   Matrix_Multiplication(a, b, c, N);\n   clock_t end = clock();\n   double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n   printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n   // Verification\n   for(int i = 0; i &lt; N; i++)\n      {\n      for(int j = 0; j &lt; N; j++)\n         {\n      printf(\"%f \", c[j]);\n\n    }\n      printf(\"\\n\");\n      }\n\n    // Deallocate memory\n    free(a); \n    free(b); \n    free(c);\n\n   return 0;\n}\n</code></pre> <pre><code>module Matrix_Multiplication_Mod  \nimplicit none \ncontains\n subroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\n\ndo row = 0, width-1\n   do col = 0, width-1\n      sum=0\n       do i = 0, width-1\n         sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n       enddo\n      c(row*width+col+1) = sum\n   enddo\nenddo\n\n\n  end subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\n\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\n   a(i) = sin(i*1D0) * sin(i*1D0)\n   b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n! Call the vector addition subroutine \ncall Matrix_Multiplication(a, b, c, n)\n\n!!Verification\ndo i=1,n*n\n   print *, c(i)\nenddo\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\n\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   \n{ \n  float sum = 0;\n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)\n        {\n          sum=0;\n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              sum += a[row*width+i] * b[i*width+col];      \n            }                                                     \n          c[row*width+col] = sum;                           \n        }\n    }   \n}\n\nint main()\n {  \n   printf(\"Programme assumes that matrix size is N*N \\n\");\n   printf(\"Please enter the N size number \\n\");\n   int N =0;\n   scanf(\"%d\", &amp;N);\n\n   // Initialize the memory \n   float *a, *b, *c;       \n\n   // Allocate memory\n   a = (float*)malloc(sizeof(float) * (N*N));\n   b = (float*)malloc(sizeof(float) * (N*N));\n   c = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize arrays\n  for(int i = 0; i &lt; (N*N); i++)\n     {\n       a[i] = 1.0f;\n       b[i] = 2.0f;\n     }\n\n   // Function call \n   Matrix_Multiplication(a, b, c, N);\n\n   // Verification\n   for(int i = 0; i &lt; N; i++)\n      {\n      for(int j = 0; j &lt; N; j++)\n         {\n      printf(\"%f \", c[j]);\n\n    }\n      printf(\"\\n\");\n      }\n\n    // Deallocate memory\n    free(a); \n    free(b); \n    free(c);\n\n   return 0;\n}\n</code></pre> <pre><code> module Matrix_Multiplication_Mod  \nimplicit none \ncontains\n subroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\n!!! ADD LOOP PARALLELISATION\ndo row = 0, width-1\n   do col = 0, width-1\n      sum=0\n       do i = 0, width-1\n         sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n       enddo\n      c(row*width+col+1) = sum\n   enddo\nenddo\n\n\n  end subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\n\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\n   a(i) = sin(i*1D0) * sin(i*1D0)\n   b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!!!! ADD PARALLEL REGION \n! Call the vector addition subroutine \ncall Matrix_Multiplication(a, b, c, n)\n\n!!Verification\ndo i=1,n*n\n   print *, c(i)\nenddo\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\n\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   \n{ \n  float sum = 0;\n  #pragma omp parallel for collapse(2) \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)\n        {\n          sum=0;\n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              sum += a[row*width+i] * b[i*width+col];      \n            }                                                     \n          c[row*width+col] = sum;                           \n        }\n    }   \n}\n\nint main()\n {  \n   printf(\"Programme assumes that matrix size is N*N \\n\");\n   printf(\"Please enter the N size number \\n\");\n   int N =0;\n   scanf(\"%d\", &amp;N);\n\n   // Initialize the memory\n   float *a, *b, *c;       \n\n   // Allocate memory\n   a = (float*)malloc(sizeof(float) * (N*N));\n   b = (float*)malloc(sizeof(float) * (N*N));\n   c = (float*)malloc(sizeof(float) * (N*N));\n\n   // Initialize arrays\n   for(int i = 0; i &lt; (N*N); i++)\n      {\n        a[i] = 1.0f;\n        b[i] = 2.0f;\n      }\n   omp_set_num_threads(omp_get_max_threads());\n   // Function call \n   double start = omp_get_wtime();\n   Matrix_Multiplication(a, b, c, N);\n   ouble end = omp_get_wtime();\n   printf(\"Time measured: %.3f seconds.\\n\", end - start);\n\n   // Verification\n   for(int i = 0; i &lt; N; i++)\n      {\n      for(int j = 0; j &lt; N; j++)\n         {\n          printf(\"%f \", c[j]);\n     }\n       printf(\"\\n\");\n      }\n\n   // Deallocate memory\n   free(a); \n   free(b); \n   free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Matrix_Multiplication_Mod  \n  implicit none \ncontains\n  subroutine Matrix_Multiplication(a, b, c, width)\n    use omp_lib\n    ! Input vectors\n    real(8), intent(in), dimension(:) :: a\n    real(8), intent(in), dimension(:) :: b\n    real(8), intent(out), dimension(:) :: c\n    real(8) :: sum = 0\n    integer :: i, row, col, width\n\n    !$omp do collapse(2) reduction(+:sum)\n    do row = 0, width-1\n       do col = 0, width-1\n          sum=0\n          do i = 0, width-1\n             sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n          enddo\n          c(row*width+col+1) = sum\n       enddo\n    enddo\n    !$omp end do\n\n  end subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\n\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\n   a(i) = sin(i*1D0) * sin(i*1D0)\n   b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!$omp parallel\n! Call the vector addition subroutine \ncall Matrix_Multiplication(a, b, c, n)\n!$omp end parallel\n\n!!Verification\ndo i=1,n*n\n   print *, c(i)\nenddo\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> Compilation and Output Serial-version(C/C++)Serial-version(FORTRAN)OpenMP(C/C++)OpenMP(FORTRAN) <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-Serial\n\n// execution \n$ ./Matrix-Multiplication-Serial\n\nProgramme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre> <pre><code>// compilation\n$ gfortran Matrix-multiplication.f90 -o Matrix-Multiplication-Serial\n\n// execution \n$ ./Matrix-Multiplication-Serial\n\nProgramme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre> <pre><code>// compilation\n$ gcc -fopenmp Matrix-multiplication-Solution.c -o Matrix-Multiplication-Solution\n\n// execution\n$ ./Matrix-Multiplication-Solution\nProgramme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre> <pre><code>// compilation\n$ gfortran -fopenmp Matrix-multiplication-Solution.f90 -o Matrix-Multiplication-Solution\n\n// execution\n$ ./Matrix-Multiplication-Solution\nProgramme assumes that the matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre> Questions <ul> <li>Right now, we are dealing with square matrices. Could you write a code for a different matrix size while still fulfilling the matrix multiplication condition?</li> </ul> <p>Could you use any one of the loop scheduling methods, such as <code>dynamic</code> or <code>static</code>? Do you see any performance gain?</p>"},{"location":"openmp/exercise-6/","title":"SIMD and Others","text":"<p>In this exercise, we will try to add the <code>simd</code> classes to our existing problems, for example, vector addition. </p> Examples and Question: SIMD - Vector Addition Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN) <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing vector addition function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n! Call the vector addition subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;omp.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n// ADD YOUR PARALLEL REGION FOR THE LOOP SIMD\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // ADD YOUR PARALLEL REGION HERE  \n  // Executing vector addition function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!! ADD YOUR PARALLEL DO LOOP WITH SIMD\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!! ADD YOUR PARALLEL REGION \n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;omp.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n// ADD YOUR PARALLEL SIMD\n#pragma omp for simd\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  double start = omp_get_wtime();\n  #pragma omp parallel \n  // Executing vector addition function \n  Vector_Add(a, b, c, N);\n  double end = omp_get_wtime();\n  printf(\"Work took %f seconds\\n\", end - start);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre> <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!$omp do simd\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n!$omp end do simd\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ndouble precision :: start, end\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\nstart = omp_get_wtime()\n!$omp parallel \n! Call the vector addition subroutine \ncall Vector_Addition(a, b, c, n)\n!$omp end parallel\nend = omp_get_wtime()\nPRINT *, \"Work took\", end - start, \"seconds\"\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre> <ul> <li>Please try the examples without the <code>simd</code> clause. Do you notice any performance differences? </li> </ul>"},{"location":"openmp/exercise-6/#critical-single-and-master","title":"Critical, Single, and Master","text":"<p>We will explore how single, master and critical are working in the OpenMP programming model. For this, we consider the following simple examples.</p> Examples and Question: Critical, Single and Master (C/C++)FORTRAN) <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\n#include &lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from the master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        cout &lt;&lt; \"Hello world from thread id \"\n        &lt;&lt; omp_get_thread_num() &lt;&lt; \" from the team size of \"\n        &lt;&lt; omp_get_num_threads()\n        &lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from the master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre> <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\n!$omp parallel \nprint *, 'Hello world from thread id ', omp_get_thread_num(), 'from the team size of', omp_get_num_threads()\n!$omp end parallel\n\nend program\n</code></pre> <ul> <li>Try single clause</li> <li>Try master clause</li> <li>Try critical clause</li> </ul>"},{"location":"openmp/preparation/","title":"Preparation","text":""},{"location":"openmp/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"openmp/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example, the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"openmp/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200898\n[u100490@login02 p200898]$ pwd\n/project/home/p200898\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200898]$ mkdir $USER\n### or \n[u100490@login02 p200898]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"openmp/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200898]$cd u100490\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200898/OpenMP .\n[u100490@login03 u100490]$ cd OpenMP/\n[u100490@login03 OpenMP]$ pwd\n/project/home/p200898/u100490/OpenMP\n[u100490@login03 OpenMP]$ ls -lthr\ndrwxr-s---. 2 u100490 p200898 4.0K May 27 21:42 Data-Sharing-Attribute\ndrwxr-s---. 2 u100490 p200898 4.0K May 28 00:35 Parallel-Region\ndrwxr-s---. 2 u100490 p200898 4.0K May 30 18:26 Dry-run-test\n...\n...\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#7-until-now-you-are-in-the-login-node-now-it-is-time-to-do-the-dry-run-test","title":"7. Until now, you are in the login node; now it is time to do the dry run test","text":"<ul> <li> <p>7.1 Reserve the interactive node for running/testing OpenMP applications    <pre><code>$ salloc -A p200898 --res ncc-openmp --partition=cpu --qos default -N 1 -t 01:00:00\n</code></pre></p> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200898 --res ncc-openmp --partition=cpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> <li> <p>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       cpu interact  u100490    p200898  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></p> </li> </ul>"},{"location":"openmp/preparation/#8-now-we-need-to-check-a-simple-openmp-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check a simple OpenMP application if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 OpenMP]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nsource.sh  Test.cc  Test.f90\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#9-finally-we-need-to-load-the-compiler-to-test-our-openmp-codes","title":"9. Finally, we need to load the compiler to test our OpenMP codes","text":"<ul> <li> <p>9.1 We will work with GNU compiler  <pre><code>$ source module.sh\n</code></pre></p> check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module list\n\ncurrently Loaded Modules:\n1) env/release/2022.1                (S)  19) libpciaccess/0.16-GCCcore-11.3.0    37) jbigkit/2.1-GCCcore-11.3.0        55) VTune/2022.3.0                          73) NSS/3.79-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1           (S)  20) X11/20220504-GCCcore-11.3.0         38) gzip/1.12-GCCcore-11.3.0          56) numactl/2.0.14-GCCcore-11.3.0           74) snappy/1.1.9-GCCcore-11.3.0\n3) GCCcore/11.3.0                         21) Arm-Forge/22.0.4-GCC-11.3.0         39) lz4/1.9.3-GCCcore-11.3.0          57) hwloc/2.7.1-GCCcore-11.3.0              75) JasPer/2.0.33-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0             22) libglvnd/1.4.0-GCCcore-11.3.0       40) zstd/1.5.2-GCCcore-11.3.0         58) OpenSSL/1.1                             76) nodejs/16.15.1-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0           23) AMD-uProf/3.6.449                   41) libdeflate/1.10-GCCcore-11.3.0    59) libevent/2.1.12-GCCcore-11.3.0          77) Qt5/5.15.5-GCCcore-11.3.0\n6) ncurses/6.3-GCCcore-11.3.0             24) Advisor/2022.1.0                    42) LibTIFF/4.3.0-GCCcore-11.3.0      60) UCX/1.13.1-GCCcore-11.3.0               78) CubeGUI/4.7-GCCcore-11.3.0\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre> </li> </ul>"},{"location":"openmp/preparation/#10-please-compile-and-test-your-openmp-application","title":"10. Please compile and test your OpenMP application","text":"<ul> <li>10.1 For example, Dry-run-test  <pre><code>// compilation (C/C++)\n$ g++ Test.cc -fopenmp\n\n// compilation (FORTRAN)\n$ gfortran Test.f90 -fopenmp\n\n// execution\n$ ./a.out\n\n// output\n$ Hello world from the master thread \n  Hello world from thread id Hello world from thread id Hello world from thread \n  id Hello world from thread id Hello world from thread id 4 from the team size of \n  1 from the team size of 20 from the team size of  from the team size of 555\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly, for the hands-on session, we need to do the node reservation:","text":"<ul> <li> <p>11.1 For example, reservation   <pre><code>$ salloc -A p200898 --res ncc-openmp --partition=cpu --qos default -N 1 -t 02:30:00\n</code></pre></p> check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200898 --res ncc-openmp --partition=cpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre> </li> </ul>"},{"location":"openmp/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands-on exercise","text":"<ul> <li>12.1 For example, in the <code>Hello World</code> example, we do the following steps:</li> </ul> <pre><code>[u100490@mel2063 OpenMP]$ pwd\n/project/home/p200898/u100490/OpenMP\n[u100490@mel2063 OpenMP]$ ls\n[u100490@mel2063 OpenMP]$ ls\ndrwxr-s---. 2 u100490 p200898 4.0K May 27 21:42 Data-Sharing-Attribute\ndrwxr-s---  2 u100490 p200898 4.0K May 28 00:35 Parallel-Region\ndrwxr-s---  2 u100490 p200898 4.0K May 28 23:45 Worksharing-Constructs-Schedule\ndrwxr-s---. 2 u100490 p200898 4.0K May 29 00:57 Worksharing-Constructs-Other\ndrwxr-s---. 2 u100490 p200898 4.0K May 29 18:07 Worksharing-Constructs-Loop\ndrwxr-s---. 2 u100490 p200898 4.0K May 30 18:25 SIMD-Others\ndrwxr-s---. 2 u100490 p200898 4.0K May 30 18:37 Dry-run-test\n-rw-r-----  1 u100490 p200898  241 May 30 18:41 module.sh\n[u100490@mel2063 OpenMP]$ source module.sh\n</code></pre>"},{"location":"openmp/profiling/","title":"Profiling and Performance","text":"<p>Profiling is an important task to be considered when a computer code is written. Writing parallel code is less challenging, but making it more efficient on a given parallel architecture is challenging. Moreover,  from the programming and programmer\u2019s perspective, we want to know where the code spends most of its time. In particular, we would like to know if the code (given algorithm) is compute bound, memory bound, cache misses, memory leak, proper vectorisation, cache misses, register spilling, or hot spot (time-consuming part in the code). Plenty of tools are available to profile a scientific code (computer code for doing arithmetic computing using processors). However, we will focus on a few of the widely used tools.</p> <ul> <li>AMD uProf</li> <li>ARM Forge</li> <li>Intel tools</li> </ul>"},{"location":"openmp/profiling/#arm-forge","title":"ARM Forge","text":"<p>Arm Forge is another standard commercial tool for debugging, profiling, and analysing scientific code on the massively parallel computer architecture. They have a separate toolset for each category with the common environment: DDT for debugging, MAP for profiling, and performance reports for analysis. It also supports the MPI, UPC, CUDA, and OpenMP programming models for different architectures with a variety of compilers. DDT and MAP will launch the GUI, where we can interactively debug and profile the code. Meanwhile, <code>perf-report</code> will provide the analysis results in <code>.html</code> and <code>.txt</code> files.</p> Example: ARM Forge C/C++FORTRAN <pre><code># compilation with debugging tool\n$ gcc test.c -g -fopenmp\n# execute and profile the code\n$ map --profile --no-mpi ./a.out\n# open the profiled result in GUI\n$ map xyz.map\n\n# for debugging\n$ ddt ./a .out\n\n# for profiling\n$ map ./a .out\n\n# for analysis\n$ perf-report ./a .out\n</code></pre> <pre><code># compilation \n$ gfortran test.f90 -fopenmp\n# execute and profile the code\n$ map --profile --no-mpi ./a.out\n# open the profiled result in GUI\n$ map xyz.map\n\n# for debugging\n$ ddt ./a .out\n\n# for profiling\n$ map ./a .out\n\n# for analysis\n$ perf-report ./a .out\n</code></pre> <p> </p>"},{"location":"openmp/profiling/#intel-tools","title":"Intel tools","text":""},{"location":"openmp/profiling/#intel-application-snapshot","title":"Intel Application Snapshot","text":"<p>Intel Application Performance Snapshot tool helps to find essential performance factors and the metrics of CPU utilisation, memory access efficiency, and vectorisation. <code>aps -help</code> will list out profiling metrics options in APS</p> <p></p> Example: APS C/C++FORTRAN <pre><code># compilation\n$ icc -qopenmp test.c\n\n# code execution\n$ aps --collection-mode=all -r report_output ./a.out\n$ aps-report -g report_output                        # create a .html file\n$ firefox report_output_&lt;postfix&gt;.html               # APS GUI in a browser\n$ aps-report report_output                           # command line output\n</code></pre> <pre><code># compilation\n$ ifort -qopenmp test.f90\n\n# code execution\n$ aps --collection-mode=all -r report_output ./a.out\n$ aps-report -g report_output                        # create a .html file\n$ firefox report_output_&lt;postfix&gt;.html               # APS GUI in a browser\n$ aps-report report_output                           # command line output\n</code></pre> <p> </p>"},{"location":"openmp/profiling/#intel-inspector","title":"Intel Inspector","text":"<p>Intel Inspector detects and locates the memory, deadlocks, and data races in the code. For example, memory access and memory leaks can be found.</p> Example: Intel Inspector C/C++FORTRAN <pre><code># compile the code\n$ icc -qopenmp example.c\n# execute and profile the code\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n$ cat inspxe-cl.txt\n# open the file to see if there is any memory leak\n=== Start: [2020/12/12 01:19:59] ===\n0 new problem(s) found\n=== End: [2020/12/12 01:20:25] ===\n</code></pre> <pre><code># compile the code\n$ ifort -qopenmp test.f90\n# execute and profile the code\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n$ cat inspxe-cl.txt\n# open the file to see if there is any memory leak\n=== Start: [2023/05/10 01:19:59] ===\n0 new problem(s) found\n=== End: [2020/05/10 01:20:25] ===\n</code></pre>"},{"location":"openmp/profiling/#intel-advisor","title":"Intel Advisor","text":"<p>Intel Advisor: a set of collection tools for the metrics and traces that can be used for further tuning in the code. <code>survey</code>: analyse and explore an idea about where to add efficient vectorisation.</p> Example: Intel Advisor C/C++FORTRAN <pre><code># compile the code\n$ icc -qopenmp test.c\n# collect the survey metrics\n$ advixe-cl -collect survey -project-dir result -- ./a.out\n# collect the report\n$ advixe-cl -report survey -project-dir result\n# open the gui for report visualization\n$ advixe-gui\n</code></pre> <pre><code># compile the code\n$ ifort -qopenmp test.90\n# collect the survey metrics\n$ advixe-cl -collect survey -project-dir result -- ./a.out\n# collect the report\n$ advixe-cl -report survey -project-dir result\n# open the gui for report visualization\n$ advixe-gui\n</code></pre> <p> </p>"},{"location":"openmp/profiling/#intel-vtune","title":"Intel VTune","text":"<ul> <li>Identify the time-consuming part of the code.</li> <li>Also, identify cache misses and latency.</li> </ul> Example: Intel VTune C/C++FORTRAN <pre><code># compile the code\n$ icc -qopenmp test.c\n# execute the code and collect the hotspots\n$ amplxe-cl -collect hotspots -r amplifier_result ./a.out\n$ amplxe-gui\n# open the GUI of the VTune amplifier\n</code></pre> <pre><code># compile the code\n$ ifort -qopenmp test.90\n# execute the code and collect the hotspots\n$ amplxe-cl -collect hotspots -r amplifier_result ./a.out\n$ amplxe-gui\n# open the GUI of the VTune amplifier\n</code></pre> <p><code>amplxe-cl</code> will list out the analysis types and <code>amplxe-cl -hlep</code> report will list out available reports in VTune.</p>"},{"location":"openmp/profiling/#amd-uprof","title":"AMD uProf","text":"<p>AMD uProf profiler follows a statistical sampling-based approach to collect profile data to identify the performance bottlenecks in the application.</p> Example: AMD uProf C/C++FORTRAN <pre><code># compile the code\n$ clang -fopenmp test.c\n$ AMDuProfCLI collect --trace openmp --config tbp --output-dir solution ./a.out -d 1\n</code></pre> <pre><code># compile the code\n$ flang -fopenmp test.90\n$ AMDuProfCLI collect --trace openmp --config tbp --output-dir solution ./a.out -d 1\n</code></pre>"},{"location":"other-training/othertraining/","title":"Other Training Events","text":""},{"location":"other-training/othertraining/#high-performance-computing-take-part-in-the-gray-scott-summer-school","title":"High Performance Computing: take part in the Gray Scott Summer School!","text":"<p>From 1 to 12 July 2024, the Laboratoire d'Annecy De Physique Des Particules (LAPP), in collaboration with the CC-FR Competence Centre is organising a free HPC training course on programming and optimisation on heterogeneous architectures, with one week dedicated to CPU and one to GPU.</p> <p>\ud83d\udc49 You can find the full program schedule here </p> <p> Do you want to develop your skills? Join the second Gray Scott Summer School on-site in Annecy (France) by registering here, there are still a few spots available: https://indico.in2p3.fr/event/30939/.</p> <p>You can also follow the HPC training course, either intensively or on demand, from your home or organisation via live streaming on  (registration required to receive the link)</p>"},{"location":"other-training/othertraining/#please-note-that-the-training-is-available-only-in-french","title":"Please note that the training is available only in French","text":"<p> For more information about the Gray Scott Summer School  </p>"},{"location":"python/","title":"Introduction to use Python in the HPC","text":"<p>In this workshop, we will explore the process of improving Python code for efficient execution. Chances are, you're already familiar with Python and Numpy. However, we will start by mastering profiling and efficient NumPy usage as these are crucial steps before venturing into parallelization. Once your code is fine-tuned with Numpy we will explore the utilization of Python's parallel libraries to unlock the potential of using multiple CPU cores. By the end, you will be well equipped to harness Python's potential for high-performance tasks on the HPC infrastructure.</p>"},{"location":"python/#target-audience-description","title":"Target Audience Description","text":"<p>The workshop is designed for individuals who are interested in advancing their skills and knowledge in Python-based scientific and data computing. The ideal participants would typically possess basic to intermediate Python and Numpy skills, along with some familiarity with parallel programming. This workshop will give a good starting point to leverage the usage of the HPC computing power to speed up your Python programs.</p>"},{"location":"python/#agenda","title":"Agenda","text":""},{"location":"python/#first-day-using-jupyter-notebook-on-hpc-infrastructure-profiling-and-using-numpy-effectively","title":"First day: Using Jupyter notebook on HPC infrastructure, profiling and using Numpy effectively","text":"<ul> <li>Setting up a Jupyter notebook on an HPC node</li> <li>Taking time and profiling python code</li> <li>Numpy basics for replacing python loops for efficient computations</li> </ul>"},{"location":"python/#second-day-improving-performance-with-python-parallel-packages","title":"Second day: Improving performance with python parallel packages","text":"<ul> <li>Use case understanding and Python implementation</li> <li>Numpy implementation</li> <li>Python\u2019s Multiprocessing</li> <li>PyMP</li> <li>Cython</li> <li>Numba and final remarks</li> </ul>"},{"location":"python/#requirements","title":"Requirements","text":"<ul> <li>Having an HPC account to access the cluster.</li> <li>Basic knowledge on SLURM.</li> <li>A basic understanding of Python programming.</li> <li>Familiarity with Jupyter Notebook (installed and configured).</li> <li>A basic understanding of Numpy and linear algebra.</li> <li>Familiarity with parallel programming.</li> </ul>"},{"location":"python/#course-organization-and-registration","title":"Course Organization and Registration","text":"<p>Format - Online  Past Event: July, 2024  Next Event: Two days - 30<sup>th</sup> (3h morning) - 31<sup>st</sup> (3h morning), January, 2025</p> <p>Registration - 30-31<sup>st</sup>, January, 2025</p>"}]}