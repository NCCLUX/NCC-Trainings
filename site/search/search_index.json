{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Trainings organization","text":""},{"location":"#supercomputing-luxembourg-and-eurocc-eurohpc-joint-undertaking","title":"<p>SuperComputing Luxembourg and EuroCC (EuroHPC Joint Undertaking)</p>","text":"<p>We have categorized our training events into three:</p>"},{"location":"#courses","title":"Courses:","text":"<p>Courses are primarily designed for just one day. It is mainly considered an effective way of learning rather than focusing on more than one training day. Courses are mainly focused on parallel programming on CPUs and GPUs and acceleration of your scientific software on supercomputers. For example, OpenMP, MPI, OpenACC, CUDA, and OpenMP Offloading using programming languages such as C/C++ and FORTRAN. Plus, also we focus on Python, R, Matlab and Julia to parallelize your application. Regarding the software acceleration, GROMACS, CFD and FEM software, material science and bioinformatics, etc., will be offered. </p> <ul> <li>The one-day training consists of three stages:<ul> <li>preparation (pre-preparation to follow up on the lecture and practicals)</li> <li>lecture (introduction and focus on the course topic)</li> <li>practicals (hands-on session)</li> </ul> </li> </ul>"},{"location":"#bootcamps","title":"Bootcamps:","text":"<p>Bootcamps are usually one- or two-day events designed to teach scientists and researchers how to start quickly accelerating codes on modern processors (for example, GPUs). Participants will be introduced to available libraries, programming models, and platforms. They will learn the basics of parallel (CPU, GPU and hybrid) programming through extensive hands-on collaboration based on real-life codes using the parallel programming model.</p>"},{"location":"#hackathons","title":"Hackathons:","text":"<p>Hackathons are dedicated for a longer duration; it could be up to one or two months. During the hackathons, participants mainly focus on the HPC problems (HPC, AI and HPDA) that come from industry, particularly local Luxembourg industries. Participants will be grouped into many and each group member will get mentors from organizers (for example, Nvidia) and also from experts from supercomputing NCC Luxembourg. At the end of the event, there will also be a winner and prize for outstanding contribution. </p>"},{"location":"Bootcamps/ai/handson/","title":"Hands-on","text":""},{"location":"Bootcamps/ai/handson/#materials-and-timeline","title":"Materials and timeline","text":"<p>This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email.</p> <p>Day 1 \u2013 Thursday, February 9th 2023: 01:30 PM \u2013 05:00 PM</p>     <pre><code>01:30 PM \u2013 01:45 PM: Welcome (Moderator)\n01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture)\n02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture)\n03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab)\n</code></pre> <p>Day 2 \u2013 Friday, February 10th  2023: 01:30 PM \u2013 05:00 PM</p>     <pre><code>01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge)\n04:45 PM \u2013 05:00 PM: Wrap up and QA\n</code></pre>"},{"location":"Bootcamps/ai/handson/#further-more-information-can-be-found-in-here","title":"Further more information can be found in here!","text":""},{"location":"Bootcamps/ai/introduction/","title":"Introduction","text":"<p>The NCC Supercompuing Luxembourg, in collaboration with NVIDIA  and OpenACC.org, is hosting online the AI for Science and Engineering Bootcamp during 2 half-days. The first part will be dedicated to theory, and the second part will focus on hands-on challenges on GPU accelerators of the MeluXina supercomputer. For whom? </p> <p>Both current or prospective users of large hybrid CPU/GPU clusters, which develop HPC and AI applications and could benefit from GPU acceleration, are encouraged to participate! What will you learn and how? </p> <p>During this online Bootcamp, participants will learn how to apply AI tools, techniques, and algorithms to real-life problems. Participants will be introduced to the critical concepts of Deep Neural Networks, how to build Deep Learning models, and how to measure and improve the accuracy of their models. Participants will also learn essential data pre-processing techniques to ensure a robust machine-learning pipeline. The Bootcamp is a hands-on learning experience where mentors guide participants. Learning outcomes</p>"},{"location":"Bootcamps/ai/introduction/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<pre><code>- Apply Deep Convolutional Neural Networks for science and engineering applications\n- Understand the Classification (multi-class classification) methodology in AI\n- Implement AI algorithms using Keras (e.g. TensorFlow)\n- Use an efficient usage of the GPU for AI algorithms (e.g. CNN) with handling large data set\n- Run AI applications in the Jupyter notebook environment (and understand singularity containers)\n</code></pre>"},{"location":"Bootcamps/ai/introduction/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with basic experience with Python. No GPU programming knowledge is required. GPU Compute Resource</p> <p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer\u202fduring the hackathon. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide. Agenda</p>"},{"location":"Bootcamps/ai/introduction/#materials-and-timeline","title":"Materials and timeline","text":"<p>This 2-day Bootcamp will be hosted online (CET time). All communication will be done through Zoom, Slack and email.</p> <p>Day 1 \u2013 Thursday, February 9th 2023: 01:30 PM \u2013 05:00 PM</p>     <pre><code>01:30 PM \u2013 01:45 PM: Welcome (Moderator)\n01:45 PM \u2013 02:30 PM: Introduction to GPU computing (Lecture)\n02:30 PM \u2013 03:30 PM: Introduction to AI (Lecture)\n03:30 PM \u2013 05:00 PM: CNN Primer and Keras (hands-on lab)\n</code></pre> <p>Day 2 \u2013 Friday, February 10th  2023: 01:30 PM \u2013 05:00 PM</p>     <pre><code>01:30 PM \u2013 04:45 PM: Tropical cycle detection (challenge)\n04:45 PM \u2013 05:00 PM: Wrap up and QA\n</code></pre>"},{"location":"Bootcamps/ai/preparation/","title":"Preparation","text":""},{"location":"Bootcamps/ai/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>Please take a look if you are using Windows</li> <li>Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"Bootcamps/ai/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<p>For exmaple the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n</code></pre></p>"},{"location":"Bootcamps/ai/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory     <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that go to project directory (Nvidia Bootcamp activites).    <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"Bootcamps/ai/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory-for-example-here-it-is-user-with-u100490","title":"4. And please create your own working folder under the project directory, for example, here it is user with <code>u100490</code>:","text":"<pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre> <ul> <li> <p>4.1 Now copy climate.simg and climate.sh from project direcoty to your user directory (for exxample, here is <code>u100490</code>) directory:    <pre><code>[u100490@login02 p200117]$ cp /project/home/p200117/climate.simg /project/home/p200117/u100490\n[u100490@login02 p200117]$ cp /project/home/p200117/climate.sh /project/home/p200117/u100490\n</code></pre></p> </li> <li> <p>4.2 Similary, copy cfd.simg and cfd.sh from project direcoty to your user directory (for example, here is <code>u100490</code>) directory:    <pre><code>[u100490@login02 p200117]$ cp /project/home/p200117/cfd.simg /project/home/p200117/u100490\n[u100490@login02 p200117]$ cp /project/home/p200117/cfd.sh /project/home/p200117/u100490\n</code></pre></p> </li> <li>4.3 Now go to your home (for example, here it is <code>u100490</code>) directory check if all the necessary files are there (.simg and .sh)    <pre><code>[u100490@login02 p200117]$ cd u100490\n[u100490@login02 u100490]$ pwd\n[u100490@login02 u100490]$ /project/home/p200117/u100490\n[u100490@login02 u100490]$ ls -lthr\ntotal 15G\n-rw-r-x---. 1 u100490 p200117  736 Feb  8 18:59 climate.sh\n-rwxr-x---. 1 u100490 p200117 7.2G Feb  8 19:19 climate.simg\n-rwxr-x---. 1 u100490 p200117 6.9G Feb  8 19:21 cfd.simg\n-rw-r-x---. 1 u100490 p200117  723 Feb  8 19:21 cfd.sh\n</code></pre></li> </ul>"},{"location":"Bootcamps/ai/preparation/#5-for-the-dry-run-9th-february-from-1130-1230-please-follow-the-following-steps","title":"5. For the dry run (9th February from 11:30-12:30), please follow the following steps:","text":"<pre><code>[u100490@login02 u100490]$ salloc -A p200117 --res gpudev -q dev -N 1 -t 01:00:0\n[u100490@mel2123 u100490]$ mkdir -p $PROJECT/$USER/workspace-climate\n[u100490@mel2123 u100490]$ module load Singularity-CE/3.10.2-GCCcore-11.3.0\n\n[u100490@mel2123 u100490]$ singularity run --bind $PROJECT/$USER $PROJECT/$USER/climate.simg cp -rT /workspace $PROJECT/$USER/workspace-climate\nINFO:    Converting SIF file to temporary sandbox...\nINFO:    Cleaning up image...\n[u100490@mel2123 u100490]$ singularity run --nv --bind $PROJECT/$USER $PROJECT/$USER/climate.simg jupyter lab --notebook-dir=$PROJECT/$USER/workspace-climate/python/jupyter_notebook --port=8888 --ip=0.0.0.0 --no-browser --NotebookApp.token=\"\"\nINFO:    Converting SIF file to temporary sandbox...\nWARNING: underlay of /usr/bin/nvidia-smi required more than 50 (452) bind mounts\n[W 10:10:32.723 LabApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n[I 10:10:33.043 LabApp] jupyter_tensorboard extension loaded.\n[I 10:10:33.047 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.8/dist-packages/jupyterlab\n[I 10:10:33.047 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab\n[I 10:10:33.048 LabApp] [Jupytext Server Extension] NotebookApp.contents_manager_class is (a subclass of) jupytext.TextFileContentsManager already - OK\n[I 10:10:33.048 LabApp] Serving notebooks from local directory: /mnt/tier2/project/p200117/u100490/workspace-climate/python/jupyter_notebook\n[I 10:10:33.048 LabApp] Jupyter Notebook 6.2.0 is running at:\n[I 10:10:33.048 LabApp] http://hostname:8888/\n[I 10:10:33.049 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre> <ul> <li> <p>5.1 Now open a new terminal on your local computer, and again login to MeluXina to access the port</p> </li> <li> <p>5.2 Make sure you use the name NODELIST (for example, here it is <code>mel2123</code> - from <code>squeue</code> command you will get this number)    <pre><code>ssh -L8080:NODELIST:8888 USERNAME@login.lxp.lu -p 8822\nssh -L8080:mel2123:8888 u100490@login.lxp.lu -p 8822\n</code></pre></p> </li> <li> <p>5.3 Keep those terminals open/alive (please do not close them)</p> </li> <li> <p>5.4 Now copy and paste localhost to your browser either to Chrome or FireFox    <pre><code>http://localhost:8080\n</code></pre></p> </li> </ul>"},{"location":"Bootcamps/ai/preparation/#6-for-the-afternoon-session-9th-and-10th-february","title":"6. For the afternoon session (9th and 10th February)","text":"<p>If have missed the dry run session, please go through the steps from 1-4</p> <ul> <li>6.1 Now it is time to edit your batch script (climate.sh) before launching your Jupyter notebook, please follow the following steps:     <pre><code>[u100490@login02 u100490]$ emacs(emacs -nw)/vim climate.sh\n#!/bin/bash -l\n#SBATCH --partition=gpu \n#SBATCH --ntasks=1\n#SBATCH --nodes=1    \n############  day one ##########\n#######SBATCH --time=02:00:00         ## use this option for day one\n#######SBATCH --res ai_bootcamp_day1   ## use this option for day one\n################################\n\n############  day two ##########\n#SBATCH --time=03:30:00         ## use this option for day two\n#SBATCH --res ai_bootcamp_day2  ## use this option for day two\n################################\n#SBATCH -A p200117\n#SBATCH --qos default\n\nmkdir -p $PROJECT/$USER/workspace-climate\nmodule load Singularity-CE/3.10.2-GCCcore-11.3.0\n\nsingularity run --bind $PROJECT/$USER $PROJECT/$USER/climate.simg cp -rT /workspace $PROJECT/$USER/workspace-climate\nsingularity run --nv --bind $PROJECT/$USER $PROJECT/$USER/climate.simg jupyter lab --notebook-dir=$PROJECT/$USER/workspace-climate/python/jupyter_notebook --port=8888 --ip=0.0.0.0 --no-browser --NotebookApp.token=\"\"\n</code></pre></li> <li>6.2 Once you have modified your climate.sh, please launch your batch script as below:    <pre><code>[u100490@login03 u100490]$ sbatch climate.sh\nSubmitted batch job 276009\n[u100490@login03 u100490]$ squeue \nJOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n276009       gpu climate.  u100490    p200117  RUNNING       0:16        20:00      1 mel2077\n</code></pre></li> <li>6.3 Now you have initiated your singularity container which will help you to open the Jupyter nootebook    <pre><code>[u100490@login03 u100490]$ ls -lthr\ntotal 7.2G\n-rwxr-x---. 1 u100490 p200117 7.2G Feb  3 14:53 climate.simg\n-rw-r-----. 1 u100490 p200117  613 Feb  3 17:06 climate.sh\n-rw-r-x---. 1 u100490 p200117  724 Feb  8 19:41 cfd.sh\n-rwxr-x---. 1 u100490 p200117 6.9G Feb  8 19:42 cfd.simg\n-rw-r--r--. 1 u100490 p200117 1.1K Feb  3 17:58 slurm-276009.out\n</code></pre></li> <li>6.4 You can also check meantime if everything OK by executing the below command and you should get similar output:    <pre><code>[u100490@login03 u100490]$ head -30 slurm-276009.out \nINFO:    Converting SIF file to temporary sandbox...\nINFO:    Cleaning up image...\nINFO:    Converting SIF file to temporary sandbox...\nWARNING: underlay of /usr/bin/nvidia-smi required more than 50 (452) bind mounts\n[W 17:58:37.489 LabApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\n[I 17:58:37.807 LabApp] jupyter_tensorboard extension loaded.\n[I 17:58:37.811 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.8/dist-packages/jupyterlab\n[I 17:58:37.811 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab\n[I 17:58:37.813 LabApp] [Jupytext Server Extension] NotebookApp.contents_manager_class is (a subclass of) jupytext.TextFileContentsManager already - OK\n[I 17:58:37.813 LabApp] Serving notebooks from local directory: /mnt/tier2/project/p200117/u100490/workspace-climate/python/jupyter_notebook\n[I 17:58:37.813 LabApp] Jupyter Notebook 6.2.0 is running at:\n[I 17:58:37.813 LabApp] http://hostname:8888/\n[I 17:58:37.813 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre></li> <li>6.5 Now open a new terminal on your local computer, and again login to MeluXina to access the port</li> <li>6.6 Make sure you use the name NODELIST (here it is <code>mel2077</code> - from <code>squeue</code> command you will get this number)    <pre><code>ssh -L8080:NODELIST:8888 USERNAME@login.lxp.lu -p 8822\nssh -L8080:mel2077:8888 u100490@login.lxp.lu -p 8822\n</code></pre></li> <li>6.7 Keep those terminals open/alive (please do not close them)</li> <li>6.8 Now copy and paste localhost to your browser either to Chrome or FireFox    <pre><code>http://localhost:8080\n</code></pre></li> </ul>"},{"location":"Hackathons/hpda/introduction/","title":"Introduction","text":""},{"location":"Hackathons/hpda/introduction/#hpda-with-nvidia-and-ceratizit-2024","title":"HPDA with (Nvidia and CERATIZIT - 2024)","text":""},{"location":"calender/future/","title":"Future Events","text":""},{"location":"calender/future/#julia-october-xx-2023","title":"Julia (October xx 2023)","text":""},{"location":"calender/future/#deep-learning-october-xx-2023","title":"Deep Learning (October xx 2023)","text":""},{"location":"calender/future/#quantum-expresso-november-xx-2023","title":"Quantum Expresso (November xx 2023)","text":""},{"location":"calender/future/#nvidia-hackathon-xx-xx-2024","title":"NVIDIA Hackathon (xx xx 2024)","text":""},{"location":"calender/past/","title":"Past Events","text":""},{"location":"calender/past/#nvidia-bootcamp-aiml","title":"Nvidia Bootcamp: AI/ML","text":""},{"location":"calender/past/#cuda-march-27th-2023","title":"CUDA (March 27th 2023)","text":""},{"location":"calender/past/#openmp-may-31st-2023","title":"OpenMP (May 31st 2023)","text":""},{"location":"calender/past/#openacc-september-19th-2023","title":"OpenACC (September 19th 2023)","text":""},{"location":"cuda/","title":"Introduction to GPU programming using CUDA","text":"<p>Participants from this course will learn GPU programming using the CUDA programming model, such as synchronisation, memory allocation and device and host calls. Furthermore, understanding the GPU architecture and how parallel threads blocks are used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the CUDA programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the CUDA programming model with mentors\u2019 guidance later in the hands-on tutorial part.</p>"},{"location":"cuda/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"cuda/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture</li> <li>Threads blocks</li> </ul> </li> <li>Implement CUDA programming model<ul> <li>Programming structure</li> <li>Device calls (threads block organisation)</li> <li>Host calls</li> </ul> </li> <li>Efficient handling of memory management<ul> <li>Host to Device</li> <li>Unified memory</li> </ul> </li> <li>Apply the CUDA programming knowledge to accelerate examples from science and engineering<ul> <li>Iterative solvers from science and engineering</li> <li>Matrix multiplication, vector addition, etc</li> </ul> </li> </ul>"},{"location":"cuda/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++. No GPU programming knowledge is required. However, knowing some basic parallel programming concepts are advantage but not necessary. </p>"},{"location":"cuda/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"cuda/exercise-1/","title":"Hello World","text":"<p>Now our first exercise would be to print out the hello world from GPU. To do that, we need to do the following things:</p> <ul> <li>Run a part or entire application on the GPU</li> <li>Call cuda function on device</li> <li>It should be called using function qualifier <code>__global__</code></li> <li>Calling the device function on the main program:</li> <li>C/C++ example, <code>c_function()</code></li> <li>CUDA example, <code>cuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;()</code> (just using 1 thread)</li> <li><code>&lt;&lt;&lt; &gt;&gt;&gt;</code>, specify the threads blocks within the bracket</li> <li>Make sure to synchronize the threads</li> <li><code>__syncthreads()</code> synchronizes all the threads within a thread block</li> <li><code>CudaDeviceSynchronize()</code> synchronizes a kernel call in host</li> <li>Most of the CUDA APIs are synchronized calls by default (but sometimes    it is good to call explicit synchronized calls to avoid errors    in the computation)</li> </ul>"},{"location":"cuda/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-versionCUDA-version   <pre><code>//-*-C++-*-\n// Hello-world.c\n\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\nvoid c_function()\n{\n  printf(\"Hello World!\\n\");\n}\n\nint main()\n{\n  c_function();\n  return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n// Hello-world.cu\n\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\n// device function will be executed on device (GPU) \n__global__ void cuda_function()\n{\n  printf(\"Hello World from GPU!\\n\");\n\n  // synchronize all the threads\n  __syncthreads();\n}\n\nint main()\n{\n  // call the kernel function \n  cuda_function&lt;&lt;&lt;1,1&gt;&gt;&gt;();\n\n  // synchronize the device kernel call\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre>      Compilation and Output Serial-versionCUDA-version   <pre><code>// compilation\n$ gcc Hello-world.c -o Hello-World-CPU\n\n// execution \n$ ./Hello-World-CPU\n\n// output\n$ Hello World from CPU!\n</code></pre>   <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n</code></pre>      Questions <p>Right now, you are printing just one <code>Hello World from GPU</code>, but what if you would like to print more <code>Hello World from GPU</code>? How can you do that?</p> QuestionAnswerSolution Output   <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\n__global__ void cuda_function()\n{\n  printf(\"Hello World from GPU!\\n\");\n  __syncthreads();\n}\n\nint main()\n{\n  // define your thread block here\n  cuda_function&lt;&lt;&lt;&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n#include&lt;stdio.h&gt;\n#include&lt;cuda.h&gt;\n\n__global__ void cuda_function()\n{\n  printf(\"Hello World from GPU!\\n\");\n  __syncthreads();\n}\n\nint main()\n{\n  // define your thread block here\n  cuda_function&lt;&lt;&lt;10,1&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre>   <pre><code>Hello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\nHello World from GPU!\n</code></pre>"},{"location":"cuda/exercise-2/","title":"Vector Addition","text":"<p>In this example, we will continue with vector addition in GPU using the CUDA programming model. This is an excellent example to begin with because we usually need to do some arithmetic operations using matrices or vectors. For that, we need to know how to access the indexes of the matrix or vector to do the computation efficiently. In this example, we will practice SIMT computation by adding two vectors.</p> <ul> <li>Memory allocation on both CPU and GPU. Because as discussed before,    GPU is an accelerator and can not act as a host machine. So therefore, the computation    has to be initiated via CPU. That means, we need to first initialise the data on the host,    that is CPU. At the same time, we also need to initialise the memory allocation on the GPU.    Because, we need to transfer the data from a CPU to GPU.</li> </ul>     <ul> <li> <p>Allocating the CPU memory for a, b, and out vector <pre><code>// Initialize the memory on the host\nfloat *a, *b, *out;\n\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * N);\nb   = (float*)malloc(sizeof(float) * N);\nout   = (float*)malloc(sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for d_a, d_b, and d_out matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_out;\n\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_b, sizeof(float) * N);\ncudaMalloc((void**)&amp;d_out, sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the     arrays a and b.  <pre><code>// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n  {\n    a[i] = 1.0f;\n    b[i] = 2.0f;\n  }\n</code></pre></p> </li> <li> <p>Transfer initialized value from CPU to GPU <pre><code>// Transfer data from host to device memory\ncudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>Creating a 2D thread block <pre><code>// Thread organization \ndim3 dimGrid(1, 1, 1);    \ndim3 dimBlock(16, 16, 1); \n</code></pre></p>  Conversion of thread blocks <pre><code>//1D grid of 1D blocks\n__device__ int getGlobalIdx_1D_1D()\n{\n  return blockIdx.x * blockDim.x + threadIdx.x;\n}\n\n\n//1D grid of 2D blocks\n__device__ int getGlobalIdx_1D_2D()\n{\n  return blockIdx.x * blockDim.x * blockDim.y\n      + threadIdx.y * blockDim.x + threadIdx.x;\n}\n\n//1D grid of 3D blocks\n__device__ int getGlobalIdx_1D_3D()\n{\n  return blockIdx.x * blockDim.x * blockDim.y * blockDim.z \n    + threadIdx.z * blockDim.y * blockDim.x\n    + threadIdx.y * blockDim.x + threadIdx.x;\n}            \n\n//2D grid of 1D blocks \n__device__ int getGlobalIdx_2D_1D()\n{\n  int blockId   = blockIdx.y * gridDim.x + blockIdx.x;\n  int threadId = blockId * blockDim.x + threadIdx.x; \n  return threadId;\n}\n\n//2D grid of 2D blocks  \n __device__ int getGlobalIdx_2D_2D()\n{\n  int blockId = blockIdx.x + blockIdx.y * gridDim.x; \n  int threadId = blockId * (blockDim.x * blockDim.y) +\n    (threadIdx.y * blockDim.x) + threadIdx.x;\n  return threadId;\n}\n\n//2D grid of 3D blocks\n__device__ int getGlobalIdx_2D_3D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x; \n  int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n   + (threadIdx.z * (blockDim.x * blockDim.y))\n   + (threadIdx.y * blockDim.x)\n   + threadIdx.x;\n  return threadId;\n}\n\n//3D grid of 1D blocks\n__device__ int getGlobalIdx_3D_1D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x \n    + gridDim.x * gridDim.y * blockIdx.z; \n  int threadId = blockId * blockDim.x + threadIdx.x;\n  return threadId;\n}\n\n//3D grid of 2D blocks\n__device__ int getGlobalIdx_3D_2D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x \n    + gridDim.x * gridDim.y * blockIdx.z; \n  int threadId = blockId * (blockDim.x * blockDim.y)\n    + (threadIdx.y * blockDim.x)\n    + threadIdx.x;\n  return threadId;\n}\n\n//3D grid of 3D blocks\n__device__ int getGlobalIdx_3D_3D()\n{\n  int blockId = blockIdx.x \n    + blockIdx.y * gridDim.x \n    + gridDim.x * gridDim.y * blockIdx.z; \n  int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)\n    + (threadIdx.z * (blockDim.x * blockDim.y))\n    + (threadIdx.y * blockDim.x)\n    + threadIdx.x;\n  return threadId;\n</code></pre>  </li> <li> <p>Calling the kernel function <pre><code>// execute the CUDA kernel function \nvector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n</code></pre></p> </li> <li> <p>Vector addition kernel function call definition</p>  vector addition function call Serial-versionCUDA-version   <pre><code>// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      out[i] = a[i] + b[i];\n    }\n  return out;\n}\n</code></pre>   <pre><code>// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n       float *out, int n) \n{\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n  __syncthreads();\n}\n</code></pre>     </li> </ul>     <ul> <li> <p>Copy back computed value from GPU to CPU <pre><code>// Transfer data back to host memory\ncudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_out);\n\n// Deallocate host memory\nfree(a); \nfree(b); \nfree(out);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-2/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial-versionCUDA-templateCUDA-version   <pre><code>//-*-C++-*-\n// Vector-addition.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *out, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      out[i] = a[i] + b[i];\n    }\n  return out;\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out;       \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  out = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Add(a, b, out, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(out);\n\n  return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n// Vector-addition-template.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \nfloat *out, int n) \n{     \n  // allign your thread id indexes \n  int i = ........\n\n  // Allow the   threads only within the size of N\n  if------\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronize all the threads \n\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a   = (float*)......\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a,......\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = ....\n      b[i] = ....\n    }\n\n  // Transfer data from host to device memory\n  cudaMemcpy.....\n\n  // Thread organization \n  dim3 dimGrid....  \n  dim3 dimBlock....\n\n  // execute the CUDA kernel function \n  vector_add&lt;&lt;&lt; &gt;&gt;&gt;....\n\n  // Transfer data back to host memory\n  cudaMemcpy....\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n     {\n       assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n     }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate device memory\n  cudaFree...\n\n\n  // Deallocate host memory\n  free..\n\n  return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n// Vector-addition.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \nfloat *out, int n) \n{\n\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronice all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  out = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_out, sizeof(float) * N); \n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Transfer data from host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n\n  // Thread organization \n  dim3 dimGrid(ceil(N/32), ceil(N/32), 1);\n  dim3 dimBlock(32, 32, 1); \n\n  // execute the CUDA kernel function \n  vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n     {\n       assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n     }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_out);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(out);\n\n  return 0;\n}\n</code></pre>      Compilation and Output Serial-versionCUDA-version   <pre><code>// compilation\n$ gcc Vector-addition.c -o Vector-Addition-CPU\n\n// execution \n$ ./Vector-Addition-CPU\n\n// output\n$ ./Vector-addition-CPU \nout[0] = 3.000000\nPASSED\n</code></pre>   <pre><code>// compilation\n$ nvcc -arch=compute_70 Vector-addition.cu -o Vector-Addition-GPU\n\n// execution\n$ ./Vector-Addition-GPU\n\n// output\n$ ./Vector-addition-GPU\nout[0] = 3.000000\nPASSED\n</code></pre>      Questions <ul> <li>What happens if you remove the syncthreads(); from the __global void vector_add(float *a, float *b, float *out, int n) function.</li> <li>Can you remove the if condition if(i &lt; n) from the global void vector_add(float *a, float *b, float *out, int n) function? If so, how can you do that?</li> <li>Here we do not use the cudaDeviceSynchronize() in the main application. Can you figure out why we do not need to use it?</li> <li>Can you create a different thread block for a larger number of arrays?</li> </ul>"},{"location":"cuda/exercise-3/","title":"Matrix Multiplication","text":"<p>We will now look into basic matrix multiplication. In this example, we will perform the matrix multiplication. Matrix multiplication involves a nested loop. Again, most of the time, we might end up doing computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future. </p>   b  <ul> <li> <p>Allocating the CPU memory for A, B, and C matrices.    Here we notice that the matrix is stored in a    1D array because we want to consider the same function concept for CPU and GPU. <pre><code>// Initialize the memory on the host\nfloat *a, *b, *c;\n\n// Allocate host memory\na   = (float*)malloc(sizeof(float) * (N*N));\nb   = (float*)malloc(sizeof(float) * (N*N));\nc   = (float*)malloc(sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Allocating the GPU memory for A, B, and C matrix <pre><code>// Initialize the memory on the device\nfloat *d_a, *d_b, *d_c;\n\n// Allocate device memory\ncudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\ncudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the matrix A and B. <pre><code>// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n   {\n    a[i] = 2.0f;\n    b[i] = 2.0f;\n   }\n</code></pre></p> </li> <li> <p>Transfer initialized A and B matrix from CPU to GPU <pre><code>cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n</code></pre></p> </li> <li> <p>2D thread block for indexing x and y <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></p> </li> <li> <p>Calling the kernel function <pre><code>// Device function call\nmatrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n</code></pre></p>  matrix multiplication function call serialcuda   <pre><code>float * matrix_mul(float *h_a, float *h_b, float *h_c, int width)\n{\n  for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          float temp = 0;\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              temp += h_a[row*width+i] * h_b[i*width+col];\n            }\n          h_c[row*width+col] = temp;\n        }\n    }\n  return h_c;\n}\n</code></pre>   <pre><code>__global__ void matrix_mul(float* d_a, float* d_b, \nfloat* d_c, int width)\n{\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if ((row &lt; width) &amp;&amp; (col &lt; width)) \n    {\n      float temp = 0;\n      // each thread computes one \n      // element of the block sub-matrix\n      for (int i = 0; i &lt; width; ++i) \n        {\n          temp += d_a[row*width+i]*d_b[i*width+col];\n        }\n      d_c[row*width+col] = temp;\n    }\n}\n</code></pre>     </li> <li> <p>Copy back computed value from GPU to CPU;    transfer the data back to GPU (from device to host).    Here is the C matrix that contains the product of the two matrices. <pre><code>// Transfer data back to host memory\ncudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n</code></pre></p> </li> <li> <p>Deallocate the host and device memory <pre><code>// Deallocate device memory\ncudaFree(d_a);\ncudaFree(d_b);\ncudaFree(d_c);\n\n// Deallocate host memory\nfree(a); \nfree(b); \nfree(c);\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial-versionCUDA-templateCUDA-version   <pre><code>//-*-C++-*-\n// Matrix-multiplication.c\n\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\nusing namespace std;\n\nfloat * matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float temp = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              temp += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = temp;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix )size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c;       \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  b   = (float*)malloc(sizeof(float) * (N*N));\n  c   = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  matrix_mul(a, b, c, N);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      for(int j = 0; j &lt; N; j++)\n         {\n          cout &lt;&lt; c[j] &lt;&lt;\" \";\n         }\n      cout &lt;&lt; \" \" &lt;&lt;endl;\n   }\n\n  // Deallocate host memory\n free(a); \n free(b); \n free(c);\n\n return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n// Matrix-multiplication-template.cu\n\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\nusing namespace std;\n\n__global__ void matrix_mul(float* d_a, float* d_b, \nfloat* d_c, int width)\n{\n\n  // create a 2d threads block\n  int row = ..................\n  int col = ....................\n\n  // only allow the threads that are needed for the computation \n  if (................................)\n    {\n      float temp = 0;\n      // each thread computes one \n      // element of the block sub-matrix\n      for (int i = 0; i &lt; width; ++i) \n        {\n          temp += d_a[row*width+i]*d_b[i*width+col];\n        }\n      d_c[row*width+col] = temp;\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;       \n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_c; \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  ...\n  ...\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n  ...\n  ...\n\n  // Transfer data from host to device memory\n  cudaMemcpy(.........................);\n  cudaMemcpy(.........................);\n\n  // Thread organization\n  int blockSize = ..............;\n  dim3 dimBlock(......................);\n  dim3 dimGrid(.......................);\n\n  // Device function call \n  matrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n  // CPU computation for verification \n  cpu_matrix_mul(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree...\n\n  // Deallocate host memory\n  free...\n\n  return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n// Matrix-multiplication.cu\n\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\nusing namespace std;\n\n__global__ void matrix_mul(float* d_a, float* d_b, \nfloat* d_c, int width)\n{\n\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if ((row &lt; width) &amp;&amp; (col &lt; width)) \n    {\n      float temp = 0;\n      // each thread computes one \n      // element of the block sub-matrix\n      for (int i = 0; i &lt; width; ++i) \n        {\n          temp += d_a[row*width+i]*d_b[i*width+col];\n        }\n      d_c[row*width+col] = temp;\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;       \n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_c; \n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  b   = (float*)malloc(sizeof(float) * (N*N));\n  c   = (float*)malloc(sizeof(float) * (N*N));\n  host_check = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n\n  // Transfer data from host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n\n  // Thread organization\n  int blockSize = 32;\n  dim3 dimBlock(blockSize,blockSize,1);\n  dim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n\n  // Device function call \n  matrix_mul&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n  // cpu computation for verification \n  cpu_matrix_mul(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n  return 0;\n}\n</code></pre>      Compilation and Output Serial-versionCUDA-version   <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-CPU\n\n// execution \n$ ./Matrix-Multiplication-CPU\n\n// output\n$ g++ Matrix-multiplication.cc -o Matrix-multiplication\n$ ./Matrix-multiplication\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n16 16 16 16 \n16 16 16 16  \n16 16 16 16  \n16 16 16 16 \n</code></pre>   <pre><code>// compilation\n$ nvcc -arch=compute_70 Matrix-multiplication.cu -o Matrix-Multiplication-GPU\n\n// execution\n$ ./Matrix-Multiplication-GPU\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number\n$ 256\n\n// output\n$ Two matrices are equal\n</code></pre>      Questions <ul> <li>Right now, we are using the 1D array to represent the matrix. However, you can also do it with the 2D matrix. Can you try with 2D array matrix multiplication with 2D thread block?</li> <li>Can you get the correct solution if you remove the <code>if ((row &lt; width) &amp;&amp; (col &lt; width))</code> condition from the <code>__global__ void matrix_mul(float* d_a, float* d_b, float* d_c, int width)</code> function?</li> <li>Please try with different thread blocks and different matrix sizes. <pre><code>// Thread organization\nint blockSize = 32;\ndim3 dimBlock(blockSize,blockSize,1);\ndim3 dimGrid(ceil(N/float(blockSize)),ceil(N/float(blockSize)),1);\n</code></pre></li> </ul>"},{"location":"cuda/exercise-4/","title":"Shared Memory","text":"<p>In this example, we try shared memory matrix multiplication. This is achieved by blocking the global matrix into a small block matrix (tiled matrix) that can fit into the shared memory of the Nvidia GPU. Shared memory from the GPUs, which has a good bandwidth within the GPUs compared to access to the global memory.</p>      <ul> <li>This is very similar to the previous example; however, we just need to allocate the small block matrix into shared memory.  The below example shows the blocking size for <code>a</code> and <code>b</code> matrices respectively for global <code>A</code> and <code>B</code> matrices.  <pre><code>  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n</code></pre></li> </ul>  <ul> <li>Then we need to iterate elements within the block size and, finally with the global index.  These can be achieved with CUDA threads. </li> </ul>     <ul> <li> <p>You can also increase the shared memory or L1 cache size by using <code>cudaFuncSetCacheConfig</code>. For more information about  CUDA API, please refer to cudaFuncSetCacheConfig.</p>  Tips <pre><code>cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferL1);\n//cudaFuncSetCacheConfig(kernel, cudaFuncCachePreferShared);\n\ncudaFuncCachePreferNone: no preference for shared memory or L1 (default)\ncudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache\ncudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory\ncudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory\n\n// simple example usage increasing more shared memory \n#include&lt;stdio.h&gt;\nint main()\n{\n  // example of increasing the shared memory \n  cudaDeviceSetCacheConfig(My_Kernel, cudaFuncCachePreferShared);\n  My_Kernel&lt;&lt;&lt;&gt;&gt;&gt;();\n  cudaDeviceSynchronize(); \n  return 0;\n}\n</code></pre>  </li> <li> <p>Different Nvidia GPUs provides different configuration, for example, Ampere GA102 GPU Architecture, will support the following configuration: <pre><code>128 KB L1 + 0 KB Shared Memory\n120 KB L1 + 8 KB Shared Memory\n112 KB L1 + 16 KB Shared Memory\n96 KB L1 + 32 KB Shared Memory\n64 KB L1 + 64 KB Shared Memory\n28 KB L1 + 100 KB Shared Memory\n</code></pre></p> </li> </ul>"},{"location":"cuda/exercise-4/#questions-and-solutions","title":"Questions and Solutions","text":"Example: Shared Memory - Matrix Multiplication Matrix-multiplication-shared-templateMatrix-multiplication-shared.cu   <pre><code>// Matrix-multiplication-shared-template.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\n// block size for the matrix \n#define BLOCK_SIZE 16\n\nusing namespace std;\n\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, \n         float *d_c, int width)\n {\n  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  ...\n\n  // Indexing for the block matrix\n  int tx = threadIdx.x;\n  ...\n\n  // Indexing global matrix to block matrix \n  int row = threadIdx.x+blockDim.x*blockIdx.x;\n  ...\n\n  // Allow threads only for size of rows and columns (we assume square matrix)\n  if ((row &lt; width) &amp;&amp; (col&lt; width))\n    {\n      // Save temporary value for the particular index\n      float temp = 0;\n      for(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n        {\n          // Allign the global matrix to block matrix \n          a_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\n          b_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n\n          // Make sure all the threads are synchronized\n          ....\n\n          // Multiply the block matrix \n          for(int j = 0; j &lt; BLOCK_SIZE; ++j)\n            {\n              temp += a_block[ty][j] * b_block[j][tx];    \n            }\n          // Make sure all the threads are synchronized\n          ...\n       }\n      // Save block matrix entry to global matrix \n      ...\n    }\n}\n\n// Host call (matrix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float temp = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              temp += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = temp;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\n\nint main()\n{  \n cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\n cout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n int N=0;\n cin &gt;&gt; N;\n\n // Initialize the memory on the host\n float *a, *b, *c, *host_check;       \n\n // Initialize the memory on the device\n float *d_a, *d_b, *d_c; \n\n // Allocate host memory\n a   = (float*)malloc(sizeof(float) * (N*N));\n b   = (float*)malloc(sizeof(float) * (N*N));\n c   = (float*)malloc(sizeof(float) * (N*N));\n host_check = (float*)malloc(sizeof(float) * (N*N));\n\n // Initialize host arrays\n for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n // Allocate device memory\n cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n cudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\n cudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n cudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n\n // Thread organization\n dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                \n ...\n\n // Device function call \n matrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n // Transfer data back to host memory\n cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n // Cpu computation for verification \n cpu_matrix_mul(a,b,host_check,N);\n\n // Verification\n bool flag=1;\n for(int i = 0; i &lt; N; i++)\n   {\n    for(int j = 0; j &lt; N; j++)\n      {\n       if(c[j*N+i]!= host_check[j*N+i])\n         {\n          flag=0;\n          break;\n         }\n      }\n   }\n  if (flag==0)\n    {\n    cout &lt;&lt;\"But,two matrices are not equal\" &lt;&lt; endl;\n    cout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n    }\n    else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n return 0;\n}\n</code></pre>   <pre><code>// Matrix-multiplication-shared.cu\n//-*-C++-*-\n#include&lt;iostream&gt;\n#include&lt;cuda.h&gt;\n\n// block size for the matrix \n#define BLOCK_SIZE 16\n\nusing namespace std;\n\n// Device call (matrix multiplication)\n__global__ void matrix_mul(const float *d_a, const float *d_b, \nfloat *d_c, int width)\n{\n  // Shared memory allocation for the block matrix  \n  __shared__ int a_block[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ int b_block[BLOCK_SIZE][BLOCK_SIZE];\n\n  // Indexing for the block matrix\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n\n  // Indexing global matrix to block matrix \n  int row = threadIdx.x+blockDim.x*blockIdx.x;\n  int col = threadIdx.y+blockDim.y*blockIdx.y;\n\n  // Allow threads only for size of rows and columns (we assume square matrix)\n  if ((row &lt; width) &amp;&amp; (col&lt; width))\n    {\n      // Save temporary value for the particular index\n      float temp = 0;\n      for(int i = 0; i &lt; width / BLOCK_SIZE; ++i)\n         {\n          // Allign the global matrix to block matrix \n          a_block[ty][tx] = d_a[row * width + (i * BLOCK_SIZE + tx)];\n          b_block[ty][tx] = d_b[(i * BLOCK_SIZE + ty) * width + col];\n\n          // Make sure all the threads are synchronized\n          __syncthreads(); \n\n          // Multiply the block matrix\n          for(int j = 0; j &lt; BLOCK_SIZE; ++j)\n            {\n              temp += a_block[ty][j] * b_block[j][tx];    \n            }\n            __syncthreads();\n         }\n      // Save block matrix entry to global matrix \n      d_c[row*width+col] = temp;\n    }\n}\n\n// Host call (matix multiplication)\nfloat * cpu_matrix_mul(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n  return h_c;           \n}\n\n\nint main()\n {  \n   cout &lt;&lt; \"Programme assumes that matrix size is N*N \"&lt;&lt;endl;\n   cout &lt;&lt; \"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n   cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n   int N=0;\n   cin &gt;&gt; N;\n\n   // Initialize the memory on the host\n   float *a, *b, *c, *host_check;       \n\n   // Initialize the memory on the device\n   float *d_a, *d_b, *d_c; \n\n   // Allocate host memory\n   a   = (float*)malloc(sizeof(float) * (N*N));\n   b   = (float*)malloc(sizeof(float) * (N*N));\n   c   = (float*)malloc(sizeof(float) * (N*N));\n   host_check = (float*)malloc(sizeof(float) * (N*N));\n\n   // Initialize host arrays\n   for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * (N*N));\n  cudaMalloc((void**)&amp;d_c, sizeof(float) * (N*N));\n\n  // Transfer data from host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_c, c, sizeof(float) * (N*N), cudaMemcpyHostToDevice);\n\n  // Thread organization\n  dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE, 1);                \n  dim3 Grid_dim(ceil(N/BLOCK_SIZE), ceil(N/BLOCK_SIZE), 1);\n\n  // Device function call \n  matrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(c, d_c, sizeof(float) * (N*N), cudaMemcpyDeviceToHost);\n\n  // cpu computation for verification \n  cpu_matrix_mul(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"But,two matrices are not equal\" &lt;&lt; endl;\n      cout &lt;&lt;\"Matrix dimensions are assumed to be multiples of BLOCK_SIZE=16\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n return 0;\n}\n</code></pre>      Compilation and Output CUDA-version   <pre><code>// compilation\n$ nvcc -arch=sm_70 Matrix-multiplication-shared.cu -o Matrix-multiplication-shared\n\n// execution\n$ ./Matrix-multiplication-shared\nProgramme assumes that matrix size is N*N \nMatrix dimensions are assumed to be multiples of BLOCK_SIZE=16\nPlease enter the N size number\n$ 256\n\n// output\n$ Two matrices are equal\n</code></pre>      Questions <ul> <li>Could you resize the <code>BLOCK_SIZE</code> number and check the solution's correctness?</li> <li>Can you also create a different kind of thread block and matrix size and check the solution's correctness?</li> <li>Please try with <code>cudaFuncSetCacheConfig</code> and check if you can successfully execute the application. </li> </ul>"},{"location":"cuda/exercise-5/","title":"Unified Memory","text":"<p>Unified memory simplifies the explicit data movement from host to device by programmers. CUDA API will manage the data transfer between CPU and GPU. In this example, we will look into vector addition in GPU using the unified memory concept.</p>     <ul> <li>Just one memory allocation is enough <code>cudaMallocManaged()</code>.  The below table summarises the required steps needed for the unified memory concept.</li> </ul>    Without unified memory With unified memory     Allocate the host memory Allocate the host memory   Allocate the device memory Allocate the device memory   Initialize the host value Initialize the host value   Transfer the host value to the device memory location Transfer the host value to the device memory location   Do the computation using the CUDA kernel Do the computation using the CUDA kernel   Transfer the data from the device to host Transfer the data from the device to host   Free device memory Free device memory   Free host memory Free host memory"},{"location":"cuda/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Unified Memory - Vector Addition Without Unified MemoryWith Unified Memory - templateWith Unified Memory-version   <pre><code>//-*-C++-*-\n// Without-unified-memory.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n        float *out, int n) \n{\n\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n   threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronice all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *out; \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMalloc((void**)&amp;d_a, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_b, sizeof(float) * N);\n  cudaMalloc((void**)&amp;d_out, sizeof(float) * N); \n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Transfer data from host to device memory\n  cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n\n  // Thread organization \n  dim3 dimGrid(ceil(N/32), ceil(N/32), 1);\n  dim3 dimBlock(32, 32, 1);\n\n  // execute the CUDA kernel function \n  vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n\n  // Transfer data back to host memory\n  cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n     {\n       assert(fabs(out[i] - a[i] - b[i]) &lt; MAX_ERR);\n     }\n\n  printf(\"out[0] = %f\\n\", out[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_out);\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(out);\n\n  return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n                           float *out, int n) \n{\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronice all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  /*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device(unified) memory\n  cudaMallocManaged......\n\n // Initialize host arrays\n for(int i = 0; i &lt; N; i++)\n   {\n     d_a[i] = ...\n     d_b[i] = ...\n   }\n\n /*\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n\n // Thread organization \n dim3 dimGrid...    \n dim3 dimBlock...\n\n // execute the CUDA kernel function \n vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n\n // synchronize if needed\n ......\n\n /*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n\n // Verification\n for(int i = 0; i &lt; N; i++)\n   {\n     assert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n   }\n\n printf(\"out[0] = %f\\n\", d_out[0]);\n printf(\"PASSED\\n\");\n\n // Deallocate device(unified) memory\n cudaFree...\n\n\n /*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\n\n return 0;\n}\n</code></pre>   <pre><code>//-*-C++-*-\n// With-unified-memory.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n__global__ void vector_add(float *a, float *b, \n                           float *out, int n) \n{\n  int i = blockIdx.x * blockDim.x * blockDim.y + \n    threadIdx.y * blockDim.x + threadIdx.x;   \n  // Allow the   threads only within the size of N\n  if(i &lt; n)\n    {\n      out[i] = a[i] + b[i];\n    }\n\n  // Synchronice all the threads \n  __syncthreads();\n}\n\nint main()\n{\n  /*\n  // Initialize the memory on the host\n  float *a, *b, *out;\n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n  */\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_out;\n\n  // Allocate device memory\n  cudaMallocManaged(&amp;d_a, sizeof(float) * N);\n  cudaMallocManaged(&amp;d_b, sizeof(float) * N);\n  cudaMallocManaged(&amp;d_out, sizeof(float) * N); \n\n // Initialize host arrays\n for(int i = 0; i &lt; N; i++)\n   {\n     d_a[i] = 1.0f;\n     d_b[i] = 2.0f;\n   }\n\n /*\n // Transfer data from host to device memory\n cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n */\n\n // Thread organization\n dim3 dimGrid(ceil(N/32), ceil(N/32), 1);\n dim3 dimBlock(32, 32, 1);\n\n // execute the CUDA kernel function \n vector_add&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_out, N);\n cudaDeviceSynchronize();\n /*\n // Transfer data back to host memory\n cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n */\n\n // Verification\n for(int i = 0; i &lt; N; i++)\n   {\n     assert(fabs(d_out[i] - d_a[i] - d_b[i]) &lt; MAX_ERR);\n   }\n\n printf(\"out[0] = %f\\n\", d_out[0]);\n printf(\"PASSED\\n\");\n\n // Deallocate device memory\n cudaFree(d_a);\n cudaFree(d_b);\n cudaFree(d_out);\n\n /*\n // Deallocate host memory\n free(a); \n free(b); \n free(out);\n */\n\n return 0;\n}\n</code></pre>      Compilation and Output Without-unified-memory.cuWith-unified-memory   <pre><code>// compilation\n$ nvcc -arch=compute_70 Without-unified-memory.cu -o Without-Unified-Memory\n\n// execution \n$ ./Without-Unified-Memory\n\n// output\n$ ./Without-Unified-Memory\nout[0] = 3.000000\nPASSED\n</code></pre>   <pre><code>// compilation\n$ nvcc -arch=compute_70 With-unified-memory.cu -o With-Unified-Memory\n\n// execution\n$ ./With-Unified-Memory\n\n// output\n$ ./With-Unified-Memory \nout[0] = 3.000000\nPASSED\n</code></pre>      Questions <ul> <li>Here in this example, we have used <code>cudaDeviceSynchronize()</code>; can you remove <code>cudaDeviceSynchronize()</code>   and still get a correct solution? If not, why (think)?</li> <li>Please try with different thread blocks and array sizes. </li> </ul>"},{"location":"cuda/preparation/","title":"Preparation","text":""},{"location":"cuda/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"cuda/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"cuda/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200117]$cd u100490\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200117/CUDA .\n[u100490@login03 u100490]$ cd CUDA/\n[u100490@login03 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@login03 CUDA]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200117   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","title":"7. Until now you are in the login node, now its time to do the dry run test","text":"<ul> <li> <p>7.1 Reserve the interactive node for running/testing CUDA applications    <pre><code>$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></p>  check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part1 --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre>  </li> <li> <p>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490    p200117  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></p> </li> </ul>"},{"location":"cuda/preparation/#8-now-we-need-to-check-simple-cuda-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check simple CUDA application, if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 CUDA]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-cuda-codes","title":"9. Finally, we need to load the compiler to test the GPU CUDA codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing CUDA code  <pre><code>$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n### or\n$ source module.sh\n</code></pre></p>  check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n[u100490@mel2131 ~]$ module list\n\nCurrently Loaded Modules:\n1) env/release/2022.1           (S)   6) numactl/2.0.14-GCCcore-11.3.0  11) libpciaccess/0.16-GCCcore-11.3.0  16) GDRCopy/2.3-GCCcore-11.3.0                  21) knem/1.1.4.90-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1      (S)   7) CUDA/11.7.0                    12) hwloc/2.7.1-GCCcore-11.3.0        17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0  22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n3) GCCcore/11.3.0                     8) NVHPC/22.7-CUDA-11.7.0         13) OpenSSL/1.1                       18) libfabric/1.15.1-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0         9) XZ/5.2.5-GCCcore-11.3.0        14) libevent/2.1.12-GCCcore-11.3.0    19) PMIx/4.2.2-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0      10) libxml2/2.9.13-GCCcore-11.3.0  15) UCX/1.13.1-GCCcore-11.3.0         20) xpmem/2.6.5-36-GCCcore-11.3.0\n\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre>  </li> </ul>"},{"location":"cuda/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>10.1 For example, Dry-run-test  <pre><code>// compilation\n$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"cuda/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly for the hands-on session, we need to do the node reservation:","text":"<ul> <li> <p>10.1 For example, reservation  <pre><code>$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre></p>  check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res training_part2 --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre>  </li> </ul>"},{"location":"cuda/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands on exercise","text":"<ul> <li>12.1 For example, <code>Hello World</code> example, we do the following steps:  <pre><code>[u100490@mel2063 CUDA]$ pwd\n/project/home/p200117/u100490/CUDA\n[u100490@mel2063 CUDA]$ ls\n[u100490@mel2063 CUDA]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Shared-memory  Vector-addition\n[u100490@mel2063 CUDA]$ source module.sh\n[u100490@mel2063 CUDA]$ cd Hello-world\n// compilation\n[u100490@mel2063 CUDA]$ nvcc -arch=compute_70 Hello-world.cu -o Hello-World-GPU \n\n// execution\n[u100490@mel2063 CUDA]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 CUDA]$ Hello World from GPU\n</code></pre></li> </ul>"},{"location":"cuda/profiling/","title":"Profiling and Performance","text":""},{"location":"cuda/profiling/#time-measurement","title":"Time measurement","text":"<p>In CUDA, the execution time can be measured by using the cuda events. CUDA API events shall be created using <code>cudaEvent_t</code>, for example, <code>cudaEvent_t start, stop;</code>. Moreover, it can be initiated by <code>cudaEventCreate(&amp;start)</code> for a start and similarly for stop, it can be created as <code>cudaEventCreate(&amp;stop)</code>. </p>  CUDA API <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start,0);\n</code></pre>  <p>And it can be initialised to measure the timing as <code>cudaEventRecord(start,0)</code> and <code>cudaEventRecord(stop,0)</code>. Then the timings can be measured as float, for example, <code>cudaEventElapsedTime(&amp;time, start, stop)</code>. Finally, all the events should be destroyed using <code>cudaEventDestroy</code>, for example, <code>cudaEventDestroy(start)</code> and <code>cudaEventDestroy(start)</code>.</p>  CUDA API <pre><code>cudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n</code></pre>  <p>The following example shows how to measure your GPU kernel call in a CUDA application:</p>  Example <pre><code>cudaEvent_t start, stop;\ncudaEventCreate(&amp;start);\ncudaEventCreate(&amp;stop);\ncudaEventRecord(start);\n\n// Device function call \nmatrix_mul&lt;&lt;&lt;Grid_dim, Block_dim&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n//use CUDA API to stop the measuring time\ncudaEventRecord(stop);\ncudaEventSynchronize(stop);\nfloat time;\ncudaEventElapsedTime(&amp;time, start, stop);\ncudaEventDestroy(start);\ncudaEventDestroy(stop);\n\ncout &lt;&lt; \" time taken for the GPU kernel\" &lt;&lt; time &lt;&lt; endl;\n</code></pre>"},{"location":"cuda/profiling/#nvidia-system-wide-performance-analysis","title":"Nvidia system-wide performance analysis","text":"<p>Nvidia profiling tools help to analyse the code when it is being spent on the given architecture. Whether it is communication or computation, we can get helpful information through traces and events. This will help the programmer optimise the code performance on the given architecture. For this, Nvidia offers three kinds of profiling options, they are:</p> <ul> <li> <p>Nsight Compute: CUDA application interactive kernel profiler: This will give traces and events of the kernel calls; this further provides both visual profile-GUI and Command Line Interface (CLI) profiling options. <code>ncu -o profile Application.exe</code> command will create an output file <code>profile.ncu-rep</code>, which can be opened using <code>ncu-ui</code>. </p>  Example <pre><code>$ ncu ./a.out\nmatrix_mul(float *, float *, float *, int), 2023-Mar-12 20:20:45, Context 1, Stream 7\nSection: GPU Speed Of Light Throughput\n---------------------------------------------------------------------- --------------- ------------------------------\nDRAM Frequency                                                           cycle/usecond                         874.24\nSM Frequency                                                             cycle/nsecond                           1.31\nElapsed Cycles                                                                   cycle                         241109\nMemory [%]                                                                           %                          13.68\nDRAM Throughput                                                                      %                           0.07\nDuration                                                                       usecond                         184.35\nL1/TEX Cache Throughput                                                              %                          82.39\nL2 Cache Throughput                                                                  %                          13.68\nSM Active Cycles                                                                 cycle                       30531.99\nCompute (SM) [%]                                                                     %                           1.84\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n     waves across all SMs. Look at Launch Statistics for more details.                                             \n\nSection: Launch Statistics\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Size                                                                                                       1024\nFunction Cache Configuration                                                                  cudaFuncCachePreferNone\nGrid Size                                                                                                          16\nRegisters Per Thread                                                   register/thread                             26\nShared Memory Configuration Size                                                  byte                              0\nDriver Shared Memory Per Block                                              byte/block                              0\nDynamic Shared Memory Per Block                                             byte/block                              0\nStatic Shared Memory Per Block                                              byte/block                              0\nThreads                                                                         thread                          16384\nWaves Per SM                                                                                                     0.10\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80             \n      multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n      concurrently with other workloads, consider reducing the block size to have at least one block per            \n      multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n      Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n      description for more details on launch configurations.                                                        \n\nSection: Occupancy\n---------------------------------------------------------------------- --------------- ------------------------------\nBlock Limit SM                                                                   block                             32\nBlock Limit Registers                                                            block                              2\nBlock Limit Shared Mem                                                           block                             32\nBlock Limit Warps                                                                block                              2\nTheoretical Active Warps per SM                                                   warp                             64\nTheoretical Occupancy                                                                %                            100\nAchieved Occupancy                                                                   %                          45.48\nAchieved Active Warps Per SM                                                      warp                          29.11\n---------------------------------------------------------------------- --------------- ------------------------------\nWRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n      theoretical (100.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads   \n      or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n      as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n      (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n      optimizing occupancy.                                                                                         \n</code></pre>  </li> <li> <p>Nsight Graphics: Graphics application frame debugger and profiler: This is quite useful for analysing the profiling results through GUI. </p> </li> <li> <p>Nsight Systems: System-wide performance analysis tool: It is needed when we try to do heterogeneous computation profiling, for example, mixing MPI and OpenMP with CUDA. This will profile the system-wide application, that is, both CPU and GPU. To learn more about the command line options, please use <code>$ nsys profile --help</code></p>  Example <pre><code>$ nsys profile -t nvtx,cuda --stats=true ./a.out\nGenerating '/scratch_local/nsys-report-ddd1.qdstrm'\n[1/7] [========================100%] report1.nsys-rep\n[2/7] [========================100%] report1.sqlite\n[3/7] Executing 'nvtxsum' stats report\nSKIPPED: /m100/home/userexternal/ekrishna/Teaching/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n[4/7] Executing 'cudaapisum' stats report\n\nTime (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)  Min (ns)  Max (ns)   StdDev (ns)        Name      \n--------  ---------------  ---------  -----------  --------  --------  ---------  -----------  ----------------\n    99.7        398381310          3  132793770.0    8556.0      6986  398365768  229992096.8  cudaMalloc      \n     0.2           714256          3     238085.3   29993.0     24944     659319     364807.8  cudaFree        \n     0.1           312388          3     104129.3   43405.0     37692     231291     110162.3  cudaMemcpy      \n     0.0            51898          1      51898.0   51898.0     51898      51898          0.0  cudaLaunchKernel\n\n[5/7] Executing 'gpukernsum' stats report\n\n\nTime (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)     GridXYZ         BlockXYZ                        Name                   \n--------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------  --------------  ------------------------------------------\n100.0           181949          1  181949.0  181949.0    181949    181949          0.0     4    4    1    32   32    1  matrix_mul(float *, float *, float *, int)\n\n[6/7] Executing 'gpumemtimesum' stats report\n\nTime (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n--------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n 75.0            11520      2    5760.0    5760.0      5760      5760          0.0  [CUDA memcpy HtoD]\n 25.0             3840      1    3840.0    3840.0      3840      3840          0.0  [CUDA memcpy DtoH]\n\n[7/7] Executing 'gpumemsizesum' stats report\n\nTotal (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n----------  -----  --------  --------  --------  --------  -----------  ------------------\n  0.080      2     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy HtoD]\n  0.040      1     0.040     0.040     0.040     0.040        0.000  [CUDA memcpy DtoH]\n\nGenerated:\n   /m100/home/userexternal/ekrishna/Teaching/report1.nsys-rep\n   /m100/home/userexternal/ekrishna/Teaching/report1.sqlite\n</code></pre>  </li> </ul>"},{"location":"cuda/profiling/#occupancy","title":"Occupancy","text":"<p>The CUDA Occupancy Calculator allows you to compute the multiprocessor occupancy of a Nvidia GPU microarchitecture by a given CUDA kernel. The multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU.</p> <p>\\(Occupancy  = \\frac{Active\\ warps\\ per\\ SM}{ Max.\\ warps\\ per\\ SM}\\)</p>  Examples Occupancy CUDACompilation and results   <pre><code>//-*-C++-*-\n#include&lt;iostream&gt;\n// Device code\n__global__ void MyKernel(int *d, int *a, int *b)\n{\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  d[idx] = a[idx] * b[idx];\n}\n\n// Host code\nint main()\n{\n  // set your numBlocks and blockSize to get 100% occupancy\n  int numBlocks = 32;        // Occupancy in terms of active blocks\n  int blockSize = 128;\n\n  // These variables are used to convert occupancy to warps\n  int device;\n  cudaDeviceProp prop;\n  int activeWarps;\n  int maxWarps;\n\n  cudaGetDevice(&amp;device);\n  cudaGetDeviceProperties(&amp;prop, device);\n\n  cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n  &amp;numBlocks,\n  MyKernel,\n  blockSize,0);\n\n  activeWarps = numBlocks * blockSize / prop.warpSize;\n  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n\n  std::cout &lt;&lt; \"Max # of Blocks : \" &lt;&lt; numBlocks &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"ActiveWarps : \" &lt;&lt; activeWarps &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"MaxWarps : \" &lt;&lt; maxWarps &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Occupancy: \" &lt;&lt; (double)activeWarps / maxWarps * 100 &lt;&lt; \"%\" &lt;&lt; std::endl;\n\n return 0;\n}\n</code></pre>   <pre><code>// compilation\n$ nvcc -arch=compute_70 occupancy.cu -o Occupancy-GPU\n\n// execution\n$ ./Occupancy-GPU\n\n// output\nMax number of Blocks : 16\nActiveWarps : 64\nMaxWarps : 64\nOccupancy: 100%\n</code></pre>      Questions <ul> <li>Occupancy: can you change <code>numBlocks</code> and <code>blockSize</code> in Occupancy.cu code  and check how it affects or predicts the occupancy of the given Nvidia microarchitecture?</li> <li>Profiling: run your <code>Matrix-multiplication.cu</code> and <code>Vector-addition.cu</code> code and observe what you notice?  for example, how to improve the occupancy? Or maximise a GPU utilization?</li> <li>Timing: using CUDA events API can you measure your GPU kernel execution, and compare how fast is your GPU computation compared to CPU computation?</li> </ul>"},{"location":"education/introduction/","title":"Introduction","text":"<p>If you are an educator and would like to provide a course, please read the following instructions:</p> <p>The following are the website and git repository for providing the training events under EuroCC (Luxembourg).</p> <p>https://ncclux.github.io/NCC-Trainings  https://github.com/NCCLUX/NCC-Trainings</p> <p>Please create your own repository and keep your training material there. We will merge with the main branch (or publish your training material) close to the event or earlier (whenever you are ready).</p> <p>We plan to keep 2 hours for theory and 2 hours for practical (hands-on) exercise. Typically, the entire event is for 5 hours; this extra 1 hour is dedicated to a dry-run (to ensure registered participants can access the machine (in this case, MeluXina) and test the training material (hands-on exercise) already. For example, <pre><code>11:00-12:00 (dry-run),\n12:00-13:00 (lunch break),\n13:00-15:00 (lecture) and\n15:00-17:00 (hands-on exercise).\n</code></pre> It would be nice to have a categorisation for the hands-on exercise, for example, going from basic to intermediate topics (or advanced). The previous course examples are here: https://ncclux.github.io/NCC-Trainings/cuda/exercise-1/</p> <p>Example case: The Quantum Expresso course comes under Computational Chemistry (if you have another name preference, please let us know). Before the event (for the event advertisement), we need to publish the introduction and learning outcome of the course; perhaps, you could take inspiration from this existing course; https://ncclux.github.io/NCC-Trainings/openmp/</p> <p>Could you please provide a suitable date (because we also need to reserve the HPC machine for the event) as soon as possible, and at the same time, we also need to publish (Luxinnovation handles the landing page information) the event info. at least 3-4 weeks before so that enough participants would be able to register for the event.</p> <p>For the training event, we usually accept 20-30 (however, as you might have known already, in an event like this, some participants would not attend the event at the last minute. Therefore, we usually accept around 50. Usually, the participants come from Luxembourg (academic and industry) and other EU countries as well. However, preference will be given to Luxembourg participants. More importantly, these events are online (mainly giving participants flexibility and avoiding local logistics burden).</p> <p>During the event, you will be using the MeluXina HPC machine once you have come up with the date for the event. We can request your account in MeluXina for the training material preparation. Regarding the participants, MeluXina access will be given before the event. </p>"},{"location":"education/requirements/","title":"Requirements","text":"<p>In order to install and check you need to have the following dependencies (and it is tested with Python 3.10.12):</p> <pre><code>mkdocs                                    == 1.4.2\nmkdocs-git-revision-date-localized-plugin == 1.2.0\nmkdocs-glightbox                          == 0.3.1\nmkdocs-material                           == 9.1.0\nmkdocs-material-extensions                == 1.1.1\nmkdocs-minify-plugin                      == 0.6.2\n</code></pre> <p>Bfore requesting the merge request, pelase make sure,  you do not have any issues (website layout) with .md files.</p>"},{"location":"julia/","title":"High-performance scientific computing using Julia","text":"<p>Have you already heard about Julia, the high-level, high-performance scientific computing language of the future?</p> <p>This course will teach you the basics of Julia, and how to get started writing your code parallel-ready. You will get a glimpse on how to scale your code in a high-performance computing (HPC) environment.</p>"},{"location":"julia/#learning-outcomes","title":"Learning outcomes","text":"<p>This is an introductory course into Julia programming. Attendees will learn, among others:</p> <ul> <li>use Julia for a wide range of computation-heavy scientific and software engineering tasks</li> <li>basic skills to process data using Julia language</li> <li>basic practices of program efficiency required for accelerating analyses</li> <li>tips on scaling up computational analyses.</li> </ul> <p>The course will give an overview of the main scientific programming libraries in the Julia ecosystem, giving the attendees the ability to quickly utilize the available software for solving their problems, with additional focus on parallelism and HPC utilization.</p>"},{"location":"julia/#agenda-tentative","title":"Agenda (tentative)","text":""},{"location":"julia/#session-i-50-mins-getting-started","title":"Session I (~50 mins): Getting started","text":"<ul> <li>Welcome</li> <li>General introduction to Julia</li> <li>Motivation \u2013 what problems are best solved with Julia</li> <li>Installation of Julia, REPL, managing packages</li> <li>Programming language basics (variables and types, loops, arrays, functions, \u2026)</li> </ul> <p>(10 mins break)</p>"},{"location":"julia/#session-ii-50-mins-data-processing","title":"Session II (~50 mins): Data processing","text":"<ul> <li>I/O and data manipulation</li> <li>Read, write, different data formats</li> <li>Plotting (data visualization)</li> </ul> <p>(10 mins break)</p>"},{"location":"julia/#session-iii-50-mins-high-performance-and-distributed-processing","title":"Session III (~50 mins): High-performance and distributed processing","text":"<ul> <li>Overview of the usual performance bottlenecks</li> <li>Parallelization model of Julia, threads</li> <li>Distributed programming and helper packages</li> <li>GPU usage</li> </ul>"},{"location":"julia/#session-iv-50-mins-reproducibility","title":"Session IV (~50 mins): Reproducibility","text":"<ul> <li>Reprodubcibility with Julia</li> <li>Dependency management</li> <li>Notes on containerization</li> </ul>"},{"location":"julia/#prerequisites","title":"Prerequisites","text":"<p>Detailed instructions on what to prepare will be sent out to confirmed participants.</p> <p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openacc/","title":"Introduction to OpenACC for Heterogeneous Computing","text":"<p>Participants from this course will learn GPU programming using the OpenACC programming model, such as compute constructs, loop constructs and data clauses. Furthermore, understanding the GPU architecture and how parallel threads blocks are created and used to parallelise the computational task. Moreover, GPU is an accelerator; hence, there must be a good understanding of memory management between the GPU and CPU, which will also be discussed in detail. Finally, participants will also learn to use the OpenACC programming model to accelerate linear algebra (routines) and iterative solvers on the GPU. Participants will learn theories first and implement the OpenACC programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openacc/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openacc/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the GPU architecture (and also the difference between GPU and CPU)<ul> <li>Streaming architecture </li> <li>Threads blocks </li> </ul> </li> <li>Implement the OpenACC programming model  <ul> <li>Compute constructs  </li> <li>Loop constructs </li> <li>Data clauses</li> </ul> </li> <li>Efficient handling of memory management  <ul> <li>Host to Device </li> <li>Unified memory </li> </ul> </li> <li>Apply the OpenACC programming knowledge to accelerate examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openacc/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No GPU programming knowledge is required; however, knowing the OpenMP programming model is advantageous. </p>"},{"location":"openacc/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openacc/exercise-1/","title":"Compute Constructs and Paralleize Loops","text":""},{"location":"openacc/exercise-1/#compute-constructs","title":"Compute Constructs","text":"<p>In our first exercise, we will look into how to offload the computation to the device (GPU). Because the main aim of the OpenACC is to facilitate offloading the computation using OpenACC APIs. OpenACC provides two kinds of variants to offload the computations to GPU. They are explained as follows,</p> <ul> <li>OpenACC provides two compute constructs to parallelize the computation</li> <li>The first one is <code>parallel</code>, and the second is <code>kernels</code></li> <li>Both of these parallel constructs perform more or less the same</li> <li>However, <code>kernels</code> will have more control over the parallel region</li> <li>Therefore, as a programmer, if you are very familiar with what you are doing in the parallel region,      you may use <code>parallel</code>; otherwise, it is better to use <code>kernels</code></li> <li>Because the compiler will take care of the safe parallelization under the <code>kernels</code> construct </li> </ul> <p>At the same time, in order to enable OpenACC constructs, clauses, and environment variables. etc., we need to include the OpenACC library as follows:</p>  <p>OpenACC library</p> C/C++FORTRAN   <pre><code>#include&lt;openacc.h&gt;\n</code></pre>   <pre><code>use openacc\n</code></pre>        <p>To create a parallel region in OpenACC, we use the following compute constructs:</p>  <p>Parallel Constructs</p> C/C++FORTRAN   <pre><code>#pragma acc parallel [clause-list] new-line\n   structured block\n</code></pre>   <pre><code>!$acc parallel [ clause-list ]\n    structured block\n!$acc end parallel\n</code></pre>      Available caluses for parallel C/C++ and FORTRAN     <pre><code>    async [ ( int-expr ) ]\n    wait [ ( int-expr-list ) ]\n    num_gangs( int-expr )\n    num_workers( int-expr )\n    vector_length( int-expr )\n    device_type( device-type-list )\n    if( condition )\n    self [ ( condition ) ]\n    reduction( operator : var-list )\n    copy( var-list )\n    copyin( [ readonly: ] var-list )\n    copyout( [ zero: ] var-list )\n    create( [ zero: ] var-list )\n    no_create( var-list )\n    present( var-list )\n    deviceptr( var-list )\n    attach( var-list )\n    private( var-list )\n    firstprivate( var-list )\n    default( none | present )\n</code></pre>   <p>Kernels Constructs</p> C/C++FORTRAN   <pre><code>#pragma acc kernels [ clause-list ] new-line\n   structured block\n</code></pre>   <pre><code>!$acc kernels [ clause-list ]\n   structured block\n!$acc end kernels\n</code></pre>      Available caluses for kernels C/C++ and FORTRAN   <pre><code>async [ ( int-expr ) ]\nwait [ ( int-expr-list ) ]\nnum_gangs( int-expr )\nnum_workers( int-expr )\nvector_length( int-expr )\ndevice_type( device-type-list )\nif( condition )\nself [ ( condition ) ]\ncopy( var-list )\ncopyin( [ readonly: ] var-list )\ncopyout( [ zero: ] var-list )\ncreate( [ zero: ] var-list )\nno_create( var-list )\npresent( var-list )\ndeviceptr( var-list )\nattach( var-list )\ndefault( none | present )\n</code></pre>"},{"location":"openacc/exercise-1/#compilers","title":"Compilers","text":"<p>The following compilers would support the OpenACC programming model.</p> <ul> <li>GNU - It is an opensource and can be used for Nvidia and AMD CPUs</li> <li>Nvidia HPC SDK - It is from Nvidia, and works very well for Nvidia GPUs</li> <li>HPE - Presently it supports the FORTRAN (not C/C++) </li> </ul>  <p>Examples (GNU, Nvidia HPC SDK and HPE): Compilation</p> Nvidia HPC SDK   <pre><code>$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel test.c \n</code></pre>"},{"location":"openacc/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-versionOpenACC-version-parallelOpenACC-version-kernels   <pre><code>//Hello-world-CPU.c \n#include&lt;stdio.h&gt;\nint main()\n{\n  printf(\"Hello World from CPU!\\n\");        \n  return 0;\n}\n</code></pre>   <pre><code>//Hello-world-parallel.c    \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{ \n#pragma acc parallel                                                             \n  printf(\"Hello World from GPU!\\n\");\n  return 0;\n}\n</code></pre>   <pre><code>//Hello-world-kernels.c \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{\n#pragma acc kernels                            \n  printf(\"Hello World from GPU!\\n\");\n  return 0;\n}\n</code></pre>      Compilation and Output Serial-versionOpenACC-version-parallelOpenACC-version-kernels   <pre><code>// compilation\n$ gcc Hello-world-CPU.c -o Hello-World-CPU\n\n// execution \n$ ./Hello-World-CPU\n\n// output\n$ Hello World from CPU!\n</code></pre>   <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-parallel.c -o Hello-World-GPU\nmain:\n7, Generating NVIDIA GPU code\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n</code></pre>   <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-kernels.c -o Hello-World-GPU\nmain:\n7, Accelerator serial kernel generated\n   Generating NVIDIA GPU code\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n</code></pre>"},{"location":"openacc/exercise-1/#loop","title":"Loop","text":"<p>Our second exercise is to work on how to parallelize the loop.  Most of the time, we would be doing the intense computation under the loop. In situations like that, it would be more efficient to parallelize the loops in the computation.  To start with a simple example, we will begin with printing <code>Hello World from GPU</code> multiple times in addition to our previous example.  Moreover, just adding  <code>#pragma acc parallel</code> or  <code>#pragma acc kernels</code> would not parallelize your computation, instead will make sure that the computation will be executed on the device. </p>     <p>Loop Constructs</p> C/C++FORTRAN   <pre><code>#pragma acc loop [clause-list] new-line\n   for loop\n</code></pre>   <pre><code>!$acc loop [clause-list]\n   do loop\n</code></pre>      Available caluses for loop C/C++ and FORTRAN   <pre><code>collapse( n )\ngang [( gang-arg-list )]\nworker [( [num:]int-expr )]\nvector [( [length:]int-expr )]\nseq\nindependent\nauto\ntile( size-expr-list )\ndevice_type( device-type-list )\nprivate( var-list )\nreduction( operator:var-list )\n</code></pre>"},{"location":"openacc/exercise-1/#questions-and-solutions_1","title":"Questions and Solutions","text":"Examples: Loop (Hello World) Serial-version-loopOpenACC-version-parallel-loopOpenACC-version-kernels-loop   <pre><code>//Hello-world-CPU-loop.c    \n#include&lt;stdio.h&gt;\nint main()\n{\n  for(int i = 0; i &lt; 5; i++)\n    {         \n      printf(\"Hello World from CPU!\\n\");\n    }       \n  return 0;\n}\n</code></pre>   <pre><code>//Hello-world-parallel-loop.c   \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{\n#pragma acc parallel loop\n  for(int i = 0; i &lt; 5; i++)\n    {                                \n      printf(\"Hello World from GPU!\\n\");\n    }\nreturn 0;\n}\n</code></pre>   <pre><code>//Hello-world-kernels-loop.c    \n#include&lt;stdio.h&gt;\n#include&lt;openacc.h&gt;     \nint main()\n{\n#pragma acc kernels loop\n  for(int i = 0; i &lt; 5; i++)\n    {                                \n      printf(\"Hello World from GPU!\\n\");\n    }\nreturn 0;\n}\n</code></pre>      Compilation and Output Serial-version-loopOpenACC-version-parallel-loopOpenACC-version-kernels-loop   <pre><code>// compilation\n$ gcc Hello-world-CPU-loop.c -o Hello-World-CPU\n\n// execution \n$ ./Hello-World-CPU\n\n// output\n$ Hello World from CPU!\n$ Hello World from CPU!\n$ Hello World from CPU!\n$ Hello World from CPU!\n$ Hello World from CPU!                                \n</code></pre>   <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-parallel-loop.c -o Hello-World-GPU\nmain:\n5, Generating NVIDIA GPU code\n  7, #pragma acc loop gang /* blockIdx.x */\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!                                \n</code></pre>   <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world-kernels-loop.c -o Hello-World-GPU\nmain:\n7, Loop is parallelizable\n   Generating NVIDIA GPU code\n    7, #pragma acc loop gang, vector(32) /* blockIdx.x threadIdx.x */\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!\n$ Hello World from GPU!                                \n</code></pre>"},{"location":"openacc/exercise-2/","title":"Data Locality","text":""},{"location":"openacc/exercise-2/#data-clauses","title":"Data Clauses","text":"<p>Vector addition is one of the basic linear algebra routines. It involves adding two vectors into one where each index of the corresponding vector should be added. This vector addition example covers two of the most important OpenACC constructs and clauses: compute constructs and data clauses. They are:</p> <ul> <li>Since the computation involves a loop, we could use, <code>#pragma acc parallel loop</code> or <code>#pragma acc kernels loop</code></li> <li>And we need to transfer the data to the GPU. For this purpose, OpenACC provides a rich set of data mapping clauses in OpenACC. </li> </ul> <p>Data clauses in OpenACC provide a convenient way of handling the data between CPU and GPU. The following list explains usage and description.</p> <ul> <li><code>copy</code>: create a space for a variable in the device, copy the data to the device before the region and copy the data back to the host after the region. And releases the memory of the variable in the device. </li> <li><code>copyin</code>:   create a space for a variable in the device, copy the data to the device before the region and do not copy the data back to the host after the region. And releases the memory of the variable in the device. </li> <li><code>copyout</code>: create a space for a variable in the device; do not copy the data to the device before the region and copy the data back to the host after the region. And releases the memory of the variable in the device. </li> <li><code>create</code>:creates a memory of the device; do not copy from host to device or device to host. </li> <li><code>present</code>: The listed variables are already present on the device, so no further action needs to be taken. </li> <li><code>deviceptr</code>: this is quite useful when data has to be managed outside of the OpenACC.</li> </ul>      <p>Data Constructs</p> C/C++FORTRAN   <pre><code>#pragma acc data [clause-list] new-line\n   structured block\n</code></pre>   <pre><code>!$acc data [clause-list]\n   structured block\n!$acc end data\n</code></pre>      Available caluses for data C/C++ and FORTRAN   <pre><code>if( condition )\nasync [( int-expr )]\nwait [( wait-argument )]\ndevice_type( device-type-list )\ncopy( var-list )\ncopyin( [readonly:]var-list )\ncopyout( [zero:]var-list )\ncreate( [zero:]var-list )\nno_create( var-list )\npresent(a var-list )\ndeviceptr( var-list )\nattach( var-list )\ndefault( none | present )\n</code></pre>     <p>We would need just two data clauses from OpenACC in the vector addition example. The two initialized vectors should be copied to the device from the host; for this purpose, we can use <code>copyin</code>. At the same time, product vectors do not need to be copied from host to device. However, it should be copied from device to host; for this, we could just use <code>copyout.</code> </p> <p>The following are the steps for learning vector addition example:</p>     <ul> <li> <p>Allocating the CPU memory for <code>a</code>, <code>b</code>, and <code>c</code> vector <pre><code>// Initialize the memory on the host\nfloat *restrict a, *restrict b, *restrict c;\n\n// Allocate host memory\na = (float*)malloc(sizeof(float) * N);\nb = (float*)malloc(sizeof(float) * N);\nc = (float*)malloc(sizeof(float) * N);\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the     arrays <code>a</code> and <code>b</code>.  <pre><code>// Initialize host arrays\nfor(int i = 0; i &lt; N; i++)\n  {\n    a[i] = 1.0f;\n    b[i] = 2.0f;\n  }\n</code></pre></p> </li> <li> <p>Vector addition kernel function call definition</p>  vector addition function call Serial-versionOpenACC-version   <pre><code>// CPU function that adds two vector \nvoid Vector_Addition(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n}\n</code></pre>   <pre><code>// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n#pragma acc kernels loop copyin(a[0:n], b[0:n]) copyout(c[0:n])\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n}       \n</code></pre>     </li> </ul>     <ul> <li>Deallocate the host memory <pre><code>// Deallocate host memory\nfree(a); \nfree(b); \nfree(c);\n</code></pre></li> </ul>"},{"location":"openacc/exercise-2/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial-versionOpenACC-templateOpenACC-version   <pre><code>// Vector-addition.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Addition(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *a, *b, *c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>// Vector-addition-template.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;    \n\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n\n// add here either parallel or kernel plus data map clauses\n#pragma acc \nfor(int i = 0; i &lt; n; i ++)\n   {\n     c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>// Vector-addition-openacc.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n// or #pragma acc kernels loop copyin(a[0:n], b[0:n]) copyout(c[0:n])\n#pragma acc kernels loop copyin(a[0:n], b[0:n]) copyout(c[0:n])\nfor(int i = 0; i &lt; n; i ++)\n   {\n    c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>      Compilation and Output Serial-versionOpenACC-version   <pre><code>// compilation\n$ gcc Vector-addition.c -o Vector-Addition-CPU\n\n// execution \n$ ./Vector-Addition-CPU\n\n// output\n$ ./Vector-addition-CPU \nPASSED\n</code></pre>   <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Vector-addition-openacc.c -o Vector-Addition-GPU\nVector_Addition:\n12, Generating copyin(a[:n]) [if not already present]\n    Generating copyout(c[:n]) [if not already present]\n    Generating copyin(b[:n]) [if not already present]\n14, Loop is parallelizable\n    Generating NVIDIA GPU code\n    14, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n\n// execution\n$ ./Vector-Addition-GPU\n\n// output\n$ ./Vector-addition-GPU\nPASSED\n</code></pre>      Question <ul> <li>Please try other data clauses for other applications and get familiarised with them.</li> </ul>"},{"location":"openacc/exercise-3/","title":"Optimize Loops","text":""},{"location":"openacc/exercise-3/#collapse","title":"Collapse","text":"<p>The collapse clause can be used for the nested loop; an entire part of the iteration will be divided by an available number of threads. If the outer loop is equal to the available threads, then the outer loop will be divided number of threads. The figure below shows an example of not using the <code>collapse</code> clause. Therefore, only the outer loop is parallelised; each outer loop index will have N number of inner loop iterations. </p>     <p>This is not what we want. Instead, with the available threads, we would like to parallelise the loops as efficiently as we could. Moreover, most of the time, we would have more threads in a given GPU, in our case, we will test Nvidia A100 GPU. Therefore, when adding the <code>collapse</code> clause, we notice that the available threads execute every single iteration, as seen in the figure below.</p>     <p>We will now look into basic matrix multiplication. In this example, we will perform the matrix multiplication. Matrix multiplication involves a nested loop. Again, most of the time, we might end up doing computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future. </p>    <ul> <li> <p>Allocating the CPU memory for A, B, and C matrices.    Here we notice that the matrix is stored in a    1D array because we want to consider the same function concept for CPU and GPU. <pre><code>// Initialize the memory on the host\nfloat *restrict a, *restrict b, *restrict c;\n\n// Allocate host memory\na  = (float*)malloc(sizeof(float) * (N*N));\nb  = (float*)malloc(sizeof(float) * (N*N));\nc  = (float*)malloc(sizeof(float) * (N*N));\n</code></pre></p> </li> <li> <p>Now we need to fill the values for the matrix A and B. <pre><code>// Initialize host matrix\nfor(int i = 0; i &lt; (N*N); i++)\n   {\n    a[i] = 2.0f;\n    b[i] = 2.0f;\n   }\n</code></pre></p> </li> <li> <p>Calling function <pre><code>// Function call\nMatrix_Multiplication(d_a, d_b, d_c, N);\n</code></pre></p>  matrix multiplication function call SerialOpenACC   <pre><code>void Matrix_Multiplication(float *a, float *b, float *c, int width)\n{\n  for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          float temp = 0;\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              temp += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = float;\n        } \n    }   \n}\n</code></pre>   <pre><code>void Matrix_Multiplication(float *restrict a, float *restrict b, float *restrict c, int width)\n{\n  int length = width*width;\n  float sum = 0;\n#pragma acc parallel copyin(a[0:(length)], b[0:(length)]) copyout(c[0:(length)])\n#pragma acc loop collapse(2) reduction (+:sum)\n for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              sum += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = sum;\n          sum=0;\n        }\n    }\n}       \n</code></pre>     </li> <li> <p>Deallocate the host memory <pre><code>// Deallocate host memory\nfree(a); \nfree(b); \nfree(c);\n</code></pre></p> </li> </ul>"},{"location":"openacc/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial-versionOpenACC-templateOpenACC-version   <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n\nvoid Matrix_Multiplication(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float temp = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              temp += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = temp;                            \n        }                                                         \n    }   \n}\n\nint main()\n{\n\n  printf(\"Programme assumes that matrix size is N*N \\n\");\n  printf(\"Please enter the N size number \\n\");\n  int N =0;\n  scanf(\"%d\", &amp;N);\n\n  // Initialize the memory on the host\n  float *a, *b, *c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * (N*N));\n  b = (float*)malloc(sizeof(float) * (N*N));\n  c = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  Matrix_Multiplication(a, b, c, N);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      for(int j = 0; j &lt; N; j++)\n         {\n          printf(\"%f \", c[j]);\n         }\n      printf(\"%f \", c[j]);\n   }\n\n  // Deallocate host memory\n free(a); \n free(b); \n free(c);\n\n return 0;\n}\n</code></pre>   <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;openacc.h&gt;\n#include&lt;stdbool.h&gt;\n\nvoid Matrix_Multiplication(float *restrict a, float *restrict b, float *restrict c, int width)\n{\n  int length = width*width;\n  float sum = 0;\n//#pragma acc ....\n//#pragma acc ....\n for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              sum += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = sum;\n          sum=0;\n        }\n    }\n}    \n\n\n// Host call (matrix multiplication)\nvoid CPU_Matrix_Multiplication(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n}\n\nint main()\n{\n\n  printf(\"Programme assumes that matrix size is N*N \\n\");\n  printf(\"Please enter the N size number \\n\");\n  int N =0;\n  scanf(\"%d\", &amp;N);\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  Matrix_Multiplication(a, b, c, N);\n\n\n  // CPU computation for verification \n  Matrix_Multiplication(a, b, host_check, N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)  \n      printf(\"Two matrices are not equal\\n\");\n  else\n      printf(\"Two matrices are equal\\n\");\n\n  // Deallocate host memory\n  free...\n\n  return 0;\n}\n</code></pre>   <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;openacc.h&gt;\n#include&lt;stdbool.h&gt;\n\nvoid Matrix_Multiplication(float *restrict a, float *restrict b, float *restrict c, int width)\n{\n  int length = width*width;\n  float sum = 0;\n#pragma acc parallel copyin(a[0:(length)], b[0:(length)]) copyout(c[0:(length)])\n#pragma acc loop collapse(2) reduction (+:sum)\n for(int row = 0; row &lt; width ; ++row)\n    {\n      for(int col = 0; col &lt; width ; ++col)\n        {\n          for(int i = 0; i &lt; width ; ++i)\n            {\n              sum += a[row*width+i] * b[i*width+col];\n            }\n          c[row*width+col] = sum;\n          sum=0;\n        }\n    }\n}       \n\n\n// Host call (matrix multiplication)\nvoid CPU_Matrix_Multiplication(float *h_a, float *h_b, float *h_c, int width)   \n{                                                                 \n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)                       \n        {                                                         \n          float single_entry = 0;                                       \n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              single_entry += h_a[row*width+i] * h_b[i*width+col];      \n            }                                                     \n          h_c[row*width+col] = single_entry;                            \n        }                                                         \n    }   \n}\n\n\nint main()\n{\n\n  cout &lt;&lt; \"Programme assumes that matrix (square matrix) size is N*N \"&lt;&lt;endl;\n  cout &lt;&lt; \"Please enter the N size number \"&lt;&lt; endl;\n  int N = 0;\n  cin &gt;&gt; N;\n\n  // Initialize the memory on the host\n  float *a, *b, *c, *host_check;\n\n  // Initialize the memory on the device\n  float *d_a, *d_b, *d_c;\n\n  // Allocate host memory\n  a   = (float*)malloc(sizeof(float) * (N*N));\n  b   = (float*)malloc(sizeof(float) * (N*N));\n  c   = (float*)malloc(sizeof(float) * (N*N));\n  host_check = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize host matrix\n  for(int i = 0; i &lt; (N*N); i++)\n    {\n      a[i] = 2.0f;\n      b[i] = 2.0f;\n    }\n\n  // Device function call \n  Matrix_Multiplication(d_a, d_b, d_c, N);\n\n  // cpu computation for verification \n  CPU_Matrix_Multiplication(a,b,host_check,N);\n\n  // Verification\n  bool flag=1;\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n         if(c[j*N+i]!= host_check[j*N+i])\n           {\n             flag=0;\n             break;\n           }\n       }\n    }\n  if (flag==0)\n    {\n      cout &lt;&lt;\"Two matrices are not equal\" &lt;&lt; endl;\n    }\n  else\n    cout &lt;&lt; \"Two matrices are equal\" &lt;&lt; endl;\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n  free(host_check);\n\n  return 0;\n}\n</code></pre>      Compilation and Output Serial-versionOpenACC-version   <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-CPU\n\n// execution \n$ ./Matrix-Multiplication-CPU\n\n// output\n$ g++ Matrix-multiplication.cc -o Matrix-multiplication\n$ ./Matrix-multiplication\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n16 16 16 16 \n16 16 16 16  \n16 16 16 16  \n16 16 16 16 \n</code></pre>   <pre><code>// compilation\n$ nvcc -arch=compute_70 Matrix-multiplication.cu -o Matrix-Multiplication-GPU\nMatrix_Multiplication:\n      9, Generating copyin(a[:length]) [if not already present]\n         Generating copyout(c[:length]) [if not already present]\n         Generating copyin(b[:length]) [if not already present]\n         Generating NVIDIA GPU code\n         12, #pragma acc loop gang collapse(2) /* blockIdx.x */\n             Generating reduction(+:sum)\n         14,   /* blockIdx.x collapsed */\n         16, #pragma acc loop vector(128) /* threadIdx.x */\n            Generating implicit reduction(+:sum)\n     16, Loop is parallelizable\n\n// execution\n$ ./Matrix-Multiplication-GPU\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number\n$ 256\n\n// output\n$ Two matrices are equal\n</code></pre>      Questions <pre><code>- Try to compute different matrix sizes instead of square matrices.\n</code></pre>"},{"location":"openacc/exercise-3/#three-levels-of-parallelism","title":"Three levels of parallelism","text":"<p>By default, the compiler chooses the best combination of the thread blocks needed for the computation. However, sometimes, if needed as a programmer, you could also control the threads block in the program. OpenACC provides straightforward clauses that can control the threads and thread blocks in the application. </p>       OpenACC CUDA Parallelism     num_gangs Grid Block coarse   numn_workers Warps fine   vector_length Threads SIMD or vector     Questions <ul> <li>Change the values in <code>num_gangs()</code>, <code>num_workers()</code> and <code>vector_length()</code> and check if you would see any performance difference compared to the default thread used by a compiler.</li> </ul>"},{"location":"openacc/exercise-4/","title":"Unified Memory","text":"<p>Unified memory simplifies the explicit data movement from host to device by programmers. OpenACC API will manage the data transfer between CPU and GPU. In this example, we will look into vector addition in GPU using the unified memory concept.</p>     <ul> <li>Just using the compiler flag <code>-gpu=managed</code> will enable the unified memory in OpenACC.</li> </ul> <p>The table below summarises the required steps needed for the unified memory concept.</p>  <p>Unified Memory</p> C/C++FORTRAN   <pre><code>nvc -fast -acc=gpu -gpu=cc80 -gpu=managed -Minfo=accel test.c\n</code></pre>   <pre><code>nvfortran -fast -acc=gpu -gpu=cc80 -gpu=managed -Minfo=accel test.c\n</code></pre>        Without unified memory With unified memory     Allocate the host memory Allocate the host memory   Initialize the host value Initialize the host value   Use data cluases, e.g,. copy, copyin Use data cluases, e.g,. copy, copyin   Do the computation using the GPU kernel Do the computation using the GPU kernel   Free host memory Free host memory"},{"location":"openacc/exercise-4/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition OpenACC-templateOpenACC-versionOpenACC-template (FORTRAN)OpenACC-version (FORTRAN)   <pre><code>// Vector-addition-template.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;    \n\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// GPU function that adds two vectors \n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n\n// add here either parallel or kernel and do need to add data map clauses\n#pragma acc \nfor(int i = 0; i &lt; n; i ++)\n   {\n     c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>// Vector-addition-openacc.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n#include &lt;openacc.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n\n// function that adds two vector \nvoid Vector_Addition(float *restrict a, float *restrict b, float *restrict c, int n) \n{\n#pragma acc kernels loop\nfor(int i = 0; i &lt; n; i ++)\n   {\n    c[i] = a[i] + b[i];\n   }\n}\n\nint main()\n{\n  // Initialize the memory on the host\n  float *restrict a, *restrict b, *restrict c;       \n\n  // Allocate host memory\n  a = (float*)malloc(sizeof(float) * N);\n  b = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize host arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing CPU function \n  Vector_Addition(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"PASSED\\n\");\n\n  // Deallocate host memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>!! Vector-addition-openacc.f90\n\nmodule Vector_Addition_Mod\n  implicit none\ncontains\n subroutine Vector_Addition(a, b, c, n)\n    ! Input vectors\n    real(8), intent(in), dimension(:) :: a                        \n    real(8), intent(in), dimension(:) :: b\n    real(8), intent(out), dimension(:) :: c\n    integer :: i, n\n    // add here your acc directive\n    do i = 1, n\n       c(i) = a(i) + b(i)\n    end do\n    !$acc.....\n  end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\n  use openacc\n  use Vector_Addition_Mod\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n  real(8), dimension(:), allocatable :: b \n  ! Output vector\n  real(8), dimension(:), allocatable :: c\n\n  integer :: n, i             \n  print *, \"This program does the addition of two vectors \"\n  print *, \"Please specify the vector size = \" \n  read *, n  \n\n  ! Allocate memory for vector\n  allocate(a(n))\n  allocate(b(n))\n  allocate(c(n))\n\n  ! Initialize content of input vectors, \n  ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\n  do i = 1, n\n     a(i) = sin(i*1D0) * sin(i*1D0)\n     b(i) = cos(i*1D0) * cos(i*1D0) \n  enddo\n\n  ! Call the vector add subroutine \n  call Vector_Addition(a, b, c, n)\n\n  !!Verification\n  do i = 1, n\n     if (abs(c(i)-(a(i)+b(i))==0.00000)) then \n     else\n        print *, \"FAIL\"\n     endif\n  enddo\n  print *, \"PASS\"\n\n  ! Delete the memory\n  deallocate(a)\n  deallocate(b)\n  deallocate(c)\n\nend program main        \n</code></pre>   <pre><code>!! Vector-addition-openacc.f90\n\nmodule Vector_Addition_Mod\n  implicit none\ncontains\n subroutine Vector_Addition(a, b, c, n)\n    ! Input vectors\n    real(8), intent(in), dimension(:) :: a                        \n    real(8), intent(in), dimension(:) :: b\n    real(8), intent(out), dimension(:) :: c\n    integer :: i, n\n    !$acc parallel loop \n    do i = 1, n\n       c(i) = a(i) + b(i)\n    end do\n    !$acc end parallel\n  end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\n  use openacc\n  use Vector_Addition_Mod\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n  real(8), dimension(:), allocatable :: b \n  ! Output vector\n  real(8), dimension(:), allocatable :: c\n\n  integer :: n, i             \n  print *, \"This program does the addition of two vectors \"\n  print *, \"Please specify the vector size = \" \n  read *, n  \n\n  ! Allocate memory for vector\n  allocate(a(n))\n  allocate(b(n))\n  allocate(c(n))\n\n  ! Initialize content of input vectors, \n  ! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\n  do i = 1, n\n     a(i) = sin(i*1D0) * sin(i*1D0)\n     b(i) = cos(i*1D0) * cos(i*1D0) \n  enddo\n\n  ! Call the vector add subroutine \n  call Vector_Addition(a, b, c, n)\n\n  !!Verification\n  do i = 1, n\n     if (abs(c(i)-(a(i)+b(i))==0.00000)) then \n     else\n        print *, \"FAIL\"\n     endif\n  enddo\n  print *, \"PASS\"\n\n  ! Delete the memory\n  deallocate(a)\n  deallocate(b)\n  deallocate(c)\n\nend program main\n</code></pre>      Compilation and Output OpenACC-versionOpenACC-version (FORTRAN)   <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel -gpu=managed Vector-addition-openacc.c -o Vector-Addition-GPU\nVector_Addition:\n12, Generating copyin(a[:n]) [if not already present]\n    Generating copyout(c[:n]) [if not already present]\n    Generating copyin(b[:n]) [if not already present]\n14, Loop is parallelizable\n    Generating NVIDIA GPU code\n    14, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n\n// execution\n$ ./Vector-Addition-GPU\n\n// output\n$ ./Vector-addition-GPU\nPASSED\n</code></pre>   <pre><code>// compilation\n$ nvfortran -fast -acc=gpu -gpu=cc80 -gpu=managed -Minfo=accel Vector-addition-openacc.f90 -o Vector-Addition-GPU\nvector_addition:\n     12, Generating NVIDIA GPU code\n         13, !$acc loop gang, vector(128) ! blockidx%x threadidx%x\n     12, Generating implicit copyin(a(:n)) [if not already present]\n         Generating implicit copyout(c(:n)) [if not already present]\n         Generating implicit copyin(b(:n)) [if not already present\n\n// execution         \n$ ./Vector-Addition-GPU\n\n// output\nThis program does the addition of two vectors \nPlease specify the vector size = \n1000000\nPASS\n</code></pre>      Questions <ul> <li>Do you already see any performance difference? Using unified memory?</li> </ul>"},{"location":"openacc/preparation/","title":"Preparation","text":""},{"location":"openacc/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"openacc/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"openacc/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"openacc/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200117]$cd u100490\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200117/OpenACC .\n[u100490@login03 u100490]$ cd OpenACC/\n[u100490@login03 OpenACC]$ pwd\n/project/home/p200117/u100490/OpenACC\n[u100490@login03 OpenACC]$ ls -lthr\ntotal 20K\n-rw-r-----. 1 u100490 p200117   51 Mar 13 15:50 module.sh\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Vector-addition\ndrwxr-s---. 2 u100490 p200117 4.0K Mar 13 15:50 Unified-memory\n...\n...\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","title":"7. Until now you are in the login node, now its time to do the dry run test","text":"<ul> <li> <p>7.1 Reserve the interactive node for running/testing OpenACC applications    <pre><code>$ salloc -A p200117 --res p200117-openacc-1 --partition=gpu --qos default -N 1 -t 01:00:00\n</code></pre></p>  check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res p200117-openacc-1 --partition=gpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre>  </li> <li> <p>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       gpu interact  u100490    p200117  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></p> </li> </ul>"},{"location":"openacc/preparation/#8-now-we-need-to-check-simple-openacc-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check simple OpenACC application, if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 OpenACC]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nHello-world.cu  module.sh\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-gpu-openacc-codes","title":"9. Finally, we need to load the compiler to test the GPU OpenACC codes","text":"<ul> <li> <p>9.1 We need a Nvidia HPC SDK compiler for compiling and testing OpenACC code  <pre><code>$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n### or\n$ source module.sh\n</code></pre></p>  check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module load OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n[u100490@mel2131 ~]$ module list\n\nCurrently Loaded Modules:\n1) env/release/2022.1           (S)   6) numactl/2.0.14-GCCcore-11.3.0  11) libpciaccess/0.16-GCCcore-11.3.0  16) GDRCopy/2.3-GCCcore-11.3.0                  21) knem/1.1.4.90-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1      (S)   7) CUDA/11.7.0                    12) hwloc/2.7.1-GCCcore-11.3.0        17) UCX-CUDA/1.13.1-GCCcore-11.3.0-CUDA-11.7.0  22) OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0\n3) GCCcore/11.3.0                     8) NVHPC/22.7-CUDA-11.7.0         13) OpenSSL/1.1                       18) libfabric/1.15.1-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0         9) XZ/5.2.5-GCCcore-11.3.0        14) libevent/2.1.12-GCCcore-11.3.0    19) PMIx/4.2.2-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0      10) libxml2/2.9.13-GCCcore-11.3.0  15) UCX/1.13.1-GCCcore-11.3.0         20) xpmem/2.6.5-36-GCCcore-11.3.0\n\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre>  </li> </ul>"},{"location":"openacc/preparation/#10-please-compile-and-test-your-cuda-application","title":"10. Please compile and test your CUDA application","text":"<ul> <li>10.1 For example, Dry-run-test  <pre><code>// compilation\n$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world.c -o Hello-World-GPU\n\n// execution\n$ ./Hello-World-GPU\n\n// output\n$ Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n  Hello World from GPU!\n</code></pre></li> </ul>"},{"location":"openacc/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly for the hands-on session, we need to do the node reservation:","text":"<ul> <li> <p>10.1 For example, reservation  <pre><code>$ salloc -A p200117 --res p200117-openacc-2 --partition=gpu --qos default -N 1 -t 02:30:00\n</code></pre></p>  check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res p200117-openacc-2 --partition=gpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre>  </li> </ul>"},{"location":"openacc/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands on exercise","text":"<ul> <li>12.1 For example, <code>Hello World</code> example, we do the following steps:  <pre><code>[u100490@mel2063 OpenACC]$ pwd\n/project/home/p200117/u100490/OpenACC\n[u100490@mel2063 OpenACC]$ ls\n[u100490@mel2063 OpenACC]$ ls\nDry-run-test  Matrix-multiplication  Profiling      Unified-memory\nHello-world   module.sh              Vector-addition\n[u100490@mel2063 OpenACC]$ source module.sh\n[u100490@mel2063 OpenACC]$ cd Hello-world\n// compilation\n[u100490@mel2063 OpenACC]$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Hello-world.c -o Hello-World-GPU\n\n// execution\n[u100490@mel2063 OpenACC]$ ./Hello-World-GPU\n\n// output\n[u100490@mel2063 OpenACC]$ Hello World from GPU\n</code></pre></li> </ul>"},{"location":"openacc/profiling/","title":"Profiling and Performance","text":"<p>Profiling is an essential procedure to make sure you are utilising the given architecture with a given algorithm. Sometimes, we might think we are doing efficient computation. However, it will not always be the case unless we do the proper profiling and check if all the resources are utilized properly. </p> <p>Using Nvidia HPC SDK, we could profile our OpenACC code. We could do the profiling in two ways: Command line and GUI.</p>"},{"location":"openacc/profiling/#command-line","title":"Command Line","text":"<ul> <li><code>export NVCOMPILER_ACC_TIME=[]</code><ul> <li>[1]: kernel launches</li> <li>[2]: data transfers</li> <li>[4]: region entry/exit</li> <li>[8]: wait for operations or synchronizations</li> <li>[16]: device memory allocates and deallocates</li> </ul> </li> </ul> <p>Setting <code>export NVCOMPILER_ACC_NOTIFY=3</code> provides kernel executions and data transfer information.</p>  <p>Profiling: Compilation</p> Nvidia HPC SDK   <pre><code>// compilation \nVector_Addition:\n     12, Generating NVIDIA GPU code\n     14, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n     12, Generating implicit copyin(a[:n]) [if not already present]\n         Generating implicit copyout(c[:n]) [if not already present]\n         Generating implicit copyin(b[:n]) [if not already present]\n\n//execution        \n[u100@mel2041 Unified-memory]$ ./a.out \nThis program does the addition of two vectors \nupload CUDA data  file=/Vector-addition-openacc.c function=Vector_Addition line=12 device=0 threadid=1 variable=b bytes=400\nupload CUDA data  file=/Vector-addition-openacc.c function=Vector_Addition line=12 device=0 threadid=1 variable=a bytes=400\nlaunch CUDA kernel  file=/Vector-addition-openacc.c function=Vector_Addition line=12 device=0 threadid=1 num_gangs=1 num_workers=1 vector_length=128 grid=1 block=128\ndownload CUDA data  file=/Vector-addition-openacc.c function=Vector_Addition line=17 device=0 threadid=1 variable=c bytes=400\nPASSED  \n</code></pre>"},{"location":"openacc/profiling/#gui","title":"GUI","text":"<p>The Visual Profiler is organized into views. Together, the views allow you to analyze and visualize the performance of your application.  The Timeline View shows CPU and GPU activity that occurred while your application was being profiled.  Multiple timelines can be opened in the Visual Profiler at the same time in different tabs. The following figure shows a Timeline View for a OpenACC application.</p> <p>In order to visualize the performance of your application, you should connect to the HPC machine via -X forward;  otherwise, you will not be able to see the GUI application. For example, on MeluXina, you should do the following.</p>  <p>GUI login</p> <pre><code>$ ssh -X meluxina\n\n$ salloc -A p200117 --res p200117-openacc-2 --partition=gpu --qos default -N 1 -t 00:30:00 srun --forward-x --pty bash -l\n</code></pre>  <p>We also need to add a few extra modules to open a GUI application.  On MeluXina, we need to add the following modules: </p>  <p>Required modules</p> <pre><code>module load NVHPC/22.7\nmodule load CUDA/11.7.0\nmodule load Mesa/22.0.3-GCCcore-11.3.0      \nmodule load Qt5/5.15.5-GCCcore-11.3.0\n</code></pre>  <p>Once the required modules are loaded, you can compile your application and visualize the performance of your application.  Finally, we need use command line <code>nsys-ui</code> to open GUI application and load <code>timeline.nsys-rep</code>. </p>  <p>Compilation and GUI<pre><code>[u100@mel2073 Vector-addition]$ nvc -fast -acc=gpu -gpu=cc80 -Minfo=accel Vector-addition.c\nnvc-Warning-CUDA_HOME has been deprecated. Please, use NVHPC_CUDA_HOME instead.\n[u100@mel2073 Vector-addition]$ nsys profile -o timeline ./a.out\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\nThis program does the addition of two vectors \nPlease specify the vector size = 10000\nPASSED\nGenerating '/tmp/nsys-report-6c02.qdstrm'\n[1/1] [========================100%] timeline.nsys-rep\n\n// Open the GUI application  and load timeline.nsys-rep\n$ nsys-ui &amp;\n</code></pre> </p> <p>   </p>"},{"location":"openmp/","title":"Introduction to OpenMP Programming for Shared Memory Parallel Architecture","text":"<p>Participants from this course will learn Multicore (shared memory) CPU programming using the OpenMP programming model, such as parallel region, environmental routines, and data sharing. Furthermore, understanding the multicore shared memory architecture and how parallel threads blocks are used to parallelise the computational task. Since we deal with multicores and parallel threads, proper parallel work sharing and the synchronisation of the parallel calls are to be studied in detail. Finally, participants will also learn to use the OpenMP programming model to accelerate linear algebra (routines) and iterative solvers on the Multicore CPU. Participants will learn theories first and implement the OpenMP programming model with mentors' guidance later in the hands-on tutorial part.</p>"},{"location":"openmp/#learning-outcomes","title":"Learning outcomes","text":""},{"location":"openmp/#after-this-course-participants-will-be-able-to","title":"After this course, participants will be able to:","text":"<ul> <li>Understanding the shared memory architecture <ul> <li>Unified Memory Access (UMA) and Non-Unified Memory Access (NUMA)  </li> <li>Hybrid distributed shared memory architecture  </li> </ul> </li> <li>Implement OpenMP programming model  <ul> <li>Parallel region  </li> <li>Environment routines  </li> <li>Data sharing  </li> </ul> </li> <li>Efficient handling of OpenMP constructs  <ul> <li>Work sharing  </li> <li>Synchronisation constructs  </li> <li>Single Instruction Multiple Data (SIMD) directive </li> </ul> </li> <li>Apply the OpenMP programming knowledge to parallelise examples from science and engineering: <ul> <li>Iterative solvers from science and engineering  </li> <li>Vector multiplication, vector addition, etc.</li> </ul> </li> </ul>"},{"location":"openmp/#prerequisites","title":"Prerequisites","text":"<p>Priority will be given to users with good experience with C/C++ and/or FORTRAN. No prior parallel programming experience is needed.</p>"},{"location":"openmp/#gpu-compute-resource","title":"GPU Compute Resource","text":"<p>Participants attending the event will be given access to\u202fthe MeluXina supercomputer during the session. To learn more about MeluXina, please consult the Meluxina overview and the MeluXina \u2013 Getting Started Guide.</p>"},{"location":"openmp/exercise-1/","title":"Parallel Region","text":""},{"location":"openmp/exercise-1/#parallel-construct","title":"Parallel Construct","text":"<p>In this exercise, we will create a parallel region and execute the computational content in parallel. First, however, this exercise is to create a parallel region and understand the threads' behaviour in parallel. In later exercises, we will study how to parallelise the computational task within the parallel region.</p>    <p>To create a parallel region, we use the following parallel constructs:</p>  <p>Parallel Constructs</p> C/C++FORTRAN   <pre><code>#pragma omp parallel\n</code></pre>   <pre><code>!$omp parallel \n</code></pre>     <p>The above figure illustrates the parallel region behaviour; as we notice, within the parallel region, we get parallel threads. This means parallel threads can be executed independently of each other, and there is no order of execution.</p> <p>At the same time, in order to enable OpenMP constructs, clauses, and environment variables. etc., we need to include the OpenMP library as follows:</p>  <p>OpenMP library</p> C/C++FORTRAN   <pre><code>#include&lt;omp.h&gt;\n</code></pre>   <pre><code>use omp_lib\n</code></pre>"},{"location":"openmp/exercise-1/#compilers","title":"Compilers","text":"<p>The following compilers would support the OpenMP programming model.</p> <ul> <li>GNU - It is an opensource and can be used for Intel and AMD CPUs</li> <li>Intel - It is from Intel and only optimized for Intel CPUs</li> <li>AOOC - Suitable for AMD CPUs, especially \u201cZen\u201d core architecture.</li> </ul>  <p>Examples (GNU, Intel and AMD): Compilation</p> GNUIntelAOOC   <pre><code>$ gcc test.c -fopenmp\n$ g++ test.cc -fopenmp\n$ gfortran test.f90 -fopenmp\n</code></pre>   <pre><code>$ icc test.c -qopenmp\n$ icpc test.cc -qopenmp\n$ ifort test.f90 -qopenmp        \n</code></pre>   <pre><code>$ clang test.c -fopenmp\n$ clang++ test.cc -fopenmp\n$ flang test.f90 -fopenmp        \n</code></pre>"},{"location":"openmp/exercise-1/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Hello World Serial-version (C/C++)Serial-version (FORTRAN)OpenMP-version (C/C++)OpenMP-version (FORTRAN)   <pre><code>#include&lt;iostream&gt;\nusing namespace std;\n\nint main()\n{\n  cout &lt;&lt; endl;\n  cout &lt;&lt; \"Hello world from master thread\"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre>   <pre><code>program Hello_world_Serial\n\nprint *, 'Hello world from master thread'\n\nend program\n</code></pre>   <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        cout &lt;&lt; \"Hello world from parallel region \"&lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre>   <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\nprint *, 'Hello world from master thread'\n\n!$omp parallel\nprint *, 'Hello world from parallel region'\n!$omp end parallel\n\nprint *,'end of the programme from master thread'\n\nend program\n</code></pre>      Compilation and Output Serial-version (C/C++)Serial-version (FORTRAN)OpenMP-version (C/C++)OpenMP-version (FORTRAN)   <pre><code>// compilation\n$ g++ Hello-world-Serial.cc -o Hello-World-Serial\n\n// execution \n$ ./Hello-World-Serial\n\n// output\n$ Hello world from master thread\n</code></pre>   <pre><code>// compilation\n$ gfortran Hello-world-Serial.f90 -o Hello-World-Serial\n\n// execution \n$ ./Hello-World-Serial\n\n// output\n$ Hello world from master thread\n</code></pre>   <pre><code>// compilation\n$ g++ -fopenmp Hello-world-OpenMP.cc -o Hello-World-OpenMP\n\n// execution\n$ ./Hello-World-OpenMP\n\n// output\n$ Hello world from parallel region\nHello world from parallel region\n..\n..\nHello world from parallel region\n\nend of the programme from master thread\n</code></pre>   <pre><code>// compilation\n$ gfortran -fopenmp Hello-world-OpenMP.f90 -o Hello-World-OpenMP\n\n// execution\n$ ./Hello-World-OpenMP\n\n// output\n$ Hello world from master thread\nHello world from parallel region\n..\n..\nHello world from parallel region\nend of the programme from master thread\n</code></pre>      Questions <ul> <li>What do you notice from those examples? Can you control parallel region printout, that is, how many times it should be printed or executed?     </li> <li>What happens if you do not use the OpenMP library, <code>#include&lt;omp.h&gt; or use omp_lib</code>?</li> </ul>  <p>Although creating a parallel region would allow us to do the parallel computation, however, at the same time, we should have control over the threads being created in the parallel region, for example, how many threads are needed for a particular computation, thread number, etc. For this, we need to know a few of the important environment routines which are provided by OpenMP. The below list shows a few of the most important environment routines that should be known by the programmer for optimised OpenMP coding.</p>"},{"location":"openmp/exercise-1/#environment-routines-important","title":"Environment Routines (important)","text":"<ul> <li> <p>Define the number of threads to be used within the parallel region</p> <pre><code>(C/C++): void omp_set_num_threads(int num_threads);\n(FORTRAN): subroutine omp_set_num_threads(num_threads) \ninteger num_threads\n</code></pre> </li> <li> <p>To get the number of threads in the current parallel region</p> <pre><code>(C/C++): int omp_get_num_threads(void);\n(FORTRAN): integer function omp_get_num_threads()\n</code></pre> </li> <li> <p>To get available maximum threads (system default)</p> <pre><code>(c/c++): int omp_get_max_threads(void);\n(FORTRAN): integer function omp_get_max_threads()\n</code></pre> </li> <li> <p>To get thread numbers (e.g., 1, 4, etc.)</p> <pre><code>(c/c+): int omp_get_thread_num(void);\n(FORTRAN): integer function omp_get_thread_num()\n</code></pre> </li> <li> <p>To know the number processors available to the device</p> <pre><code>(c/c++): int omp_get_num_procs(void);\n(FROTRAN): integer function omp_get_num_procs()\n</code></pre> </li> </ul>"},{"location":"openmp/exercise-1/#questions-and-solutions_1","title":"Questions and Solutions","text":"Questions <ul> <li>How can you identify the thread numbers within the parallel region?</li> <li>What happens if you not set <code>omp_set_num_threads()</code>, for example, <code>omp_set_num_threads(5)|call omp_set_num_threads(5)</code>, what do you notice? </li> <li>Alternatively, you can also set a number of threads to be used in the application while the compilation <code>export OMP_NUM_THREADS</code>; what do you see?</li> </ul> Question (C/C++)Question (FORTRAN)Answer (C/C++)Answer (FORTRAN)AnswerSolution Output (C/C++)Solution Output (FORTRAN)   <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        //cout &lt;&lt; \"Hello world from thread id \"\n        &lt;&lt; \" from the team size of \"\n        &lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre>   <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\n!$omp parallel \n!! print *, \n!$omp end parallel\n\nend program\n</code></pre>   <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        cout &lt;&lt; \"Hello world from thread id \"\n        &lt;&lt; omp_get_thread_num() &lt;&lt; \" from the team size of \"\n        &lt;&lt; omp_get_num_threads()\n        &lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre>   <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\n!$omp parallel \nprint *, 'Hello world from thread id ', omp_get_thread_num(), 'from the team size of', omp_get_num_threads()\n!$omp end parallel\n\nend program\n</code></pre>   <pre><code>$ export OMP_NUM_THREADS=10\n// or \n$ setenv OMP_NUM_THREADS 4\n// or\n$ OMP NUM THREADS=4 ./omp code.exe\n</code></pre>   <pre><code>ead id Hello world from thread id Hello world from thread id 3 from the team size of 9 from the team size of 52 from the team size of  from the team size of 10\n0 from the team size of 10\n10\n10\n10\n7 from the team size of 10\n4 from the team size of 10\n8 from the team size of 10\n1 from the team size of 10\n6 from the team size of 10\n</code></pre>   <pre><code>Hello world from thread id            0 from the team size of          10\nHello world from thread id            4 from the team size of          10\nHello world from thread id            5 from the team size of          10\nHello world from thread id            9 from the team size of          10\nHello world from thread id            2 from the team size of          10\nHello world from thread id            3 from the team size of          10\nHello world from thread id            7 from the team size of          10\nHello world from thread id            6 from the team size of          10\nHello world from thread id            8 from the team size of          10\nHello world from thread id            1 from the team size of          10\n</code></pre>"},{"location":"openmp/exercise-1/#utilities","title":"Utilities","text":"<p>The main aim is to do the parallel computation to speed up computation on a given parallel architecture. Therefore, measuring the timing and comparing the solution between serial and parallel code is very important. In order to measure the timing, OpenMP provides an environmental variable, <code>omp_get_wtime()</code>.</p>  Time measuring C/C++FORTRAN   <pre><code>double start; \ndouble end; \nstart = omp_get_wtime(); \n... work to be timed ... \nend = omp_get_wtime(); \nprintf(\"Work took %f seconds\\n\", end - start);\n</code></pre>   <pre><code>DOUBLE PRECISION START, END \nSTART = omp_get_wtime() \n... work to be timed ... \nEND = omp_get_wtime() \nPRINT *, \"Work took\", END - START, \"seconds\"        \n</code></pre>"},{"location":"openmp/exercise-2/","title":"Data Sharing Attribute","text":""},{"location":"openmp/exercise-2/#shared-variable","title":"Shared variable","text":"<ul> <li>All the threads have access to the shared variable.</li> <li>By default, in the parallel region, all the variables are considered shared variables except the loop iteration counter variables.</li> </ul>  <p>Note</p> <p>Shared variables should be handled carefully; otherwise, it causes race conditions in the program.</p>      Examples: Shared variable (C/C++)(FORTRAN)   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  // Array size\n  int N = 10;\n\n  // Initialize the variables\n  float *a;\n\n  // Allocate the memory\n  a  = (float*)malloc(sizeof(float) * N);\n\n  //#pragma omp parallel for\n  // or \n#pragma omp parallel for shared(a)\n  for (int i = 0; i &lt; N; i++)\n    {\n      a[i] = a[i] + i;  \n      cout &lt;&lt; \"value of a in the parallel region\" &lt;&lt; a[i] &lt;&lt; endl;\n    }\n\n  for (int i = 0; i &lt; N; i++)\n    cout &lt;&lt; \"value of a after the parallel region \" &lt;&lt; a[i] &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre>   <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n\n  integer :: n, i\n  n=10\n\n  ! Allocate memory for vector\n  allocate(a(n))\n\n  !$omp parallel shared(a)\n  !$omp do\n  do i = 1, n\n      a(i) = a(i) + i\n      print*, 'value of a in the parallel region', a(i)\n  end do\n  !$omp end do\n  !$omp end parallel\n\n  do i = 1, n\n      a(i) = a(i) + i\n      print*,'value of a after the parallel region', a(i)\n  end do\n\n  ! Delete the memory\n  deallocate(a)\n\nend program main\n</code></pre>      Questions <ul> <li>Does the value of vector <code>a</code> change after the parallel loop, if not why, think?</li> <li>Do we really need to mention <code>shared(a)</code>, is it neccessary? </li> </ul>"},{"location":"openmp/exercise-2/#private-variable","title":"Private variable","text":"<ul> <li>Each thread will have its own copy of the private variable.</li> <li>And the private variable is only accessible within the parallel region,  not outside of the parallel region.</li> <li>By default, the loop iteration counters are considered as a private.</li> <li>A change made by one thread is not visible to other threads.</li> </ul>     Examples: Private variable (C/C++)(FORTRAN)   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  // Array size\n  int N = 10;\n\n  // Initialize the variables\n  float *a,b,c;\n  b = 1.0;\n  c = 2.0;\n\n  // Allocate the memory\n  a  = (float*)malloc(sizeof(float) * N);\n\n#pragma omp parallel for private(b,c)\n  for (int i = 0; i &lt; N; i++)\n    {\n      b = a[i] + i;\n      c = b + 10 * i;\n      cout &lt;&lt; \"value of c in the parallel region \" &lt;&lt; c &lt;&lt; endl;\n    }\n\n  cout &lt;&lt; \"value of c after the parallel region \" &lt;&lt; c &lt;&lt; endl; \n\n  return 0;\n}\n</code></pre>   <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n\n  real(8) :: b, c\n  integer :: n, i  \n  n=10\n  b=1.0\n  c=2.0\n\n  ! Allocate memory for vector\n  allocate(a(n))\n\n  !$omp parallel private(b,c) shared(a)\n  !$omp do\n  do i = 1, n\n      b = a(i) + i\n      c = b + 10 * i\n      print*, 'value of c in the parallel region', c\n  end do\n  !$omp end do\n  !$omp end parallel\n  print*, 'value of c after the parallel region', c\n\n  ! Delete the memory\n  deallocate(a)\n\nend program main\n</code></pre>      Questions <ul> <li>What is the value of the varible <code>a</code> in the parallel region and after the parallel region?</li> <li>After the parallel region, does variable <code>a</code> has been updated or not? </li> </ul>"},{"location":"openmp/exercise-2/#lastprivate","title":"Lastprivate","text":"<ul> <li>lastprivate: is also similar to a private clause</li> <li>But each thread will have an uninitialized copy of the variables passed  as lastprivate</li> <li>At the end of the parallel loop or sections, the final variable value will  be the last thread accessed value in the section or in a parallel loop.</li> </ul>  Examples: Lastprivate variable (C/C++)(FORTRAN)   <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int n = 10;\n  int var = 5;\n  omp_set_num_threads(10);\n#pragma omp parallel for lastprivate(var)\n  for(int i = 0; i &lt; n; i++)\n    {\n      var += omp_get_thread_num();\n      cout &lt;&lt; \" lastprivate in the parallel region \" &lt;&lt; var &lt;&lt; endl;\n    } /*-- End of parallel region --*/\n  cout &lt;&lt; \"lastprivate after the parallel region \" &lt;&lt; var &lt;&lt;endl;\n\n  return 0;\n}\n</code></pre>   <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Initialise the variable\n  real(8) :: var\n  integer :: n, i  \n  n = 10\n  var = 5\n\n  call omp_set_num_threads(10)\n\n  !$omp parallel \n  !$omp do lastprivate(var)\n  do i = 1, n\n     var  =  var + omp_get_thread_num()\n     print*, 'lastprivate in the parallel region ', var\n  end do\n  !$omp end do\n  !$omp end parallel\n\n  print*, 'lastprivate after the parallel region ', var\n\nend program main\n</code></pre>      Questions <ul> <li>What is the value of the varible <code>var</code> in the parallel region and after the parallel region?</li> <li>Do you think the initial value of varibale <code>var</code> is been considered within the parallel region? </li> </ul>"},{"location":"openmp/exercise-2/#firstprivate","title":"Firstprivate","text":"<ul> <li>firstprivate: is similar to a private clause</li> <li>But each thread will have an initialized copy of the variables passed  as firstprivate</li> <li>Available for parallel constructs, loop, sections and single  constructs</li> </ul>  Examples: Firstprivate variable (C/C++)(FORTRAN)   <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int n = 10;\n  int var = 5;\n  omp_set_num_threads(10);\n#pragma omp parallel for firstprivate(var)\n  for(int i = 0; i &lt; n; i++)\n    {\n      var += omp_get_thread_num();\n      cout &lt;&lt; \" lastprivate in the parallel region \" &lt;&lt; var &lt;&lt; endl;\n    } /*-- End of parallel region --*/\n  cout &lt;&lt; \"lastprivate after the parallel region \" &lt;&lt; var &lt;&lt;endl;\n\n  return 0;\n}\n</code></pre>   <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Initialise the variable\n  real(8) :: var\n  integer :: n, i  \n  n = 10\n  var = 5\n\n  call omp_set_num_threads(10)\n\n  !$omp parallel \n  !$omp do firstprivate(var)\n  do i = 1, n\n     var  =  var + omp_get_thread_num()\n     print*, 'lastprivate in the parallel region ', var\n  end do\n  !$omp end do\n  !$omp end parallel\n\n  print*, 'lastprivate after the parallel region ', var\n\nend program main\n</code></pre>      Questions <ul> <li>What is the value of the varible <code>var</code> in the parallel region and after the parallel region?</li> <li>Is variable <code>var</code> has been updated after the parallel region, if not why, think?</li> </ul>"},{"location":"openmp/exercise-3/","title":"Work Sharing Constructs(loop)","text":""},{"location":"openmp/exercise-3/#serial-version-discussion","title":"Serial version discussion","text":"<p>To begin to understand the work-sharing constructs, we need to learn how to parallelise the <code>for - C/C++</code> or <code>do - FORTRAN</code> loop. For this, we will learn simple vector addition examples.</p>     <p>As we can see from the above figure, the two vectors should be added to get a single vector. This is done by iterating over the elements and adding them together. For this, we use <code>for - C/C++</code> or <code>do - FORTRAN</code>.  Since there are no data dependencies, the loop indexes do not have any data dependency on the other indexes. Therefore, it is easy to parallelise.</p>  Examples: Loop Serial(C/C++)Serial(FORTRAN)   <pre><code>for(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n</code></pre>   <pre><code>do i = 1, n\n  c(i) = a(i) + b(i)\nend do\n</code></pre>      <p>Note</p> <p>FORTRAN has a column-major order and C/C++ has a row-major order</p> <pre><code>Fortran array index starts from 1\nC/C++ arrray index starts from 0\n</code></pre>"},{"location":"openmp/exercise-3/#parallel-version-discussion","title":"Parallel version discussion","text":"<p>Now we will look into the how to parallelise the <code>for - C/C++</code> or <code>do - FORTRAN</code> loops. For this, we just need to add below syntax (OpenMP directives).</p>    Functionality Syntax in C/C++ Syntax in FORTRAN     Distribute iterations over the threads #pragma omp for !$omp do    <p>With the help of the above syntax the loops can be easily parallelised. The figure below shows an example of how the loops are parallelised. As we can notice here, we set the <code>omp_set_num_threads(5)</code> for the number of parallel threads that should be used within the loops. Furthermore, the loop index goes from <code>0</code> to <code>9</code>; in total, we need to iterate <code>10</code> elements. </p> <p>In this example, using <code>5</code> threads would divide <code>10</code> iterations by <code>two</code>. Therefore, each thread will handle <code>2</code> iterations. In total, <code>5</code> threads will do just <code>2</code> iterations in parallel for <code>10</code> elements.  </p>      Examples: Loops parallelisation Serial(C/C++)FORTRAN(C/C++)   <pre><code>#pragma omp parallel for\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n\n//or\n\n#pragma omp parallel \n#pragma omp for\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n</code></pre>   <pre><code>!$omp parallel do\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end parallel do\n\n//or\n\n!$omp parallel\n!$omp do\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end do\n!$omp end parallel\n</code></pre>     <p>From understating loop parallelisation, we will continue with vector operations in parallel, that is, adding two vectors. It is very simple, and we just need to add the <code>#pragma omp parallel for</code> for C/C++, <code>!$omp parallel do</code> for FORTRAN. Could you try this by yourself? The serial code, templates and compilation command have been provided as follows.</p>"},{"location":"openmp/exercise-3/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Vector Addition Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN)   <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing vector addtion function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>   <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n// ADD YOUR PARALLEL REGION FOR THE LOOP\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // ADD YOUR PARALLEL REGION HERE  \n  // Executing vector addtion function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!! ADD YOUR PARALLEL DO LOOP\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!! ADD YOUR PARALLEL REGION \n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>   <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n#pragma omp for\n// ADD YOUR PARALLE\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  #pragma omp parallel \n  // Executing vector addtion function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!$omp do\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n!$omp end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!$omp parallel \n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n!$omp end parallel\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>      Compilation and Output Serial(C/C++)Serial(FORTRAN)Solution(C/C++)Solution(FORTRAN)   <pre><code>// compilation\n$ gcc Vector-addition-Serial.c -o Vector-addition-Serial\n\n// execution \n$ ./Vector-addition-Serial\n\n// output\n$ ./Vector-addition-Serial\n</code></pre>   <pre><code>// compilation\n$ gfortran Vector-addition-Serial.f90 -o Vector-addition-Serial\n\n// execution\n$ ./Vector-addition-Serial\n\n// output\n$ ./Vector-addition-Serial\n</code></pre>   <pre><code>// compilation\n$ gcc -fopennmp Vector-addition-OpenMP-solution.c -o Vector-addition-Solution\n\n// execution \n$ ./Vector-addition-Solution\n\n// output\n$ ./Vector-addition-Solution\n</code></pre>   <pre><code>// compilation\n$ gfortran -fopenmp Vector-addition-OpenMP-solution.f90 -o Vector-addition-Solution\n\n// execution\n$ ./Vector-addition-Solution\n\n// output\n$ ./Vector-addition-Solution\n</code></pre>      Questions <ul> <li>Can you measure the performance speedup for parallelising loop? Do you see any speedup?</li> <li>For example, can you create more threads to speed up the computation? If yer or not, why?</li> </ul>"},{"location":"openmp/exercise-4/","title":"Work Sharing Constructs(loop-scheduling)","text":""},{"location":"openmp/exercise-4/#loop-scheduling","title":"Loop scheduling","text":"<p>However, the above example is very simple.    Because, in most cases, we would end up doing a large list of arrays with complex computations within the loop.    Therefore, the work loading should be optimally distributed among the threads in those cases.    To handle those considerations, OpenMP has provided the following loop-sharing clauses. They are: <code>Static</code>, <code>Dynamic</code>, <code>Guided</code>, <code>Auto</code>, and, <code>Runtime</code>.</p>      Example - Loop scheduling clauses Serial(C/C++)FORTRAN(C/C++)   <pre><code>#pragma omp parallel for schedule(static)\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n\n//or\n\n#pragma omp parallel \n#pragma omp for schedule(static)\nfor(int i = 0; i &lt; n; i ++)\n  {\n    c[i] = a[i] + b[i];\n  }\n</code></pre>   <pre><code>!$omp parallel do schedule(static)\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end parallel do\n\n//or\n\n!$omp parallel\n!$omp do schedule(static)\ndo i = 1, n\n  c(i) = a(i) + b(i)\nend do\n!$omp end do\n!$omp end parallel\n</code></pre>"},{"location":"openmp/exercise-4/#static","title":"Static","text":"<ul> <li>The number of iterations are divided by chunksize. </li> <li>If the chunksize is not provided, a number of iterations will be divided by the size of the team of threads.<ul> <li>e.g., n=100, numthreads=5; each thread will execute the 20 iterations in parallel.</li> </ul> </li> <li>This is useful when the computational cost is similar to each iteration.</li> </ul>  Examples and Question: static OpenMP(C/C++)OpenMP(FORTRAN)Output   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(static)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre>   <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(static)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre>   <pre><code>Thread id           0\nThread id           0\nThread id           4\nThread id           4\nThread id           3\nThread id           3\nThread id           2\nThread id           2\nThread id           1\nThread id           1\n</code></pre>    <ul> <li>What happens if you would set the chunksize, for example, <code>schedule(static,4)</code>? What do you notice?</li> </ul>"},{"location":"openmp/exercise-4/#dynamic","title":"Dynamic","text":"<ul> <li>The number of iterations are divided by chunksize.</li> <li>If the chunksize is not provided, the default value will be considered 1.</li> <li>This is useful when the computational cost is different in the iteration.</li> <li>This will quickly place the chunk of data in the queue.</li> </ul>  Examples and Question: dynamic OpenMP(C/C++)OpenMP(FORTRAN)Output   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(dynamic)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre>   <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(dynamic)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre>   <pre><code>Thread id  Thread id 20 Thread id \n4 Thread id 2\nThread id 2\nThread id 2\nThread id 2\nThread id 2\nThread id\nThread id 1 \n3\n</code></pre>    <ul> <li>What happens if you would set the chunksize, for example, schedule(dynamic,4)? What do you notice?</li> <li>Do you notice if the iterations are divided by the chunksize that we set?</li> </ul>"},{"location":"openmp/exercise-4/#guided","title":"Guided","text":"<ul> <li>Similar to dynamic scheduling, the number of iterations are divided by chunksize.</li> <li>But the chunk of the data size is decreasing, which is proportional to the number of unsigned iterations divided by the number of threads.</li> <li>If the chunksize is not provided, the default value will be considered 1.</li> <li>This is useful when there is poor load balancing at the end of the iteration.</li> </ul>  Examples and Question: guided OpenMP(C/C++)OpenMP(FORTRAN)Output   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(guided)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre>   <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(guided)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre>   <pre><code>Thread id Thread id   Thread id0 41\nThread id 0\n\nThread id 4\nThread id 4\nThread id 2\nThread id 2\nThread id 3 Thread id\n</code></pre>    <ul> <li>Are there any differences between <code>auto</code> and <code>guided</code> or <code>dynamic</code>?</li> </ul>"},{"location":"openmp/exercise-4/#auto","title":"Auto","text":"<ul> <li>Here the compiler chooses the best combination of the chunksize to be used. </li> </ul>  Examples and Question: auto OpenMP(C/C++)OpenMP(FORTRAN)Output   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(auto)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre>   <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(auto)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre>   <pre><code>Thread id Thread id Thread id    Thread id0 34 Thread id \nThread id 0\n1\n Thread id 1\n\n Thread id 3\n2\n Thread id 2\n\n Thread id 4\n</code></pre>    <ul> <li>What would you choose for your application, auto, dynamic, guided, or static? If you are going to choose either one of them, then have a valid reason. </li> </ul>"},{"location":"openmp/exercise-4/#runtime","title":"Runtime","text":"<ul> <li>During the compilation, we simply set the loop scheduling concept.</li> </ul>  Example:Loop scheduling clauses - runtime Compilation   <pre><code>setenv OMP_SCHEDULE=\"guided,4\" \nsetenv OMP_SCHEDULE=\"dynamic\" \nsetenv OMP_SCHEDULE=\"nonmonotonic:dynamic,4\"\n// or\nexport OMP_SCHEDULE=\"guided,4\" \nexport OMP_SCHEDULE=\"dynamic\" \nexport OMP_SCHEDULE=\"nonmonotonic:dynamic,4\"\n</code></pre>      Examples and Question: runtime OpenMP(C/C++)OpenMP(FORTRAN)Compilation   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nint main()\n{\n int N = 10;\n omp_set_num_threads(5);\n\n#pragma omp parallel for schedule(runtime)\nfor(int i = 0; i &lt; N; i++)\n   {\n    cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;    \n   }  \n  return 0;\n}\n</code></pre>   <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i  \nn = 10\n\ncall omp_set_num_threads(5)\n\n!$omp parallel\n!$omp do schedule(runtime)\ndo i = 1, n\n  print*, 'Thread id', omp_get_thread_num()\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre>   <pre><code>export OMP_SCHEDULE=\"dynamic,3\"\n// check if you have exported the environment value\n$ env | grep OMP_SCHEDULE\n$ OMP_SCHEDULE=dynamic,3 \n// if you want to unset\n$ unset OMP_SCHEDULE\n$ env | grep OMP_SCHEDULE\n// it(OMP_SCHEDULE=dynamic,3) will be removed\n</code></pre>"},{"location":"openmp/exercise-5/","title":"Worksharing Constructs(others)","text":"<p>Most of the time, we end up having more than one loop, a nested loop, where two or three loops will be next to each other. OpenMP provides a clause for handling this kind of situation with <code>collapse</code>. To understand this, we will now study Matrix multiplication, which involves a nested loop. Again, most of the time, we might do computation with a nested loop. Therefore, studying this example would be good practice for solving the nested loop in the future.</p>"},{"location":"openmp/exercise-5/#collapse","title":"Collapse","text":"<p>The collapse clause can be used for the nested loop; an entire part of the iteration will be divided by an available number of threads. If the outer loop is equal to the available threads, then the outer loop will be divided number of threads. The figure below shows an example of not using the <code>collapse</code> clause. Therefore, only the outer loop is parallelised; each outer loop index will have N number of inner loop iterations. </p>     <p>This is not what we want. Instead, with the available threads, we would like to parallelise the loops as efficiently as we could. Moreover, most of the time, we might have more threads available on a machine; for example, on MeluXina, we can have up to 256 threads. Therefore, when adding the <code>collapse</code> clause, we notice that the available threads execute every single iteration, as seen in the figure below.</p>      Collapse C/C++FORTRAN   <pre><code>#pragma omp parallel\n#pragma omp for collapse(2)\n  for(int i = 0; i &lt; N; i++)\n     {\n      for(int j = 0; j &lt; N; j++)\n        {     \n         cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n        }\n    }\n\n// Or\n\n#pragma omp parallel for collapse(2)\n  for(int i = 0; i &lt; N; i++)\n    {\n      for(int j = 0; j &lt; N; j++)\n        { \n        cout &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n        }\n    }\n</code></pre>   <pre><code>!$omp parallel\n!$omp do collapse(2) \ndo i = 1, n\n   do j = 1, n\n      print*, 'Thread id', omp_get_thread_num()\n   end do\nend do\n!$omp end do\n!$omp end parallel\n\n!! Or\n\n!$omp parallel do collapse(2)\ndo i = 1, n\n   do j = 1, n\n      print*, 'Thread id', omp_get_thread_num()\n   end do\nend do\n!$omp end parallel do\n</code></pre>      Examples and Questions: Collapse OpenMP(C/C++)OpenMP(FORTRAN)Output(FORTRAN)   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int N=5;\n\n#pragma omp parallel\n#pragma omp for collapse(2)\n  for(int i = 0; i &lt; N; i++)\n    {\n     for(int j = 0; j &lt; N; j++)\n       {\n        cout &lt;&lt; \"Outer loop id \" &lt;&lt; i &lt;&lt; \" Inner loop id \"&lt;&lt; j &lt;&lt; \" Thread id\" &lt;&lt; \" \" &lt;&lt; omp_get_thread_num() &lt;&lt; endl;\n       }\n    }\n\n  return 0;\n}\n</code></pre>   <pre><code>program main\nuse omp_lib\nimplicit none\n\ninteger :: n, i, j  \nn=5\n\n!$omp parallel\n!$omp do collapse(2) \ndo i = 1, n\n   do j = 1, n\n      print*, 'Outer loop id ', i , 'Inner loop id ', j , 'Thread id', omp_get_thread_num()\n   end do\nend do\n!$omp end do\n!$omp end parallel\n\nend program main\n</code></pre>   <pre><code>Outer loop id            4 Inner loop id            2 Thread id          16\nOuter loop id            1 Inner loop id            4 Thread id           3\nOuter loop id            5 Inner loop id            1 Thread id          20\nOuter loop id            4 Inner loop id            1 Thread id          15\nOuter loop id            2 Inner loop id            1 Thread id           5\nOuter loop id            3 Inner loop id            1 Thread id          10\nOuter loop id            3 Inner loop id            4 Thread id          13\nOuter loop id            4 Inner loop id            4 Thread id          18\nOuter loop id            4 Inner loop id            3 Thread id          17\nOuter loop id            3 Inner loop id            3 Thread id          12\nOuter loop id            1 Inner loop id            2 Thread id           1\nOuter loop id            2 Inner loop id            3 Thread id           7\nOuter loop id            1 Inner loop id            5 Thread id           4\nOuter loop id            2 Inner loop id            2 Thread id           6\nOuter loop id            3 Inner loop id            2 Thread id          11\nOuter loop id            2 Inner loop id            5 Thread id           9\nOuter loop id            3 Inner loop id            5 Thread id          14\nOuter loop id            5 Inner loop id            3 Thread id          22\nOuter loop id            5 Inner loop id            4 Thread id          23\nOuter loop id            5 Inner loop id            5 Thread id          24\nOuter loop id            2 Inner loop id            4 Thread id           8\nOuter loop id            1 Inner loop id            3 Thread id           2\nOuter loop id            4 Inner loop id            5 Thread id          19\nOuter loop id            1 Inner loop id            1 Thread id           0\nOuter loop id            5 Inner loop id            2 Thread id          21\n</code></pre>    <ul> <li>Can you add here any of the scheduling clauses, for example, static, dynamic, etc?</li> <li>Is it really necessary to them when you use the <code>collapse</code>, or is it dependent on other factors, such as the nature of the    computation and available threads?</li> </ul>"},{"location":"openmp/exercise-5/#reduction","title":"Reduction","text":"<p>The reduction clauses are data-sharing attribute clauses that can be used to perform some forms of repetition calculations in the parallel region.</p> <ul> <li>it can be used for arithmetic reductions: +,*,-,max,min</li> <li>and also with logical operator reductions in C: &amp; &amp;&amp; | || \u02c6</li> </ul>  Reduction C/C++FORTRAN   <pre><code>#pragma omp parallel\n#pragma omp for reduction(+:sum)\n  for(int i = 0; i &lt; N; i++)\n     {\n      sum +=a[i];\n     }\n\n// Or\n\n#pragma omp parallel for reduction(+:sum)\n  for(int i = 0; i &lt; N; i++)\n    {\n     sum += a[i];\n    }\n</code></pre>   <pre><code>!$omp parallel\n!$omp do reduction(+:sum)\ndo i = 1, n\n   sum = sum + a(i)\nend do\n!$omp end do\n!$omp end parallel\n\n!! Or\n\n!$omp parallel do reduction(+:sum)\ndo i = 1, n\n   sum = sum + a(i)\nend do\n!$omp end parallel do\n</code></pre>      Examples and Question: Reduction OpenMP(C/C++)OpenMP(FORTRAN)   <pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nusing namespace std;\n\nint main()\n{\n  int sum,N = 10;\n  float *a = (float*)malloc(sizeof(float) * N);\n\n#pragma omp parallel for reduction(+:sum)\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = i;\n      sum += a[i];\n    }\n  cout &lt;&lt; \"Sum is \"&lt;&lt; sum &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre>   <pre><code>program main\n  use omp_lib\n  implicit none\n\n  ! Input vectors\n  real(8), dimension(:), allocatable :: a\n\n  integer :: n, i, sum\n  n=10\n\n  ! Allocate memory for vector\n  allocate(a(n))\n\n  !$omp parallel do reduction(+:sum)\n  do i = 1, n\n      a(i) = i\n      sum = sum + a(i)\n  end do\n  !$omp end parallel do\n\n  print *, 'Sum is ', sum\n\nend program main\n</code></pre>    <ul> <li>What happens if you do not use the reduction clause? Do we still get the correct answer?</li> </ul>"},{"location":"openmp/exercise-5/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>In this example, we consider a square matrix; <code>M=N</code> is equal for both <code>A</code> and <code>B</code> matrices. Even though we deal here with a 2D matrix, we create a 1D array to represent a 2D matrix. In this example,  we must use <code>collapse</code> clause since matrix multiplication deals with 3 loops. The first 2 outer loops will take rows of the <code>A</code> matrix and columns of the <code>B</code> matrix. Therefore, these two loops can be easily parallelised. But then we need to sum the value of those two outer loops value finally; this is where we should use the <code>reduction</code> clause. </p>  matrix multiplication function call C/C++FORTRAN   <pre><code>for(int row = 0; row &lt; width ; ++row)                           \n   {                                                             \n     for(int col = 0; col &lt; width ; ++col)\n       {\n         sum=0;\n         for(int i = 0; i &lt; width ; ++i)                         \n           {                                                     \n             sum += a[row*width+i] * b[i*width+col];      \n           }                                                     \n         c[row*width+col] = sum;                           \n       }\n   } \n</code></pre>   <pre><code>do row = 0, width-1\n   do col = 0, width-1\n      sum=0\n      do i = 0, width-1\n         sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n      enddo\n      c(row*width+col+1) = sum\n   enddo\nenddo\n</code></pre>"},{"location":"openmp/exercise-5/#questions-and-solutions","title":"Questions and Solutions","text":"Examples: Matrix Multiplication Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN)   <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\n\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   \n{ \n  float sum = 0;\n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)\n        {\n          sum=0;\n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              sum += a[row*width+i] * b[i*width+col];      \n            }                                                     \n          c[row*width+col] = sum;                           \n        }\n    }   \n}\n\nint main()\n {  \n   printf(\"Programme assumes that matrix size is N*N \\n\");\n   printf(\"Please enter the N size number \\n\");\n   int N =0;\n   scanf(\"%d\", &amp;N);\n\n   // Initialize the memory\n   float *a, *b, *c;       \n\n   // Allocate memory\n   a = (float*)malloc(sizeof(float) * (N*N));\n   b = (float*)malloc(sizeof(float) * (N*N));\n   c = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize arrays\n  for(int i = 0; i &lt; (N*N); i++)\n     {\n       a[i] = 1.0f;\n       b[i] = 2.0f;\n     }\n\n   // Fuction call \n   Matrix_Multiplication(a, b, c, N);\n\n   // Verification\n   for(int i = 0; i &lt; N; i++)\n      {\n      for(int j = 0; j &lt; N; j++)\n         {\n      printf(\"%f \", c[j]);\n\n    }\n      printf(\"\\n\");\n      }\n\n    // Deallocate memory\n    free(a); \n    free(b); \n    free(c);\n\n   return 0;\n}\n</code></pre>   <pre><code>module Matrix_Multiplication_Mod  \nimplicit none \ncontains\n subroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\n\ndo row = 0, width-1\n   do col = 0, width-1\n      sum=0\n       do i = 0, width-1\n         sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n       enddo\n      c(row*width+col+1) = sum\n   enddo\nenddo\n\n\n  end subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\n\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\n   a(i) = sin(i*1D0) * sin(i*1D0)\n   b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n! Call the vector add subroutine \ncall Matrix_Multiplication(a, b, c, n)\n\n!!Verification\ndo i=1,n*n\n   print *, c(i)\nenddo\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>   <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\n\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   \n{ \n  float sum = 0;\n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)\n        {\n          sum=0;\n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              sum += a[row*width+i] * b[i*width+col];      \n            }                                                     \n          c[row*width+col] = sum;                           \n        }\n    }   \n}\n\nint main()\n {  \n   printf(\"Programme assumes that matrix size is N*N \\n\");\n   printf(\"Please enter the N size number \\n\");\n   int N =0;\n   scanf(\"%d\", &amp;N);\n\n   // Initialize the memory \n   float *a, *b, *c;       \n\n   // Allocate memory\n   a = (float*)malloc(sizeof(float) * (N*N));\n   b = (float*)malloc(sizeof(float) * (N*N));\n   c = (float*)malloc(sizeof(float) * (N*N));\n\n  // Initialize arrays\n  for(int i = 0; i &lt; (N*N); i++)\n     {\n       a[i] = 1.0f;\n       b[i] = 2.0f;\n     }\n\n   // Fuction call \n   Matrix_Multiplication(a, b, c, N);\n\n   // Verification\n   for(int i = 0; i &lt; N; i++)\n      {\n      for(int j = 0; j &lt; N; j++)\n         {\n      printf(\"%f \", c[j]);\n\n    }\n      printf(\"\\n\");\n      }\n\n    // Deallocate memory\n    free(a); \n    free(b); \n    free(c);\n\n   return 0;\n}\n</code></pre>   <pre><code> module Matrix_Multiplication_Mod  \nimplicit none \ncontains\n subroutine Matrix_Multiplication(a, b, c, width)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\nreal(8) :: sum = 0\ninteger :: i, row, col, width\n!!! ADD LOOP PARALLELISATION\ndo row = 0, width-1\n   do col = 0, width-1\n      sum=0\n       do i = 0, width-1\n         sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n       enddo\n      c(row*width+col+1) = sum\n   enddo\nenddo\n\n\n  end subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\n\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\n   a(i) = sin(i*1D0) * sin(i*1D0)\n   b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!!!! ADD PARALLEL REGION \n! Call the vector add subroutine \ncall Matrix_Multiplication(a, b, c, n)\n\n!!Verification\ndo i=1,n*n\n   print *, c(i)\nenddo\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>   <pre><code>#include&lt;stdio.h&gt;\n#include&lt;stdlib.h&gt;\n#include&lt;omp.h&gt;\n\nvoid Matrix_Multiplication(float *a, float *b, float *c, int width)   \n{ \n  float sum = 0;\n  #pragma for loop collapse(2) reduction (+:sum)\n  for(int row = 0; row &lt; width ; ++row)                           \n    {                                                             \n      for(int col = 0; col &lt; width ; ++col)\n        {\n          sum=0;\n          for(int i = 0; i &lt; width ; ++i)                         \n            {                                                     \n              sum += a[row*width+i] * b[i*width+col];      \n            }                                                     \n          c[row*width+col] = sum;                           \n        }\n    }   \n}\n\nint main()\n {  \n   printf(\"Programme assumes that matrix size is N*N \\n\");\n   printf(\"Please enter the N size number \\n\");\n   int N =0;\n   scanf(\"%d\", &amp;N);\n\n   // Initialize the memory\n   float *a, *b, *c;       \n\n   // Allocate memory\n   a = (float*)malloc(sizeof(float) * (N*N));\n   b = (float*)malloc(sizeof(float) * (N*N));\n   c = (float*)malloc(sizeof(float) * (N*N));\n\n   // Initialize arrays\n   for(int i = 0; i &lt; (N*N); i++)\n      {\n        a[i] = 1.0f;\n        b[i] = 2.0f;\n      }\n   #pragma omp parallel\n   // Fuction call \n   Matrix_Multiplication(a, b, c, N);\n\n   // Verification\n   for(int i = 0; i &lt; N; i++)\n      {\n      for(int j = 0; j &lt; N; j++)\n         {\n          printf(\"%f \", c[j]);\n     }\n       printf(\"\\n\");\n      }\n\n   // Deallocate memory\n   free(a); \n   free(b); \n   free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Matrix_Multiplication_Mod  \n  implicit none \ncontains\n  subroutine Matrix_Multiplication(a, b, c, width)\n    use omp_lib\n    ! Input vectors\n    real(8), intent(in), dimension(:) :: a\n    real(8), intent(in), dimension(:) :: b\n    real(8), intent(out), dimension(:) :: c\n    real(8) :: sum = 0\n    integer :: i, row, col, width\n\n    !$omp do collapse(2) reduction(+:sum)\n    do row = 0, width-1\n       do col = 0, width-1\n          sum=0\n          do i = 0, width-1\n             sum = sum + (a((row*width)+i+1) * b((i*width)+col+1))\n          enddo\n          c(row*width+col+1) = sum\n       enddo\n    enddo\n    !$omp end do\n\n  end subroutine Matrix_Multiplication\nend module Matrix_Multiplication_Mod\n\nprogram main\nuse Matrix_Multiplication_Mod\nuse omp_lib\nimplicit none\n\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b\n\n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n*n))\nallocate(b(n*n))\nallocate(c(n*n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n*n\n   a(i) = sin(i*1D0) * sin(i*1D0)\n   b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!$omp parallel\n! Call the vector add subroutine \ncall Matrix_Multiplication(a, b, c, n)\n!$omp end parallel\n\n!!Verification\ndo i=1,n*n\n   print *, c(i)\nenddo\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>      Compilation and Output Serial-version(C/C++)Serial-version(FORTRAN)OpenMP(C/C++)OpenMP(FORTRAN)   <pre><code>// compilation\n$ gcc Matrix-multiplication.c -o Matrix-Multiplication-Serial\n\n// execution \n$ ./Matrix-Multiplication-Serial\n\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre>   <pre><code>// compilation\n$ gfortran Matrix-multiplication.f90 -o Matrix-Multiplication-Serial\n\n// execution \n$ ./Matrix-Multiplication-Serial\n\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre>   <pre><code>// compilation\n$ gcc -fopenmp Matrix-multiplication-Solution.c -o Matrix-Multiplication-Solution\n\n// execution\n$ ./Matrix-Multiplication-Solution\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre>   <pre><code>// compilation\n$ gfortran -fopenmp Matrix-multiplication-Solution.f90 -o Matrix-Multiplication-Solution\n\n// execution\n$ ./Matrix-Multiplication-Solution\nProgramme assumes that matrix (square matrix) size is N*N \nPlease enter the N size number \n4\n8 8 8 8 \n8 8 8 8  \n8 8 8 8  \n8 8 8 8 \n</code></pre>      Questions <ul> <li>Right now, we are dealing with square matrices. Could you write a code for a different matrix size while still fulfilling the matrix multiplication condition?</li> </ul> <p>   </p> <ul> <li>Could you use any one of the loop scheduling, for example, <code>dynamic</code> or <code>static</code>? Do you see any performance gain?</li> </ul>"},{"location":"openmp/exercise-6/","title":"SIMD and Others","text":"<p>In this exercise, we will try to add the <code>simd</code> classes to our existing problems, for example, vector addition. </p>  Examples and Question: SIMD - Vector Addition Serial(C/C++)Serial(FORTRAN)Template(C/C++)Template(FORTRAN)Solution(C/C++)Solution(FORTRAN)   <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // Executing vector addtion function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>   <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n{\n// ADD YOUR PARALLEL REGION FOR THE LOOP SIMD\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  // Start measuring time\n  clock_t start = clock();\n\n  // ADD YOUR PARALLEL REGION HERE  \n  // Executing vector addtion function \n  Vector_Add(a, b, c, N);\n\n  // Stop measuring time and calculate the elapsed time\n  clock_t end = clock();\n  double elapsed = (double)(end - start)/CLOCKS_PER_SEC;\n\n  printf(\"Time measured: %.3f seconds.\\n\", elapsed);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!! ADD YOUR PARALLEL DO LOOP WITH SIMD\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\n!! ADD YOUR PARALLEL REGION \n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>   <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;math.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;time.h&gt;\n\n#define N 5120\n#define MAX_ERR 1e-6\n\n// CPU function that adds two vector \nfloat * Vector_Add(float *a, float *b, float *c, int n) \n#pragma omp for simd\n// ADD YOUR PARALLE SIMD\n  for(int i = 0; i &lt; n; i ++)\n    {\n      c[i] = a[i] + b[i];\n    }\n  return c;\n}\n\nint main()\n{\n  // Initialize the variables\n  float *a, *b, *c;       \n\n  // Allocate the memory\n  a   = (float*)malloc(sizeof(float) * N);\n  b   = (float*)malloc(sizeof(float) * N);\n  c = (float*)malloc(sizeof(float) * N);\n\n  // Initialize the arrays\n  for(int i = 0; i &lt; N; i++)\n    {\n      a[i] = 1.0f;\n      b[i] = 2.0f;\n    }\n\n  double start = omp_get_wtime();\n  #pragma omp parallel \n  // Executing vector addtion function \n  Vector_Add(a, b, c, N);\n  double end = omp_get_wtime();\n  printf(\"Work took %f seconds\\n\", end - start);\n\n  // Verification\n  for(int i = 0; i &lt; N; i++)\n    {\n      assert(fabs(c[i] - a[i] - b[i]) &lt; MAX_ERR);\n    }\n\n  printf(\"c[0] = %f\\n\", c[0]);\n  printf(\"PASSED\\n\");\n\n  // Deallocate the memory\n  free(a); \n  free(b); \n  free(c);\n\n  return 0;\n}\n</code></pre>   <pre><code>module Vector_Addition_Mod  \nimplicit none \n  contains\nsubroutine Vector_Addition(a, b, c, n)\nuse omp_lib\n! Input vectors\nreal(8), intent(in), dimension(:) :: a\nreal(8), intent(in), dimension(:) :: b\nreal(8), intent(out), dimension(:) :: c\ninteger :: i, n\n!$omp do simd\n  do i = 1, n\n    c(i) = a(i) + b(i)\n  end do\n!$omp end do simd\n end subroutine Vector_Addition\nend module Vector_Addition_Mod\n\nprogram main\nuse Vector_Addition_Mod\nimplicit none\n! Input vectors\nreal(8), dimension(:), allocatable :: a\nreal(8), dimension(:), allocatable :: b \n! Output vector\nreal(8), dimension(:), allocatable :: c\n! real(8) :: sum = 0\ndouble precision :: start, end\n\ninteger :: n, i  \nprint *, \"This program does the addition of two vectors \"\nprint *, \"Please specify the vector size = \"\nread *, n\n\n! Allocate memory for vector\nallocate(a(n))\nallocate(b(n))\nallocate(c(n))\n\n! Initialize content of input vectors, \n! vector a[i] = sin(i)^2 vector b[i] = cos(i)^2\ndo i = 1, n\n  a(i) = sin(i*1D0) * sin(i*1D0)\n  b(i) = cos(i*1D0) * cos(i*1D0) \nenddo\n\nstart = omp_get_wtime()\n!$omp parallel \n! Call the vector add subroutine \ncall Vector_Addition(a, b, c, n)\n!$omp end parallel\nend = omp_get_wtime()\nPRINT *, \"Work took\", end - start, \"seconds\"\n\n!!Verification\ndo i = 1, n\n  if (abs(c(i)-(a(i)+b(i)) == 0.00000)) then \n   else\n     print *, \"FAIL\"\n   endif\nenddo\nprint *, \"PASS\"\n\n! Delete the memory\ndeallocate(a)\ndeallocate(b)\ndeallocate(c)\n\nend program main\n</code></pre>    <ul> <li>Please try the examples without the <code>simd</code> clause. Do you notice any performance differences? </li> </ul>"},{"location":"openmp/exercise-6/#critical-single-and-master","title":"Critical, Single, and Master","text":"<p>We will explore how single, master and critical are working in the OpenMP programming model. For this, we consider the following simple examples.</p>  Examples and Question: Critical, Single and Master (C/C++)FORTRAN)   <pre><code>#include&lt;iostream&gt;\n#include&lt;omp.h&gt;\nusing namespace std;\nint main()\n{\n  cout &lt;&lt; \"Hello world from master thread \"&lt;&lt; endl;\n  cout &lt;&lt; endl;\n\n  // creating the parallel region (with N number of threads)\n  #pragma omp parallel\n   {\n        cout &lt;&lt; \"Hello world from thread id \"\n        &lt;&lt; omp_get_thread_num() &lt;&lt; \" from the team size of \"\n        &lt;&lt; omp_get_num_threads()\n        &lt;&lt; endl;\n    } // parallel region is closed\n\ncout &lt;&lt; endl;\ncout &lt;&lt; \"end of the programme from master thread\" &lt;&lt; endl;\nreturn 0;\n}\n</code></pre>   <pre><code>program Hello_world_OpenMP\nuse omp_lib\n\n!$omp parallel \nprint *, 'Hello world from thread id ', omp_get_thread_num(), 'from the team size of', omp_get_num_threads()\n!$omp end parallel\n\nend program\n</code></pre>    <ul> <li>Try single clause</li> <li>Try master clause</li> <li>Try critical clause</li> </ul>"},{"location":"openmp/preparation/","title":"Preparation","text":""},{"location":"openmp/preparation/#1-how-to-login-to-meluxina-machine","title":"1. How to login to MeluXina machine","text":"<ul> <li>1.1 Please take a look if you are using Windows</li> <li>1.2 Please take a look if you are using Linux/Mac</li> </ul>"},{"location":"openmp/preparation/#2-use-your-username-to-connect-to-meluxina","title":"2. Use your username to connect to MeluXina","text":"<ul> <li>2.1 For example the below example shows the user of <code>u100490</code> <pre><code>$ ssh u100490@login.lxp.lu -p 8822\n### or\n$ ssh meluxina \n</code></pre></li> </ul>"},{"location":"openmp/preparation/#3-once-you-have-logged-in","title":"3. Once you have logged in","text":"<ul> <li>3.1 Once you have logged in, you will be in a default home directory    <pre><code>[u100490@login02 ~]$ pwd\n/home/users/u100490\n</code></pre></li> <li>3.2 After that, go to the project directory.   <pre><code>[u100490@login02 ~]$ cd /project/home/p200117\n[u100490@login02 p200117]$ pwd\n/project/home/p200117\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#4-and-please-create-your-own-working-folder-under-the-project-directory","title":"4. And please create your own working folder under the project directory","text":"<ul> <li>4.1 For example, here is the user with <code>u100490</code>:   <pre><code>[u100490@login02 p200117]$ mkdir $USER\n### or \n[u100490@login02 p200117]$ mkdir u100490  \n</code></pre></li> </ul>"},{"location":"openmp/preparation/#5-now-it-is-time-to-move-into-your-home-directory","title":"5. Now it is time to move into your home directory","text":"<ul> <li>5.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login02 p200117]$cd u100490\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#6-now-it-is-time-to-copy-the-folder-which-has-examples-and-source-files-to-your-home-directory","title":"6. Now it is time to copy the folder which has examples and source files to your home directory","text":"<ul> <li>6.1 For example, with user home directory <code>u100490</code> <pre><code>[u100490@login03 u100490]$ cp -r /project/home/p200117/OpenMP .\n[u100490@login03 u100490]$ cd OpenMP/\n[u100490@login03 OpenMP]$ pwd\n/project/home/p200117/u100490/OpenMP\n[u100490@login03 OpenMP]$ ls -lthr\ndrwxr-s---. 2 u100490 p200117 4.0K May 27 21:42 Data-Sharing-Attribute\ndrwxr-s---. 2 u100490 p200117 4.0K May 28 00:35 Parallel-Region\ndrwxr-s---. 2 u100490 p200117 4.0K May 30 18:26 Dry-run-test\n...\n...\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#7-until-now-you-are-in-the-login-node-now-its-time-to-do-the-dry-run-test","title":"7. Until now you are in the login node, now its time to do the dry run test","text":"<ul> <li> <p>7.1 Reserve the interactive node for running/testing OpenMP applications    <pre><code>$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 01:00:00\n</code></pre></p>  check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 01:00:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre>  </li> <li> <p>7.2 You can also check if you got the interactive node for your computations, for example, here with the user <code>u100490</code>:  <pre><code>[u100490@mel2131 ~]$ squeue -u u100490\n            JOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n           304381       cpu interact  u100490    p200117  RUNNING       0:37     01:00:00      1 mel2131\n</code></pre></p> </li> </ul>"},{"location":"openmp/preparation/#8-now-we-need-to-check-simple-openmp-application-if-that-is-going-to-work-for-you","title":"8. Now we need to check simple OpenMP application, if that is going to work for you:","text":"<ul> <li>8.1 Go to folder <code>Dry-run-test</code> <pre><code>[u100490@login03 OpenMP]$ cd Dry-run-test/\n[u100490@login03 Dry-run-test]$ ls \nsource.sh  Test.cc  Test.f90\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#9-finally-we-need-to-load-the-compiler-to-test-the-our-openmp-codes","title":"9. Finally, we need to load the compiler to test the our OpenMP codes","text":"<ul> <li> <p>9.1 We will work with GNU compiler  <pre><code>$ source module.sh\n</code></pre></p>  check if the module is loaded properly <pre><code>[u100490@mel2131 ~]$ module list\n\ncurrently Loaded Modules:\n1) env/release/2022.1                (S)  19) libpciaccess/0.16-GCCcore-11.3.0    37) jbigkit/2.1-GCCcore-11.3.0        55) VTune/2022.3.0                          73) NSS/3.79-GCCcore-11.3.0\n2) lxp-tools/myquota/0.3.1           (S)  20) X11/20220504-GCCcore-11.3.0         38) gzip/1.12-GCCcore-11.3.0          56) numactl/2.0.14-GCCcore-11.3.0           74) snappy/1.1.9-GCCcore-11.3.0\n3) GCCcore/11.3.0                         21) Arm-Forge/22.0.4-GCC-11.3.0         39) lz4/1.9.3-GCCcore-11.3.0          57) hwloc/2.7.1-GCCcore-11.3.0              75) JasPer/2.0.33-GCCcore-11.3.0\n4) zlib/1.2.12-GCCcore-11.3.0             22) libglvnd/1.4.0-GCCcore-11.3.0       40) zstd/1.5.2-GCCcore-11.3.0         58) OpenSSL/1.1                             76) nodejs/16.15.1-GCCcore-11.3.0\n5) binutils/2.38-GCCcore-11.3.0           23) AMD-uProf/3.6.449                   41) libdeflate/1.10-GCCcore-11.3.0    59) libevent/2.1.12-GCCcore-11.3.0          77) Qt5/5.15.5-GCCcore-11.3.0\n6) ncurses/6.3-GCCcore-11.3.0             24) Advisor/2022.1.0                    42) LibTIFF/4.3.0-GCCcore-11.3.0      60) UCX/1.13.1-GCCcore-11.3.0               78) CubeGUI/4.7-GCCcore-11.3.0\nWhere:\n    S:  Module is Sticky, requires --force to unload or purge\n</code></pre>  </li> </ul>"},{"location":"openmp/preparation/#10-please-compile-and-test-your-openmp-application","title":"10. Please compile and test your OpenMP application","text":"<ul> <li>10.1 For example, Dry-run-test  <pre><code>// compilation (C/C++)\n$ g++ Test.cc -fopenmp\n\n// compilation (FORTRAN)\n$ gfortran Test.f90 -fopenmp\n\n// execution\n$ ./a.out\n\n// output\n$ Hello world from master thread \n  Hello world from thread id Hello world from thread id Hello world from thread \n  id Hello world from thread id Hello world from thread id 4 from the team size of \n  1 from the team size of 20 from the team size of  from the team size of 555\n</code></pre></li> </ul>"},{"location":"openmp/preparation/#11-similarly-for-the-hands-on-session-we-need-to-do-the-node-reservation","title":"11. Similarly for the hands-on session, we need to do the node reservation:","text":"<ul> <li> <p>11.1 For example, reservation   <pre><code>$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 02:30:00\n</code></pre></p>  check if your reservation is allocated <pre><code>[u100490@login03 ~]$ salloc -A p200117 --res intro_openmp --partition=cpu --qos default -N 1 -t 02:30:00\nsalloc: Pending job allocation 296848\nsalloc: job 296848 queued and waiting for resources\nsalloc: job 296848 has been allocated resources\nsalloc: Granted job allocation 296848\nsalloc: Waiting for resource configuration\nsalloc: Nodes mel2131 are ready for job\n</code></pre>  </li> </ul>"},{"location":"openmp/preparation/#12-we-will-continue-with-our-hands-on-exercise","title":"12. We will continue with our Hands on exercise","text":"<ul> <li>12.1 For example, <code>Hello World</code> example, we do the following steps:</li> </ul> <pre><code>[u100490@mel2063 OpenMP]$ pwd\n/project/home/p200117/u100490/OpenMP\n[u100490@mel2063 OpenMP]$ ls\n[u100490@mel2063 OpenMP]$ ls\ndrwxr-s---. 2 u100490 p200117 4.0K May 27 21:42 Data-Sharing-Attribute\ndrwxr-s---  2 u100490 p200117 4.0K May 28 00:35 Parallel-Region\ndrwxr-s---  2 u100490 p200117 4.0K May 28 23:45 Worksharing-Constructs-Schedule\ndrwxr-s---. 2 u100490 p200117 4.0K May 29 00:57 Worksharing-Constructs-Other\ndrwxr-s---. 2 u100490 p200117 4.0K May 29 18:07 Worksharing-Constructs-Loop\ndrwxr-s---. 2 u100490 p200117 4.0K May 30 18:25 SIMD-Others\ndrwxr-s---. 2 u100490 p200117 4.0K May 30 18:37 Dry-run-test\n-rw-r-----  1 u100490 p200117  241 May 30 18:41 module.sh\n[u100490@mel2063 OpenMP]$ source module.sh\n</code></pre>"},{"location":"openmp/profiling/","title":"Profiling and Performance","text":"<p>Profiling is an important task to be considered when a computer code is written. Writing parallel code is less challenging, but making it more efficient on a given parallel architecture is challenging. Moreover,  from the programming and programmer\u2019s perspective, we want to know where the code spends most of its time. In particular, we would like to know if the code (given algorithm) is compute bound, memory bound, cache misses, memory leak, proper vectorisation, cache misses, register spilling, or hot spot (time-consuming part in the code). Plenty of tools are available to profile a scientific code (computer code for doing arithmetic computing using processors). However, Here, we will focus few of the widely used tools.</p> <ul> <li>AMD uProf</li> <li>ARM Forge</li> <li>Intel tools</li> </ul>"},{"location":"openmp/profiling/#arm-forge","title":"ARM Forge","text":"<p>Arm Forge is another standard commercial tool for debugging, profiling, and analysing scientific code on the massively parallel computer architecture. They have a separate toolset for each category with the common environment: DDT for debugging, MAP for profiling, and performance reports for analysis. It also supports the MPI, UPC, CUDA, and OpenMP programming models for a different architecture with different variety of compilers. DDT and MAP will launch the GUI, where we can interactively debug and profile the code. Whereas <code>perf-report</code> will provide the analysis results in <code>.html</code> and <code>.txt</code> files.</p>  Example: ARM Forge C/C++FORTRAN   <pre><code># compilation with debugging tool\n$ gcc test.c -g -fopenmp\n# execute and profile the code\n$ map --profile --no-mpi ./a.out\n# open the profiled result in GUI\n$ map xyz.map\n\n# for debugging\n$ ddt ./a .out\n\n# for profiling\n$ map ./a .out\n\n# for analysis\n$ perf-report ./a .out\n</code></pre>   <pre><code># compilation \n$ gfortran test.f90 -fopenmp\n# execute and profile the code\n$ map --profile --no-mpi ./a.out\n# open the profiled result in GUI\n$ map xyz.map\n\n# for debugging\n$ ddt ./a .out\n\n# for profiling\n$ map ./a .out\n\n# for analysis\n$ perf-report ./a .out\n</code></pre>    <p>   </p>"},{"location":"openmp/profiling/#intel-tools","title":"Intel tools","text":""},{"location":"openmp/profiling/#intel-application-snapshot","title":"Intel Application Snapshot","text":"<p>Intel Application Performance Snapshot tool helps to find essential performance factors and the metrics of CPU utilisation, memory access efficiency, and vectorisation. <code>aps -help</code> will list out profiling metrics options in APS</p>      Example: APS C/C++FORTRAN   <pre><code># compilation\n$ icc -qopenmp test.c\n\n# code execution\n$ aps --collection-mode=all -r report_output ./a.out\n$ aps-report -g report_output                        # create a .html file\n$ firefox report_output_&lt;postfix&gt;.html               # APS GUI in a browser\n$ aps-report report_output                           # command line output\n</code></pre>   <pre><code># compilation\n$ ifort -qopenmp test.f90\n\n# code execution\n$ aps --collection-mode=all -r report_output ./a.out\n$ aps-report -g report_output                        # create a .html file\n$ firefox report_output_&lt;postfix&gt;.html               # APS GUI in a browser\n$ aps-report report_output                           # command line output\n</code></pre>    <p>   </p>"},{"location":"openmp/profiling/#intel-inspector","title":"Intel Inspector","text":"<p>Intel Inspector detects and locates the memory, deadlocks, and data races in the code. For example, memory access and memory leaks can be found.</p>  Example: Intel Inspector C/C++FORTRAN   <pre><code># compile the code\n$ icc -qopenmp example.c\n# execute and profile the code\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n$ cat inspxe-cl.txt\n# open the file to see if there is any memory leak\n=== Start: [2020/12/12 01:19:59] ===\n0 new problem(s) found\n=== End: [2020/12/12 01:20:25] ===\n</code></pre>   <pre><code># compile the code\n$ ifort -qopenmp test.f90\n# execute and profile the code\n$ inspxe-cl -collect mi1 -result-dir mi1 -- ./a.out\n$ cat inspxe-cl.txt\n# open the file to see if there is any memory leak\n=== Start: [2023/05/10 01:19:59] ===\n0 new problem(s) found\n=== End: [2020/05/10 01:20:25] ===\n</code></pre>"},{"location":"openmp/profiling/#intel-advisor","title":"Intel Advisor","text":"<p>Intel Advisor: a set of collection tools for the metrics and traces that can be used for further tuning in the code. <code>survey</code>: analyse and explore an idea about where to add efficient vectorisation.</p>  Example: Intel Advisor C/C++FORTRAN   <pre><code># compile the code\n$ icc -qopenmp test.c\n# collect the survey metrics\n$ advixe-cl -collect survey -project-dir result -- ./a.out\n# collect the report\n$ advixe-cl -report survey -project-dir result\n# open the gui for report visualization\n$ advixe-gui\n</code></pre>   <pre><code># compile the code\n$ ifort -qopenmp test.90\n# collect the survey metrics\n$ advixe-cl -collect survey -project-dir result -- ./a.out\n# collect the report\n$ advixe-cl -report survey -project-dir result\n# open the gui for report visualization\n$ advixe-gui\n</code></pre>    <p>   </p>"},{"location":"openmp/profiling/#intel-vtune","title":"Intel VTune","text":"<ul> <li>Identifying the time consuming part in the code.</li> <li>And also identify the cache misses and latency.</li> </ul>  Example: Intel VTune C/C++FORTRAN   <pre><code># compile the code\n$ icc -qopenmp test.c\n# execute the code and collect the hotspots\n$ amplxe-cl -collect hotspots -r amplifier_result ./a.out\n$ amplxe-gui\n# open the GUI of VTune amplifier\n</code></pre>   <pre><code># compile the code\n$ ifort -qopenmp test.90\n# execute the code and collect the hotspots\n$ amplxe-cl -collect hotspots -r amplifier_result ./a.out\n$ amplxe-gui\n# open the GUI of VTune amplifier\n</code></pre>    <p>   </p> <p><code>amplxe-cl</code> will list out the analysis types and <code>amplxe-cl -hlep</code> report will list out available reports in VTune.</p>"},{"location":"openmp/profiling/#amd-uprof","title":"AMD uProf","text":"<p>AMD uProf profiler follows a statistical sampling-based approach to collect profile data to identify the performance bottlenecks in the application.</p>  Example: AMD uProf C/C++FORTRAN   <pre><code># compile the code\n$ clang -fopenmp test.c\n$ AMDuProfCLI collect --trace openmp --config tbp --output-dir solution ./a.out -d 1\n</code></pre>   <pre><code># compile the code\n$ flang -fopenmp test.90\n$ AMDuProfCLI collect --trace openmp --config tbp --output-dir solution ./a.out -d 1\n</code></pre>"}]}